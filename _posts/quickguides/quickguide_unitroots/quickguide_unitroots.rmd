---
title: "Quick Guide - Unit roots"
author: Ralf Martin
output:
  distill::distill_article:
    self_contained: false
    css: ../../webex.css
    includes:
      after_body: ../../webex.js
categories:
  - Quickguides
editor_options: 
  chunk_output_type: inline
---




```{r setup, include = FALSE, message=FALSE}
library("webex")
library(ggplot2)
#library(tutorial)
#tutorial::go_interactive()
#go_interactive( greedy = FALSE, height = 200 )

```

<!--html_preserve-->
<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>

<style>
  /* Rearrange console label */
  .datacamp-exercise ol li, .datacamp-exercise ul li {
    margin-bottom: 0em !important;
  }
  
  /* Remove bullet marker */
  .datacamp-exercise ol li::before, .datacamp-exercise ul li::before {
    content: '' !important;
  }
</style>
<!--/html_preserve-->



When dealing with time series we have to account for the possibility that the observations in our data are correlated from one observations to the next; e.g. if we see a particularly high value in one period we can also expect a higher value in the next period.

One of the simplest possibilities is a so called autoregressive model with one lag; i.e.
$$y_t=\beta_0+\beta_1 y_{t-1}+\epsilon_t \tag{1}$$
Hence, this is like the normal linear regression model except that one of our explanatory variables is the dependent variable in the previous period.
In principle we can estimate that like any other regression model; i.e. with OLS  using the `lm()` command in R.


However, this goes wrong if $\beta_1$ gets too big; i.e. if it is equal or larger than 1 or smaller than -1 (i.e. $|\beta_1|>1$). This is also referred to as the times series having a "unit root".

If this happens, 

- OLS will give us a (downward) biased estimate of $\beta_1$
- We might find spurious relationships between $y_t$ and other variables that have a unit root

i.e. we might find a strong correlation between two data series even though there is no causal mechanism between them. All that we are picking up is that both series have a unit root.

To check if we are dealing with a unit root in a given data series we can use the so called Dickey-Fuller (DF) test.
This test is based on a transformed version of the above model where we subtract $y_{t-1}$ from either side of the equation to get:

$$\Delta y_t= y_t - y_{t-1}=  \beta_0+ \delta  y_{t-1}+\epsilon_t \tag{2}$$



where $\delta = \beta_1-1$. Hence, in Equation 2 a unit root would imply $\delta=0$ and this is the (null) hypothesis the DF test examines.

We can implement this in R with the `ur.df()` command from the `urca` package. Here we do this with a simulated series with unit roots:


```{r}
set.seed(2.22112)

```

```{r,echo=TRUE}

obs=100
eps=rnorm(obs)
y100=eps
for(i in 2:obs){
  y100[i]= y100[i-1]*1+eps[i]
}
sdf=data.frame(y100,eps,period=1:obs) 

```


If we plot y100 this will look as follows:

```{r}
library(ggplot2)
ggplot(sdf,aes(x=period))  + geom_line(aes(y=y100))+
           theme_minimal()+ylab("y")


```



We can now implement the DF test:

```{r,echo=TRUE}
library(urca)
library(dplyr)

ur.df(sdf$y100,lags=0) %>% summary()

```

The summary of the output of the `ur.df()` provides us with a the regression output we would be getting if we simply implmented equation 2 as an OLS model.
What is called `z.lag.1` corresponds to $\delta$ in equation 2. HOwever, as we said above: this estimate will be biased and so will be the P-value which we could normally use to decide if a regression coefficient is different from 0. Equally, we CANNOT use the typical critical values we might normally compare the t-value to. Luckily, `ur.df()` reports
some new critical thresholds that we can use at the bottom of its output (i.e. the `Critical values for test statistics:`). We see critical values for a 1, 5 and 10% significance level.
What we need to check is if our test statistic is smaller than the values reported there. If that is the case we can reject the hypothesis that $\beta_1$ is equal to 1. Not surprisingly - given that we simulated our series with a $\beta_1=1$ we cannot do that. So we conclude that we have unit root and we see if can get rid of it by differencing the series:


```{r,echo=TRUE}
library(urca)
library(dplyr)

ur.df(diff(sdf$y100),lags=0) %>% summary()

```

This turns out to be the case. The test statistics is now much smaller than the critical value.


# Unit roots and causal inference

A problem with unit roots is that they create spurious correlations. Let's explore this by creating another time series with unit root:

```{r,echo=TRUE}
epsx=rnorm(obs)
x100=eps
for(i in 2:obs){
  x100[i]= x100[i-1]*1+epsx[i]
}
sdf=bind_cols(sdf,data.frame(x100,epsx)) 


```
<br>
Let's plot the time series

```{r}
library(ggplot2)
ggplot(sdf,aes(x=period))  + geom_line(aes(y=y100,color="Y"))+
           theme_minimal() + geom_line(aes(y=x100,color="X"))+ylab("")+ labs(color = "")



```

Let's also run a regression of the Y on X:

```{r,echo=TRUE}
lm(y100~x100,sdf) %>% summary()
lm(y100~x100+period,sdf) %>% summary()

```


Hence, despite X and Y being absolutely un-related to each other we find a strong correlation. The matter cannot in general be addressed by including a linear time trend. Let's see what happens if we difference both series:


```{r echo=TRUE}

sdf=sdf %>% mutate(Dy100=y100-dplyr::lag(y100))
lm(Dy100~x100,sdf %>% filter(period>1)) %>% summary()

```

i.e. we cannot find a significant relationship (as we should not).


Let's also see what happens if we have a Y variable that is actually driven by X. We using the following true model:


$$ y_t=\beta_0 +  y_{t-1}+ 0.5  x_{t} +\varepsilon_t$$
<br>
Let's head for Monte Carlo and simulate data from this model:


```{r,echo=TRUE}
epsyyy=rnorm(obs)
yyy=eps+ 0.5 * x100[1]
for(i in 2:obs){
  yyy[i]= yyy[i-1]*1+epsyyy[i] + 0.5 * x100[i]
}
sdf=bind_cols(sdf,data.frame(yyy,epsyyy)) 
```

<br>
Once more we can plot:

```{r}

library(ggplot2)
ggplot(sdf,aes(x=period))  + geom_line(aes(y=y100,color="Y"))+ 
            geom_line(aes(y=yyy,color="Y(x)"))+
           theme_minimal() +geom_line(aes(y=x100,color="X"))+ylab("")+ labs(color = "")


```

And regress:

```{r,echo=TRUE}
lm(yyy~x100,sdf) %>% summary()

sdf=sdf %>%  mutate(Dyyy=yyy-dplyr::lag(yyy),Dx100=x100-dplyr::lag(x100))


lm(Dyyy~x100,sdf ) %>% summary()

```
<br>
Note that if we run a regression of `yyy` on `x100` we get an estimate that is wildly off the true on estimate. If we run a regression in first differences we get something fairly close to the true value of 0.5.


Also good practice to check that `yyy` is really stationary now:

```{r,echo=TRUE}

ur.df(diff(sdf$yyy),lags=0) %>% summary()

```

Indeed, we can reject $\delta=0$ on the differenced series (at least at 5%).

# Extensions
Above we explore the simplest autoregressive model. More complex autoregressive models could include more lags, a (nonzero) constant or a time trend. We can accomodate these cases with the `ur.df()` command. For instance

```{r, echo=TRUE}
ur.df((sdf$y100),lags=2, type="trend") %>% summary()
```

examines a model with 3 lags and a time trend. How do we know if that is necessary? Note that it says `lags=2` because that refers to lags beyond lag one.
How do  we know if such a more general model is needed? We can go from more general to more specific; i.e. this would look as follows:


$$\Delta y_t=\beta_0 +\rho \times t+ \delta y_{t-1}+ \beta_2 \Delta y_{t-1} + \beta_3 \Delta y_{t-2}+\varepsilon_t$$

Firstly, on the lags. It turns out that the standard errors for the coefficients on the lags are actually not biased in the differences from of the equation. Hence, we can use the normal t-tests to throw out lags; e.g. let's try `lags=1` only as above the second lag term `z.diff.lag2` is not significant.


```{r}
ur.df((sdf$y100),lags=1, type="trend") %>% summary()
```

Turns out the `z.diff.lag1` is not significiant either. So maybe back to `lags=0`?

```{r}
ur.df((sdf$y100),lags=0, type="trend") %>% summary()
```

But what about the time trend? Note that compared to our simple case above we now have more values being reported as test statistics and we also have more rows in our critical values table above. These are test statistics along with critical values.
What they can refer to can be a bit confusing at first, but hopefully it will become clear. Again, you might want to think of this as going from a more general to a more specific model.
So Let's first look a the third value in the `Value of test-statistic is:`  line above which is `4.3396`. That refers to the test statistic of a joint hypothesis test with 
$$H0: \delta = 1, \rho=0, \beta_0=0$$
Given that $4.3396 < 5.47$ we cannot reject all of these restrictions jointly. Consequently, we have to proceed on the basis, that there is a unit root but there are no time trends or intercepts. Note that if we have a situation where the first test statistic is larger than tau3 and the third test statistic is larger than phi3 we can conclude that we have a unit root and a time trend.
The second test statistic (i.e. 2.0237) examines the hypothesis that we have a unit root but no constant term (i.e. $H0: \delta = 1, \beta_0=0$). 



```{r  fig.width=30,echo=F,preview=TRUE,out.width = '80%'}
knitr::include_graphics("265-2656331_ginseng-root-png.png")
```


