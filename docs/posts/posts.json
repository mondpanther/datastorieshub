[
  {
    "path": "posts/exercises/exercises2/",
    "title": "Exercises 2",
    "description": "Exercises to get familiar with R.",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2021-09-30",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\nExercise 2.1\r\nIn the lecture we introduced R. The goal of this problem set is for you to work with R by yourself, so that you start to become comfortable with some basic commands, which will be needed throughout the course (and beyond?). Hence, even if you think you could answer a question with a different software package, could you please try using R?\r\nIn this exercise we are using a dataset called auto.csv. It contains various characteristics for a sample of cars. You can download this dataset from here from the course data repository. Note that a big part of working with data is to become familiar with different data formats. R can easily work with almost any dataformat imaginable but only if the user (i.e. you) gives it the right instructions. You are probably already familiar with the .csv format. But just in case not: csv stands for comma separated values. In other words: this is a table with arranged as a simple text file where rows represent table rows and the values that go into separate columns are separated by commas.\r\nTo do anything with data you need get a dataset into a dataframe (essentially another word for table) in memory. For that you can first store it on your computer’s harddrive and then load it. You can do that using RStudio’s import file menu very similar to how you would open an Excel worksheet, say:\r\n\r\n\r\n\r\nPart (a)\r\nInstead of using the import dialog, can you load the dataset with an R command either from the console command line or via an R script?\r\n\r\n\r\nHint:\r\n\r\nIf you use the RStudio dialog just discussed it will show you the R command that is needed in the Console window\r\n\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nauto <- read.csv(\"https://www.dropbox.com/s/gm22o5efboc3q0w/auto.csv?dl=1\", header=TRUE)\r\n\r\n\r\n\r\nOf course the exact formulation depends on where you saved your file on disk; i.e. it is unlikely it will be in a directory called C:/Users/Ralf Martin/Dropbox/datastories/data/.\r\nNote that header=TRUE means that R can figure out the column names from the first line of the auto.csv file.\r\n\r\nPart (b)\r\nCan you suggest an alternative version of the previous command with a relative path to locate the file?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\ngetwd()\r\n\r\n\r\n[1] \"C:/Users/rmart/github/datastorieshub/_posts/exercises/exercises2\"\r\n\r\nTo write a relative path you need to know which the current active directory is. With the getwd() command you can find out. In my case this happens to be the C:/Users/Ralf Martin/Dropbox/datastories/datastorieshub/_posts/exercises/exercises2 directory. Hence to get to directory C:/Users/Ralf Martin/Dropbox/datastories/data we need to go 4 directories up (to directory C:/Users/Ralf Martin/Dropbox/datastories ) and then down into the data directory. Note that you can use .. to indicate to R that you want to move up one layer of the directory structure. Hence the corect command with relative path notation becomes:\r\n\r\n\r\nauto <- read.csv(\"../../../../data/auto.csv\", header=TRUE)\r\n\r\n\r\n\r\n\r\nPart (c)\r\nProbably the easiest way to load a web based CSV file into R is directly from the web. This can be achieved by using the file URL in the read.csv command. Can you work out how?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nauto <- read.csv(\"https://www.dropbox.com/s/gm22o5efboc3q0w/auto.csv?dl=1\", header=TRUE)\r\n\r\n\r\n\r\nYou can figure out the URL by right clicking on a hyperlink.\r\n\r\nPart (d)\r\nDescribe the data set.\r\nHow many observations does the data set have? \r\nand how many variables does it have? \r\nWhat variables are contained in the dataset?\r\n\r\n\r\nHint:\r\n\r\nR commands you might want to use for that include summary, nrow, ncol. You can try to type ? in the RStudio console to learn more about the function. Alternatively, try googling and see how people use those functions.\r\n\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n# Summary stats for all variables\r\nsummary(auto)\r\n\r\n\r\n       X             make               price            mpg       \r\n Min.   : 1.00   Length:74          Min.   : 3291   Min.   :12.00  \r\n 1st Qu.:19.25   Class :character   1st Qu.: 4220   1st Qu.:18.00  \r\n Median :37.50   Mode  :character   Median : 5006   Median :20.00  \r\n Mean   :37.50                      Mean   : 6165   Mean   :21.30  \r\n 3rd Qu.:55.75                      3rd Qu.: 6332   3rd Qu.:24.75  \r\n Max.   :74.00                      Max.   :15906   Max.   :41.00  \r\n                                                                   \r\n     rep78          headroom         trunk           weight    \r\n Min.   :1.000   Min.   :1.500   Min.   : 5.00   Min.   :1760  \r\n 1st Qu.:3.000   1st Qu.:2.500   1st Qu.:10.25   1st Qu.:2250  \r\n Median :3.000   Median :3.000   Median :14.00   Median :3190  \r\n Mean   :3.406   Mean   :2.993   Mean   :13.76   Mean   :3019  \r\n 3rd Qu.:4.000   3rd Qu.:3.500   3rd Qu.:16.75   3rd Qu.:3600  \r\n Max.   :5.000   Max.   :5.000   Max.   :23.00   Max.   :4840  \r\n NA's   :5                                                     \r\n     length           turn        displacement     gear_ratio   \r\n Min.   :142.0   Min.   :31.00   Min.   : 79.0   Min.   :2.190  \r\n 1st Qu.:170.0   1st Qu.:36.00   1st Qu.:119.0   1st Qu.:2.730  \r\n Median :192.5   Median :40.00   Median :196.0   Median :2.955  \r\n Mean   :187.9   Mean   :39.65   Mean   :197.3   Mean   :3.015  \r\n 3rd Qu.:203.8   3rd Qu.:43.00   3rd Qu.:245.2   3rd Qu.:3.353  \r\n Max.   :233.0   Max.   :51.00   Max.   :425.0   Max.   :3.890  \r\n                                                                \r\n    foreign      \r\n Min.   :0.0000  \r\n 1st Qu.:0.0000  \r\n Median :0.0000  \r\n Mean   :0.2973  \r\n 3rd Qu.:1.0000  \r\n Max.   :1.0000  \r\n                 \r\n\r\n# Number of observations:\r\nnrow(auto)\r\n\r\n\r\n[1] 74\r\n\r\n#Number of variables\r\nncol(auto)\r\n\r\n\r\n[1] 13\r\n\r\n#Just the names of all variables\r\nnames(auto)\r\n\r\n\r\n [1] \"X\"            \"make\"         \"price\"        \"mpg\"         \r\n [5] \"rep78\"        \"headroom\"     \"trunk\"        \"weight\"      \r\n [9] \"length\"       \"turn\"         \"displacement\" \"gear_ratio\"  \r\n[13] \"foreign\"     \r\n\r\n\r\nPart (e)\r\nMuch of the power of R derives from extensions (so called packages) that have been contributed to widen the capabilities of the basic software. A particularly nice extension are the dplyr and tidyr packages that allow vastly more efficient and transparent wrangling of data. Can you install those packages on your computer and load them into memory?\r\n\r\n\r\nAnswer:\r\n\r\nTo install these packages use install.packages(\"dplyr\",\"tidyr\")\r\nNote that you only need to do this once on a given computer (it’s like installing software). However, each time you re-start R you need to load the packages you want to use into memory again. For that you use\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\n\r\n\r\n\r\n\r\nPart (f)\r\nOne of the great things included in those packages is the functionality of piping, which we shall be using extensively throughout this course. Piping allows you to send the result from one R command to the next. This typically allows for more transparent code as you will hopefully appreciate maybe not just yet but sometime halfway through the course.\r\nJust to give you an idea what piping does look at the code below. It does exactly the same as the summary(auto) command but we have now written with the piping operator %>%. What it does is that it sends whatever comes on the left to the right (i.e. flushes it down the %>% pipe ). On the right hand side it is then being used as an argument to the code that comes there (hence we don’t need to put anything in brackets in the summary command)\r\n\r\n\r\nauto %>% summary()\r\n\r\n\r\n       X             make               price            mpg       \r\n Min.   : 1.00   Length:74          Min.   : 3291   Min.   :12.00  \r\n 1st Qu.:19.25   Class :character   1st Qu.: 4220   1st Qu.:18.00  \r\n Median :37.50   Mode  :character   Median : 5006   Median :20.00  \r\n Mean   :37.50                      Mean   : 6165   Mean   :21.30  \r\n 3rd Qu.:55.75                      3rd Qu.: 6332   3rd Qu.:24.75  \r\n Max.   :74.00                      Max.   :15906   Max.   :41.00  \r\n                                                                   \r\n     rep78          headroom         trunk           weight    \r\n Min.   :1.000   Min.   :1.500   Min.   : 5.00   Min.   :1760  \r\n 1st Qu.:3.000   1st Qu.:2.500   1st Qu.:10.25   1st Qu.:2250  \r\n Median :3.000   Median :3.000   Median :14.00   Median :3190  \r\n Mean   :3.406   Mean   :2.993   Mean   :13.76   Mean   :3019  \r\n 3rd Qu.:4.000   3rd Qu.:3.500   3rd Qu.:16.75   3rd Qu.:3600  \r\n Max.   :5.000   Max.   :5.000   Max.   :23.00   Max.   :4840  \r\n NA's   :5                                                     \r\n     length           turn        displacement     gear_ratio   \r\n Min.   :142.0   Min.   :31.00   Min.   : 79.0   Min.   :2.190  \r\n 1st Qu.:170.0   1st Qu.:36.00   1st Qu.:119.0   1st Qu.:2.730  \r\n Median :192.5   Median :40.00   Median :196.0   Median :2.955  \r\n Mean   :187.9   Mean   :39.65   Mean   :197.3   Mean   :3.015  \r\n 3rd Qu.:203.8   3rd Qu.:43.00   3rd Qu.:245.2   3rd Qu.:3.353  \r\n Max.   :233.0   Max.   :51.00   Max.   :425.0   Max.   :3.890  \r\n                                                                \r\n    foreign      \r\n Min.   :0.0000  \r\n 1st Qu.:0.0000  \r\n Median :0.0000  \r\n Mean   :0.2973  \r\n 3rd Qu.:1.0000  \r\n Max.   :1.0000  \r\n                 \r\n\r\nYou can even do several of those pipes in a row. For instance, look at the following piece of code. Can you work out what it does?\r\n\r\n\r\nauto_summary <- auto %>%   select(mpg, weight)  %>% \r\n             summarise_all( list(mmm=mean,  p50=median, sd=sd  ) ) \r\n\r\n\r\n\r\n\r\n\r\nAnswer:\r\n\r\nThis piece of code takes first selects only 2 columns of the auto dataframe. It sends this smaller dataframe with only 2 variables to the command summarise_all. Summarise all will compute the statistics you mention within list(...) for all the variables in the dataframe it is being given. Note that in the list what comes after the = sign is the statistic you want whereas what comes before is a lable that will appear in the resulting dataframe, which we called auto_summary. You can look at it simply by typing its name in the console or a script:\r\n\r\n\r\nauto_summary\r\n\r\n\r\n  mpg_mmm weight_mmm mpg_p50 weight_p50   mpg_sd weight_sd\r\n1 21.2973   3019.459      20       3190 5.785503  777.1936\r\n\r\n\r\nPart (g)\r\nThe summary statistics in the previous part are not displayed very nice. The dplyr and tidyr offer alot of nice functions to reshape any output in whichever way you want. Below is a somewhat elaborate example. Below I am explaining step by step what it does. However, before you look at my explanation, maybe you can run those commands yourself step by step and figure what they do?\r\n\r\n\r\nauto_summary_reshape <- auto_summary %>% gather(stat, val) %>%\r\n                    separate(stat, into = c(\"var\", \"stat\"), sep = \"_\") %>%\r\n                    spread(stat,val) \r\n\r\n\r\n\r\n\r\n\r\nExplanation:\r\n\r\nThe code above reshapes the auto_summary dataframe so that it looks like this:\r\n\r\n\r\nauto_summary_reshape \r\n\r\n\r\n     var       mmm  p50         sd\r\n1    mpg   21.2973   20   5.785503\r\n2 weight 3019.4595 3190 777.193567\r\n\r\nWe can break this in all the steps to see how it does that:\r\n\r\n\r\n step = auto_summary %>% gather(stat, val)\r\n step\r\n\r\n\r\n        stat         val\r\n1    mpg_mmm   21.297297\r\n2 weight_mmm 3019.459459\r\n3    mpg_p50   20.000000\r\n4 weight_p50 3190.000000\r\n5     mpg_sd    5.785503\r\n6  weight_sd  777.193567\r\n\r\nThe gather command arranges what you sent them down the pipe as key value pairs; i.e. 2 columns where the first column is the key (i.e. a name or label) for the value that follows in the value column. stat and val are simply how we want to call those two columns\r\n\r\n\r\n step = step %>% separate(stat, into = c(\"var\", \"stat\"), sep = \"_\")\r\n step\r\n\r\n\r\n     var stat         val\r\n1    mpg  mmm   21.297297\r\n2 weight  mmm 3019.459459\r\n3    mpg  p50   20.000000\r\n4 weight  p50 3190.000000\r\n5    mpg   sd    5.785503\r\n6 weight   sd  777.193567\r\n\r\nThe separate command splits the stat column in two new columns, var and stat where sep=\"_\" tells R where to separate the original stat column.\r\n\r\n\r\n step = step %>% spread(stat, val) \r\n step\r\n\r\n\r\n     var       mmm  p50         sd\r\n1    mpg   21.2973   20   5.785503\r\n2 weight 3019.4595 3190 777.193567\r\n\r\nThe spread command spreads out the stat column again; i.e. creates a new column for every category represented in stat. Alternatively, we could have spread var:\r\n\r\n\r\nauto_summary %>% gather(stat, val) %>%\r\n                    separate(stat, into = c(\"var\", \"stat\"), sep = \"_\") %>%\r\n                    spread(var,val) \r\n\r\n\r\n  stat       mpg    weight\r\n1  mmm 21.297297 3019.4595\r\n2  p50 20.000000 3190.0000\r\n3   sd  5.785503  777.1936\r\n\r\n\r\nExercise 2.2\r\nThis github repository provides COVID related data for the US at the state and county level: https://github.com/nytimes/covid-19-data\r\nPart(a)\r\nDownload the state level data (in csv format) from\r\nhere and load it into an R dataframe. Have a look at the dataframe. How many rows are there? What do the rows represent? What do the columns represent?\r\n\r\n\r\nAnswer:\r\n\r\nTo load the dataset into a dataframe you can use:\r\n\r\n\r\ncovid=read.csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\")\r\n\r\n\r\n\r\nYou can view the dataframe like an excel table by clicking on the name of the dataframe in the Global Environment tab, which also shows us the number of rows and columns of the dataframe (note that you might have a different number of rows as the dataframe is constantly updated online)\r\n\r\n\r\n\r\nWe see that the dataframe contains daily COVID case and death numbers for every us state. Note that the numbers never reduce which suggests that these are cumulative figues (i.e. total number of cases and deaths up until a give day)\r\n\r\nPart(b)\r\nCompute the death rate (deaths over cases) for every state by July 31st, 2020 using R. Also add columns that contain the share of deaths in total deaths and the share of cases in total cases\r\n\r\n\r\nAnswer\r\n\r\nWith filter() we can select specific rows of a dataframe. With mutate() we can create new variables in dataframe or modify exisitng ones. Note that the date variable is a factor variable. You can see this in the Global Environment tab:\r\n\r\n\r\n\r\nWe will talk a lot about factor variables later in the course. For now it’s enough to know that with the function as.character() you can convert the date variable from a factor to a character string which is necessary to compare it to character string such as “2020-08-01”. As ever you can only compare like with like.\r\nHence, the following comand provides the result (note I am loading again the library dplyr which is not necessariy if you have done that already in this session)\r\n\r\n\r\nlibrary(dplyr)\r\n\r\n\r\ncovid_july=covid %>% dplyr::filter(as.character(date)==\"2020-07-31\") %>%\r\n          mutate(deathsOcases=deaths/cases,\r\n                 deaths_sh=deaths/sum(deaths),\r\n                 cases_sh=cases/sum(cases))\r\n\r\nhead(covid_july,10)\r\n\r\n\r\n         date                state fips  cases deaths deathsOcases\r\n1  2020-07-31              Alabama    1  87723   1580  0.018011240\r\n2  2020-07-31               Alaska    2   3675     21  0.005714286\r\n3  2020-07-31              Arizona    4 174108   3695  0.021222460\r\n4  2020-07-31             Arkansas    5  42511    453  0.010656065\r\n5  2020-07-31           California    6 502273   9222  0.018360533\r\n6  2020-07-31             Colorado    8  46948   1841  0.039213598\r\n7  2020-07-31          Connecticut    9  49810   4432  0.088978117\r\n8  2020-07-31             Delaware   10  14788    585  0.039559102\r\n9  2020-07-31 District of Columbia   11  12126    585  0.048243444\r\n10 2020-07-31              Florida   12 470378   6842  0.014545748\r\n      deaths_sh    cases_sh\r\n1  0.0102689423 0.019188397\r\n2  0.0001364859 0.000803864\r\n3  0.0240150265 0.038084122\r\n4  0.0029441967 0.009298792\r\n5  0.0599368265 0.109866440\r\n6  0.0119652676 0.010269335\r\n7  0.0288050331 0.010895364\r\n8  0.0038021084 0.003234705\r\n9  0.0038021084 0.002652423\r\n10 0.0444684198 0.102889776\r\n\r\nThe head() command shows you the first couple of lines of a dataframe. That is useful if you wann a have a peek to examine larger dataframes.\r\n\r\nPart(c)\r\nWhich is the state with the higest death rate (by July 31)? Can you work it out with an R command (rather than just looking through the table)?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\ncovid_july %>% filter(deathsOcases==max(deathsOcases))\r\n\r\n\r\n        date       state fips cases deaths deathsOcases  deaths_sh\r\n1 2020-07-31 Connecticut    9 49810   4432   0.08897812 0.02880503\r\n    cases_sh\r\n1 0.01089536\r\n\r\n\r\nPart(d)\r\nCan you compute daily figures of deaths and cases for the US as a whole from the state level data? We haven’t discussed all commands necessary for that yet. But see if you can work it out from checking online. If that takes too much time just check the answer below. As ever, ther are many different ways how could do that.\r\n\r\n\r\nAnswer:\r\n\r\nWe can use the group_by() command which is par of dplyr. It allows to group rows of a dataframe by one (or several) categorical variables. We can then apply other commands that will be done seperately for each of those groups. In the case below we want to sum over all states for every day:\r\n\r\n\r\nusdaily=covid %>% group_by(date) %>% summarise(deaths=sum(deaths),cases=sum(cases))\r\nhead(usdaily)\r\n\r\n\r\n# A tibble: 6 x 3\r\n  date       deaths cases\r\n  <chr>       <int> <int>\r\n1 2020-01-21      0     1\r\n2 2020-01-22      0     1\r\n3 2020-01-23      0     1\r\n4 2020-01-24      0     2\r\n5 2020-01-25      0     3\r\n6 2020-01-26      0     5\r\n\r\n\r\nPart(e)\r\nExamine the US death rate over time. How would you expect this series to behave? How does it behave? How do you explain what you find?\r\n\r\n\r\nAnswer:\r\n\r\nThe best way to do this is with a figure. We shall be talking in the next lecture a lot more about plotting. However, no harm done trying this out. There are a number of commands to plot something. However, the most versatile is the ggplot command. (You need to install and load the ggplot2 package before you can use it.). Also: things will look better if you tell R that the date column is date. We can do this with the as.Date() command.\r\nCouple of words about the ggplot command:\r\nYou start by telling R which dataframe to use (usdaily)\r\nwith aes() for aesthetic you declare which variables to use for x and y axis.\r\nBy adding further commands with a + sign you control how things look; e.g. that you want to see points (geom_point()) and that you want monthly date labels.\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nusdaily=usdaily %>% mutate(date=as.Date(date), deathsOcases=deaths/cases)\r\nggplot(usdaily, aes(x=date, y=deathsOcases)) +\r\n      geom_point() + \r\n      scale_x_date(date_breaks=\"1 month\",date_labels = \"%b\")\r\n\r\n\r\n\r\n\r\nNow what do we expect to find? To die from COVID you have te get sick and it will take at least a couple of days. Hence, we would expect to see death rates to be low at first and then to increase. Over time we would hope that doctors get better at treating COVID patients. Also, people who know they are at risk will increasingly shielding so that the only people who get sick will be those who are less likely to die. Both factors should bring the death rate down after an initial peak. This is what we are seeing when only looking at the series from the end of March onwards with a peak in mid May. However, there is a first peak in early March. What could explain this? The most plausible explanation is probably measurement error: early on cases of infections where probably not counted properly as there was no systematic way of testing the population. On the other, if somebody was so ill that they die this would almost certainly be picked up by the authorities. Hence death figures were not under counted. Also, early on numbers were low. So a couple of counts too few for cases could easily swing the ratio. Here is some further discussion of this.\r\n\r\nPart(f)\r\nA large part of data analysis work is getting the data into a format that lends itself to analysis. And a very big part of that is the process of combining datasets from different sources. Here we start learning how this can be done in an efficient way. Here you find a dataset in csv format with information on the size (in terms of population) of different US states. Can you combine this with the COVID case count dataframe for US states at the end of July that we created earlier?\r\n\r\n\r\nAnswer:\r\n\r\nFirst you need to load the population dataset into a dataframe as well:\r\n\r\n\r\npop=read.csv(\"https://www.dropbox.com/s/tp4kiq8if372rcz/populationdata.csv?dl=1\")\r\n\r\n\r\n\r\nTo combine the two dataframes we can use a command called merge():\r\n\r\n\r\ncovid_julyb=covid_july %>% merge(pop,by=\"state\", all.x=TRUE)\r\n\r\n\r\n\r\nThis tells R to create a new dataframe (covid_julyb) by combinig the information in covid_july and pop. Rows are being matched using the variable state. Maybe you noted that pop has only 52 observations whereas covid_july has 55. This is because pop doesn’t include information on places such as Guam and a couple of other Islands that are not formal US states but controlled by the US, whereas covid_july does. The all.x option tells R to keep observations from the first dataset (i.e. covid_july) even if no equivalent observation in the second dataset can be found. This is also known as a left join. Similarly there is a right join with all.y=TRUE or a full join with all=TRUE. You might want to read a bit more about the merge command e.g. here\r\n\r\nPart(g)\r\nUsing the newly created dataframe create a new variable COVID cases per capita (i.e. per head of population). Explore the relationship between per capita cases and population density. What kind of relationship would you expect? What do you find?\r\n\r\n\r\nAnswer:\r\n\r\nAdd per capita cases with mutate (to make things easier to read we use cases per 1000 of population):\r\n\r\n\r\ncovid_julyb=covid_julyb %>% mutate(casesOpop000=cases/pop*1000)\r\n\r\n\r\n\r\nWe would we expect a positive relationship between per capita cases and density: if\r\nA scatter plot is usually the best way to get going on a question like this. We can once more use the ggplot command (geom_smooth(method=lm) adds a regression line):\r\n\r\n\r\nusdaily=usdaily %>% mutate(date=as.Date(date), deathsOcases=deaths/cases)\r\nggplot(covid_julyb, aes(x=density, y=casesOpop000 ))+ \r\n      geom_point() + geom_smooth(method=lm)\r\n\r\n\r\n\r\n\r\nDropping the outlier on the right (i.e. Washington DC which is not really a State)\r\n\r\n\r\nggplot(covid_julyb %>% filter(density<6000), aes(x=density, y=casesOpop000 ))+ \r\n      geom_point() + geom_smooth(method=lm)\r\n\r\n\r\n\r\n\r\nHence, there is clearly a positive relationship between density and covid cases. It becomes more pro-nounced when dropping Washington DC. This is also confirmed when looking at the parameters of the regression line directly, which we can do with the lm() command.\r\n\r\n\r\nsummary(lm(casesOpop000~density,covid_julyb  %>% filter(density<6000) ))\r\n\r\n\r\n\r\nCall:\r\nlm(formula = casesOpop000 ~ density, data = covid_julyb %>% filter(density < \r\n    6000))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-10.6365  -3.1810  -0.4811   3.3123  14.2090 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 10.103760   0.970901  10.407 5.27e-14 ***\r\ndensity      0.006921   0.002751   2.516   0.0152 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 5.466 on 49 degrees of freedom\r\nMultiple R-squared:  0.1144,    Adjusted R-squared:  0.0963 \r\nF-statistic: 6.328 on 1 and 49 DF,  p-value: 0.01521\r\n\r\n\r\nPart (h)\r\nComputers - unlike humans - are good at doing repeated tasks reliably without frustration. A basic way to accomplish that in a programming language are are loops. Let’s look at a concrete example. Suppose you want to create time series figures of COVID death rates for several US states: California, Texas, New York and Alaska, say. Try if you can work out how to do that with a for loop using the help function and online googling. If your are stuck check out the answer below.\r\n\r\n\r\nAnswer:\r\n\r\nLet’s start by arranging the states in a collection. Also add a death rate column to the covid dataframe (and clear up the date variable):\r\n\r\n\r\nstates=c(\"California\",\"Texas\",\"New York\",\"Alaska\")\r\n\r\ncovid=covid%>% mutate(deathsOcases=deaths/cases, date=as.Date(date))\r\n\r\n\r\n\r\nNow loop over those states:\r\n\r\n\r\nfor(s in states){\r\n  print(s)\r\n  p=ggplot(covid  %>% filter(state==s), aes(x=date, y=deathsOcases)) +\r\n      geom_point() + ggtitle(s)\r\n      scale_x_date(date_breaks=\"1 month\",date_labels = \"%b\")\r\n  \r\n  print(p)\r\n}\r\n\r\n\r\n[1] \"California\"\r\n\r\n[1] \"Texas\"\r\n\r\n[1] \"New York\"\r\n\r\n[1] \"Alaska\"\r\n\r\n\r\nIt’s interesting to see quite a variation in the shape of this series between different states. New York hasn’t managed a meaningful decline in death rates. In Texas on the other hand death rates are increasing again after a substantial decline in July.\r\n\r\nExercise 2.3\r\nPart (a)\r\nBesides analysing data we can use R to make up data. Why would we do that? It’s a common approach called “Monte-Carlo Analysis”. If we make up the data we know the underlying model that is driving the data. We can use to this to test the methods we are using. Do they lead us to the true underlying model or do they have biases that give us the wrong answer? In this exercise we walk through how this can be done in a simple case. As ever: try to work through all of this while creating an R script or R markdown file. First we need to decide about the sample size. Let’s use a sample of size 100. It makes sense to assign this number to a variable as follows:\r\n\r\n\r\nobs<-100\r\n\r\n\r\n\r\nAssume that the data is driven by the following true model \\(Y=2+0.5 X +\\epsilon\\) where \\(\\epsilon\\) is a random variable that follows a standard normal distribution; i.e. its distribution follows the famous bell curve. Using R you can draw realisations of such a random variable using the function rnorm() To assign those values to a variable called eps we can write\r\n\r\n\r\ndf=data.frame(eps=rnorm(obs))\r\n\r\n\r\n\r\nwhere the data.frame command arranges the newly created variable eps into a dataframe rather than being standalone.\r\nSimilarly we can generate the explanatory variable X as:\r\n\r\n\r\ndf=df%>% mutate(X=runif(obs))\r\n\r\n\r\n\r\nThis will draw the values of X by drawing random variables on the interval between 0 and 1 with uniform (i.e. all values have the same) probility. Look at the values of X and \\(\\epsilon\\). Here are the first 10:\r\n\r\n\r\nhead(df,10)\r\n\r\n\r\n          eps         X\r\n1  -1.2900028 0.5958205\r\n2   0.8168841 0.5829558\r\n3   1.2326044 0.5726064\r\n4   0.3055738 0.6577908\r\n5  -0.3046453 0.9053362\r\n6   1.7298321 0.1800592\r\n7   0.9339064 0.3002180\r\n8  -0.6631717 0.3234343\r\n9  -0.1526502 0.3775629\r\n10  1.3318213 0.8236410\r\n\r\nPart (b)\r\nNow create the Y variable according to the formula above.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\ndf=df%>% mutate(Y=2+0.5*X+eps)\r\n\r\n\r\n\r\n\r\nPart (c)\r\nRun a regression of Y on X. What value do you find for the coefficient on X? Would you think this is an unbiased estimate?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n    reg=lm(Y~X,df)\r\n    summary(reg)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Y ~ X, data = df)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.37013 -0.54542  0.05759  0.65863  2.52561 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   2.4044     0.2211   10.88   <2e-16 ***\r\nX             0.1349     0.3746    0.36     0.72    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.95 on 98 degrees of freedom\r\nMultiple R-squared:  0.001321,  Adjusted R-squared:  -0.008869 \r\nF-statistic: 0.1296 on 1 and 98 DF,  p-value: 0.7196\r\n\r\n\r\nPart (d)\r\nNow use a “for”-loop to repeat the steps from section (a)-(c) 1000 times.\r\nWe can do this in R with the following for-loop command\r\nfor(i in 1:1000) {\r\n<several lines of code >\r\n}\r\ni.e. the lines of code between the first line and } will be repeated 1000 times. The variable i will increase by 1 on each round.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nbeta <- double(1000)\r\nfor(i in 1:1000){\r\n    \r\n    #print(i)\r\n    \r\n    df=data.frame(eps=rnorm(obs))\r\n    df=df%>% mutate(X=runif(obs))\r\n    df=df%>% mutate(Y=2+0.5*X+eps)\r\n    #print(lm(Y~X,df)$coefficients[[2]])\r\n    beta[i]<-lm(Y~X,df)$coefficients[[2]]\r\n}\r\n\r\n\r\n\r\nNote: with $coefficients[[2]] we extract the coefficient estimate of the second (i.e. the slope) coefficient from the regression results and store it in a vector called beta\r\n\r\nPart (e)\r\nCompute the average value of the estimate of the X coefficient across the 1000 repetitions.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nmean(beta)\r\n\r\n\r\n[1] 0.5226581\r\n\r\nresults=data.frame(beta)\r\nggplot(results, aes(x=beta)) + \r\n       geom_histogram(color=\"black\",fill=\"white\",bins=40) + \r\n       theme_minimal()\r\n\r\n\r\n\r\n\r\nNote: we can use ggplot to draw a histogram of all the results for beta as well. This shows that while we not always get a value close to 0.5, over many replications the value we find are centered around the true value of 0.5\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/exercises/exercises2/../../../images/fileimport.png",
    "last_modified": "2021-10-04T16:17:40+01:00",
    "input_file": {},
    "preview_width": 838,
    "preview_height": 643
  },
  {
    "path": "posts/quickguides/quickguide_Rcommands/",
    "title": "Quick Guide - Glossary of R commands",
    "description": {},
    "author": [
      {
        "name": "Ralf Martin",
        "url": {}
      }
    ],
    "date": "2021-09-28",
    "categories": [
      "Quickguides"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n  /* Rearrange console label */\r\n  .datacamp-exercise ol li, .datacamp-exercise ul li {\r\n    margin-bottom: 0em !important;\r\n  }\r\n  \r\n  /* Remove bullet marker */\r\n  .datacamp-exercise ol li::before, .datacamp-exercise ul li::before {\r\n    content: '' !important;\r\n  }\r\nabline()\r\n\r\n\r\nMore…\r\n\r\nWith a function like ggplot() you have an endless range of possilities for add-ons. The abline command adds a straight line to a scatterplot.\r\n\r\nas.Date()\r\n\r\n\r\nMore…\r\n\r\nSometimes when you upload data for the first time into R and you have a date variable, R will not understand that it is a date. To plot etc., you will need to convert it to a date variable.\r\nE.g.\r\nmydates <- as.Date(c(“2007-06-22”, “2004-02-13”))\r\n\r\nc()\r\n\r\n\r\nMore…\r\n\r\nThis is a function to tie together (or ‘concatenate’) objects, e.g. x = c(3,5,8,9) or y = c(”Jack”,”Queen”,”King”).\r\n\r\ncor()\r\n\r\n\r\nMore…\r\n\r\nProduces a correlation matrix.\r\n\r\nboxplot()\r\n\r\n\r\nMore…\r\n\r\nProduces boxplots with whiskers - which are sometimes neat for showing the structure of a variable. The syntax is: boxplot(x,main=“title”)\r\n\r\nfactor()\r\n\r\n\r\nMore…\r\n\r\nConverts a numeric or string variable into a categorical variable. This is useful - for instance - to create sets of dummy varaiables based on a categorical variable.\r\n\r\nfor()\r\n\r\n\r\nMore…\r\n\r\nRepeating a series of commands several times\r\nExample\r\n\r\n\r\n  \r\n  for(ii in 1:10){\r\n     cat(\"Round\")\r\n     cat(ii)\r\n     cat(\"\\n\")\r\n  }\r\n  \r\n\r\n More Information\r\n\r\ngetwd()\r\n\r\n\r\nMore…\r\n\r\nPrints current working directory.\r\n\r\nggplot()\r\n\r\n\r\nMore…\r\n\r\nGeneral purpose plotting command.\r\n\r\ngroup_by()\r\n\r\n\r\nMore…\r\n\r\nDefines groupings within a dataframe based on one or several categorical variables. Useful to compute statistics at the level of these groupw\r\n\r\nhead()\r\n\r\n\r\nMore…\r\n\r\nShows the first couple of lines in a dataframe\r\n\r\n\r\nrdf=data.frame(v1=runif(100),v2=runif(100))\r\nhead(rdf)\r\n\r\n\r\n          v1         v2\r\n1 0.02305153 0.12122816\r\n2 0.40087975 0.24056851\r\n3 0.27557525 0.52671220\r\n4 0.27287104 0.98594268\r\n5 0.53335167 0.38574363\r\n6 0.27718645 0.01763647\r\n\r\n\r\nhelp()\r\n\r\n\r\nMore…\r\n\r\nThis command will fetch a help sheet on the function specified in the brackets, which includes information about usage and syntax.\r\n\r\nhist()\r\n\r\n\r\nMore…\r\n\r\nProduces a histogram - which can be useful when you’re learning more about your variables and want to assess, for instance, if you should log them.\r\n\r\ninner_join()\r\n\r\n\r\nMore…\r\n\r\nCombine 2 dataframe on the basis of common key variable. Inner join only keeps observations with information in both dataframes. See also left_join(), full_join(), etc. More information\r\n\r\ninstall.packages()\r\n\r\n\r\nMore…\r\n\r\nWill install a programming package. The name of the package in the backets must be in quotation marks.\r\n\r\nivreg()\r\n\r\n\r\nMore…\r\n\r\nInstrumental variable 2 stage least squares regression\r\n\r\nlibrary()\r\n\r\n\r\nMore…\r\n\r\nLoads an extension package into memory.\r\nExample\r\n\r\n\r\nlibrary(car)\r\n\r\n\r\n\r\nLoads the car library that allows you to run the linearHypothesis() command (and more).\r\nMore Information\r\n\r\nlinearHypothesis()\r\n\r\n\r\nMore…\r\n\r\nGeneric function for testing a linear hypothesis. The car package needs to be installed and loaded for it to work.\r\n\r\nlm()\r\n\r\n\r\nMore…\r\n\r\nImplements the a linear regression model using the least squares algorithm\r\nExample\r\n\r\n\r\n  \r\n    library(AER)\r\n    library(dplyr)\r\n    data(\"Affairs\")  # Loads `Affairs` dataframe into memory (part of AER library)\r\n    \r\n    reg=lm(affairs~age+gender,data=Affairs)\r\n    \r\n    reg %>% summary()\r\n  \r\n  \r\n\r\n\r\n\r\nlog()\r\n\r\n\r\nMore…\r\n\r\nYou will sometimes need to transform your variables into natural logs (as explained in the lectures). The log() fundction transforms a variable into a natural logarithm (i.e. base e).\r\n\r\nprop.table()\r\n\r\n\r\nMore…\r\n\r\nWill generate a table of proportions. prop.table(table(data\\(var1,data\\)var2)) divides each cell by the total of all cells, while the command prop.table(table(data\\(var1,data\\)var2),1) divides each cell by the total of its row and prop.table(table(data\\(var1,data\\)var2),2) by the total of its column.\r\n\r\nread.csv()\r\n\r\n\r\nMore…\r\n\r\nReads csv files (comma separated value; i.e. a basic table format) from your harddrive or the web into an R dataframe.\r\nExample\r\n\r\n\r\ndf=read.csv(\"https://www.dropbox.com/s/a2opu10e2hz0dps/brexit.csv?dl=1\")\r\n\r\n\r\n\r\nLoads the brexit.csv dataset\r\n\r\nread_excel()\r\n\r\n\r\nMore…\r\n\r\nThis command will load an excel spreadsheet. Needs to be preceded by library(readxl). The data file name inside the brackets must be in quotation marks.\r\n\r\nseq()\r\n\r\n\r\nMore…\r\n\r\nCreate a sequence of numbers; e.g.\r\n\r\n\r\nseq(0,20,2)\r\n\r\n\r\n [1]  0  2  4  6  8 10 12 14 16 18 20\r\n\r\n\r\nsetwd()\r\n\r\n\r\nMore…\r\n\r\nWill set working directory. E.g. setwd(“c:/folder/folder2”)\r\n\r\nstargazer()\r\n\r\n\r\nMore…\r\n\r\nPossibly the most amazing function in R. Will let you produce very nice looking descriptive statistics tables and regression tables.\r\n\r\nstr()\r\n\r\n\r\nMore…\r\n\r\nWill display the structure of an R object.\r\n\r\nsubset()\r\n\r\n\r\nMore…\r\n\r\nThe subset( ) function is the easiest way to select variables and observations.\r\nE.g. In the following example, we select all rows that have a value of age greater than or equal to 20 or age less then 10. We keep the ID and Weight columns.\r\nnewdata <- subset(mydata, age >= 20 | age < 10, select=c(ID, Weight))\r\n\r\nsummarise()\r\n\r\n\r\nMore…\r\n\r\nCompute summary statistics\r\n\r\nsummary()\r\n\r\n\r\nMore…\r\n\r\nGeneral command to provide summary information about an object.\r\n\r\ntable()\r\n\r\n\r\nMore…\r\n\r\nThis command will generate a contingency table. The first variable will show the levels of the categorical variable as rows and the second variable will display the levels of the categorical variable as colums.\r\nE.g. table(data\\(call,data\\)black) will return a 2x2 table where callback frequency is given by the rows (1=callback) and race frequency is given by the columns (1=black).\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/quickguides/quickguide_Rcommands/R.png",
    "last_modified": "2021-10-04T16:17:40+01:00",
    "input_file": {},
    "preview_width": 559,
    "preview_height": 397
  },
  {
    "path": "posts/exercises/exercises1/",
    "title": "Exercises 1",
    "description": "Starting to think like an econometrician.",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2021-09-27",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\nIn the lecture we discussed the notion of causality. We contrasted the causality mechanism that you might be most interested in (your story) with the causality mechanism(s) that might be driving your data (the data story). For good data-analysis you need to be aware of both: have a clear idea of what you want to analyse or explore as well as of all the things that could be going on in your data. To practise this I ask you to go through a couple of examples. These are suggestive causal relationships typically found on the basis of some correlation in in one dataset or another. In each case:\r\nCan you think of a mechanism that would support the causal relationship suggested?\r\nCan you also think of at least one reason of why the relationship might have been the consequence of confounding drivers in the data or indeed reverse causality? As consequence the effect suggested might be either non-existent or biased?\r\nCan you also discuss the direction of the bias that would be introduced by your suggestion?\r\nFor each case I give you a suggestion as example when clicking the answer tab. You might also come across some interesting causal claims that are made e.g. in the news. If so you are very welcome to share those on the course forum.\r\n\r\nExercise 1.1\r\nPolice body worn cameras increase violence against the police?\r\n\r\n\r\n\r\n(For the full story, visit: https://www.govtech.com/public-safety/study-finds-negative-effects-of-police-worn-body-cameras.html)\r\n\r\n\r\nExample Answer:\r\n\r\nWe would typically expect the opposite; i.e. if people are on camera it is more likely that they can be prosecuted and punished for attacking a police officer. But as I suggested: a key skill of an applied data analyst is to think of imaginative stories. So here we can test that skill: Maybe there are people who like a reputation as misfits. Maybe that’s a badge of honour in their peer group. Now what could be better for those people than being known and seen on camera for attacking a police officer? Another story could be as follows: perhaps having a camera changes the behaviour of the officers. They now feel more confident - indeed perhaps overconfident - to engage with people that are more prone to attack officers.\r\nOn the other hand: Maybe the story of the data behind these results is another one. Perhaps the introduction of cameras let violence against police officers to be better measured. Maybe police officers previously did not report all violence against them as doing so would require them to substantiate their claims which is harder if you don’t have a video record. With that available reporting went up but not actual incidence of violence. This “data story” would imply an upward bias on the actual impact of cameras on violence.\r\n\r\nExercise 1.2\r\nGuacamole makes you more succesful at online dating?\r\n\r\n\r\n\r\n\r\n\r\n\r\nExample Answer:\r\n\r\nThe headline suggests that there is a positive causal effect from mentioning Guacamole to receiving responses in online dating. A driver for this is could be that Guacamole is a signal for a healthy lifestyle and health and fitness are qualities that are desired when looking for a romantic partner (The story of the article)\r\nThe story of the data could be another:\r\nPerhaps younger people are more likely to be into guacamole. But younger people can also be expected to get more dating responses. This would imply an upward bias of the Guacamole->Dating success effect.\r\nA taste for guacamole could be more prevalent among people living in cities. People are more likely to respond to people that are close by. Hence, people in cities will have more people close by and therefore receive more responses. Again this implies an upward bias.\r\n\r\nExercise 1.3\r\nGoing to the Museum makes you live longer?\r\n\r\n\r\n\r\n(For the full story, visit: https://www.nytimes.com/2019/12/22/us/arts-health-effects-ucl-study.html)\r\n\r\n\r\nExample Answer:\r\n\r\nIt’s conceivable that visiting a museum calms you down, allows time for reflection and leas to insights that help you to be more healthy and less stressed and as a consequence makes you live longer.\r\nEqually, museum visits could simply be conflated with other more clear-cut factors that make you live longer. Income and education would be clear candidates. That said, the study quoted in the article has already accounted for these. But there are other factors that are not easily controlled for. For instance, having a stressful job with little spare time it probably not good for both finding time to go the museum as well as for your life expectancy. This mechanism would bias the museum -> life expectancy effect upward (stressful job is negatively correlated with both museum visits and life expectancy).\r\n\r\nExercise 1.4\r\nEating chocolate helps you win a Nobel?\r\n\r\n\r\n\r\n\r\n\r\n\r\nExample Answer:\r\n\r\nA New York doctor wanted to investigate the effect of flavanols, compounds found in chocolate but also tea and red wine, on cognitive ability. He used the number of Nobels as a convenient proxy for his outcome variable and country-level chocolate consumption data as his explanatory variable and got a highly significant positive result. Before you rush to the shop for your bar of Cadbury’s though, think of what else might be driving the result. It might well be that we have an omitted variable here - wealth - which drives that positive relationship. A richer country (like Switzerland, which has 26 Nobel winners) will have more resources to invest in research and its affluent citizens might be more likely to be able to treat themselves frequently to chocolate. It might also be that people who study (and are therefore more likely to get a Nobel, of course), need a sugar fix more often and snack more. In both of these cases we’d have an upwards bias as wealth and time spent studying are positively correlated with both chocolate consumption and the number of Nobel prizes.\r\n\r\nExercise 1.5\r\nSex makes you rich?\r\n\r\n\r\n\r\n\r\n\r\n\r\nExample Answer:\r\n\r\nInstitute for the Study of Labor based in Bonn, Germany, published a study examining the “The Effect of Sexual Activity on Wages” in their discussion paper series in 2013. Sure, it might be the case that having sex triggers a rush of hormones, which then make you more productive and give you an edge in the workplace. But it is perhaps more plausible (and this is in fact what the original study claims) that sex is another indicator of health, which is our omitted variable. In this case, we would have an upwards bias.\r\nAnother possibility is that there could be a measurement error. The data comes from a survey and you should always ask yourself whether it might be the case that respondents could systematically misrepresent certain information. Other studies have found that when it comes to sex, men often over-report how much of it they’re having while women under-report. If the sample was not gender-balanced we may get upwards or downwards bias, depending on which gender prevails.\r\n\r\nExercise 1.6\r\nMarriage makes you happy?\r\n\r\n\r\n\r\nFull article\r\n\r\n\r\nExample Answer:\r\n\r\nAnd they lived happily ever after…or did they? This study from a 2006 issue of the Journal of Socio-Economics actually does not make any grand claims at the outset but instead sets out to investigate which way the causal story is going: is it that happier people get married or that marriage actually makes people happy? Reverse causality is a tricky nut to crack. This study finds evidence that happier singles opt more frequently for marriage and that benefits of marriage vary widely among couples – not surprisingly an equal division of labour at home is an important factor driving happiness in marriage.\r\n\r\nExercise 1.7\r\nSleeping with a night light as a kid makes you blind later?\r\n\r\n\r\n\r\n\r\n\r\n\r\nExample Answer:\r\n\r\nShort-sightedness is a growing problem globally. Researchers at the Children’s Hospital of Philadelphia thought they found the culprit: night-lights. The data, at face value, seemed to have suggested that kids who sleep with night lights develop myopia later in life. According to the researchers, the story was that the light makes the eye develop abnormally, which then affects focus. There could be a more obvious explanation, however, which is that myopia is hereditary – myopic parents are more likely to install night lights in the house, including their children’s bedrooms, but the light itself has nothing to do with their kids becoming short-sighted. This is, again, a case of upwards bias as myopia in parents is positively correlated with the presence of night lights in the house and with myopia in their children.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/exercises/exercises1/Mente.png",
    "last_modified": "2021-09-28T17:34:39+01:00",
    "input_file": {},
    "preview_width": 2795,
    "preview_height": 3207
  },
  {
    "path": "posts/exercises/exercises3/",
    "title": "Exercises 3",
    "description": "How to have visualisations",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\nYour task this week is to get some data from somewhere and create a figure or visualisation in R along with a discussion of what we might be seeing. The R Markdown for lecture 3 provides you with some starting points. You find the Rmd code here and the compiled html output here\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-09-14T16:43:37+01:00",
    "input_file": {}
  },
  {
    "path": "posts/exercises/exercises10/",
    "title": "Exercises 10",
    "description": "More exercises to help you become an econometrics superstar",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2020-12-18",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\n\r\n\r\n\r\nExercise 10.1\r\nSuppose you estimate the gender difference in returns to education using the following mode:\r\n\\[log(wage)=(\\beta_0+\\delta_0 female)+(\\beta_1+\\delta_1 female) \\times educ + u\\] where wage is the hourly wage, female is a gender dummy, which is equal 1 if the individual is female, and educ is the number of years of education. Provide an interpretation if \\(δ_0<0\\) and \\(δ_1<0\\).\r\n\r\n\r\nAnswer:\r\n\r\n This is a linear model where the intercept men is \\(\\beta_0\\) and for women is \\(\\beta_0+\\delta_0 female\\). The change in log wages for men is \\(\\beta_1\\) whereas for women it is \\(\\beta_1+\\delta_1 female\\). Also note that because we have log wage as dependent variable, these coefficients can be interpreted as percentage changes in wage. If \\(δ_0<0\\) and \\(δ_1<0\\), this means that women earn less for a given level of education and also that the change in wage for a given change in education (i.e. the returns to education) are lower for women.\r\nThe figure below illustrates this model:  Note that \\(\\beta_1\\) represents the slope of the line for men whereas \\(\\delta_1\\) reperesents how much less the line for women is sloped compared to men.\r\n\r\nSomeone asserts that expected wages are the same for men and women who have the same level of education. Referring to the model in part (a), what would be your null hypothesis to test this? How you would test it.\r\n\r\n\r\nAnswer:\r\n\r\nThis would require the joint hypothesis: \\(δ_0=0\\) & \\(δ_1=0\\). This can be implemented via a joint (F-) test; e.g. in R this can be done with the linearHypothesis Command.\r\n\r\nSuppose your estimation returns the following values for the model from part (a):\\(\\hat{δ}_0=-0.1\\), \\(\\hat{δ}_1=-0.01\\). Based on this, what is the expected wage differential between a man and a woman with 10 years of schooling?\r\n\r\n\r\nHint:\r\n\r\nMan with 10 years: \\(E\\{log(wage)|Man\\}=\\beta_0+\\beta_1 \\times educ\\)\r\nWomen with 10 years: \\(E\\{log(wage)|Women\\}=\\beta_0+\\delta_0+(\\beta_1+\\delta_1) \\times educ\\)\r\n\r\n\r\n\r\n\r\nAnswer:\r\n\r\nThe wage differential between a man an women with the same 10 years of education becomes \\[E\\{log(wage)|Man,10years\\} - E\\{log(wage)|Women,10 years\\}=-(\\delta_0+\\delta_1\\times 10)\\] \\[=0.1+10\\times0.01=0.2\\] Thus we would expect the women to have a a 20% lower wage\r\n\r\nSuppose you find in addition that \\(β_1=0.01\\). What does it imply about the effect of 5 years more of education on the expected wage of a woman?\r\n\r\n\r\nAnswer:\r\n\r\nConsequently the effect of education on women’s wages would become \\(β_1+\\delta_1=0.01-0.01=0\\). This would mean that education has no effect on women’s wages.\r\n\r\nSuppose we have estimated the following wage equation \\[W =10+10AGE-0.1AGE^2+ϵ\\] Based on this, at what age would we expect the highest wage?\r\n\r\n\r\nAnswer:\r\n\r\nThe equation describes a hump shaped relationship between wages and age (since the squared term is negative). It therefore makes sense to find the top of the hump which will have an age gradient of 0. The gradient can be found by differentiating with respect to age: \\[\\frac{\\partial W}{\\partial AGE}=10-0.1\\times 2 \\times AGE\\] Thus setting \\(\\frac{\\partial W}{\\partial AGE}\\) equal to zero leads to \\[AGE^{max.wage}=\\frac{10}{0.1\\times 2}=50\\]\r\n\r\nExercise 10.2\r\nConsider the dataset ets_thres_final.csv. It contains emission figures (lnco2=log of CO2 emissions) for a sample of firms regulated by the European Emissions Trading System (EUETS) for the years from 2005 to 2017 although the firm identifiers have gone missing from the dataset. Note that an Emissions Trading System requires firms to buy permits for every unit of CO2 they emit. By restricting the total number of permits that are issued governments can control the total amount of emissions while allowing firms to trade permits freely so that they can be used with those businesses that find it hardest to reduce emissions. In the early days of the EU ETS (which started in 2005) permits where freely given to firms. This changed from 2013 onwards when permits where only given to certain firms and sectors that were deemed at risk from foreign competition. The variable nonfree indicates those firms in the dataset. According to economic theory the method of permits allocation should have no effect on the eventual emissions by firms (Independence hypothesis). Firms that have been given free permits will have an incentive to reduce emissions as that frees up permits to sell within the permit market.\r\nExamine this hypothesis by running a regression of lnco2 on the nonfree variable. Report what you find.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlibrary(dplyr)\r\ndf=read.csv(\"https://www.dropbox.com/s/urro3ty46kr4f7z/ets_thres_final.csv?dl=1\")\r\ndf=df %>% mutate(free=factor(nonfree),period=factor(period))\r\nhead(df)\r\n\r\n  X.1 X year COUNTRY_CODE xCOU period    lnco2 free nonfree\r\n1   1 1 2009           AT    1      1 9.179675    1       1\r\n2   2 2 2010           AT    1      1 9.200492    1       1\r\n3   3 3 2011           AT    1      1 9.326789    1       1\r\n4   4 4 2012           AT    1      1 9.324919    1       1\r\n5   5 5 2013           AT    1      2 9.332470    1       1\r\n6   6 6 2014           AT    1      2 9.424483    1       1\r\n\r\nlm(lnco2~nonfree,df) %>% summary()\r\n\r\nCall:\r\nlm(formula = lnco2 ~ nonfree, data = df)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-9.3522 -0.7237  0.0633  0.8396  6.4050 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  9.85616    0.02427  406.09   <2e-16 ***\r\nnonfree     -0.50395    0.03361  -14.99   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.387 on 6827 degrees of freedom\r\n  (443 observations deleted due to missingness)\r\nMultiple R-squared:  0.03188,   Adjusted R-squared:  0.03174 \r\nF-statistic: 224.8 on 1 and 6827 DF,  p-value: < 2.2e-16\r\n\r\ndf %>% group_by(year) %>% summarise(n())\r\n\r\n# A tibble: 9 x 2\r\n   year `n()`\r\n  <int> <int>\r\n1  2009   808\r\n2  2010   808\r\n3  2011   808\r\n4  2012   808\r\n5  2013   808\r\n6  2014   808\r\n7  2015   808\r\n8  2016   808\r\n9  2017   808\r\n\r\n\r\nProvide an interpretation of the regression coefficient along with a discussion of the implications of your result.\r\n\r\n\r\nAnswer:\r\n\r\nThe firms that stop receiving free permits in 2013 pollute 50% less on average over the 2009 to 2017 period.\r\n\r\nThe variable period is a categorical variable equal to 1 for observations from before 2013 and equal to 2 for observations from year 2013 onward. Convert it into a factor variable and run a regression of lnco2 on period. Provide an interpretation of the estimated coefficients.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlm(lnco2~period,df) %>% summary()\r\n\r\nCall:\r\nlm(formula = lnco2 ~ period, data = df)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-9.5943 -0.7241  0.0457  0.8979  6.6685 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  9.594291   0.025888 370.602   <2e-16 ***\r\nperiod2     -0.001621   0.034425  -0.047    0.962    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.41 on 6827 degrees of freedom\r\n  (443 observations deleted due to missingness)\r\nMultiple R-squared:  3.247e-07, Adjusted R-squared:  -0.0001462 \r\nF-statistic: 0.002216 on 1 and 6827 DF,  p-value: 0.9625\r\n\r\nThis shows that on average emission after 2012 are .16 percent lower than before 2013, a value that is not significantly different from zero.\r\n\r\nWould you say your results in part (a) provide a causal estimate of the effect of not giving free permits?\r\n\r\n\r\nAnswer:\r\n\r\nThe results in part a) confound the treatment effect with any - pre-existing - firm characteristics that might have influenced the allocation of permits. For instance it might well be that the most energy (and therefore pollution) intensive firms were given given an exemption from having to buy all their permits. Hence, firms who have to buy permits (nonfree firms) are those with lower CO2 consumption to begin with.\r\n\r\nWith the data at hand can you propose and implement an alternative regression approach that might address any concerns raised in (d)? If yes, implement this regression and discuss its results. What does the result tell you about the Independence hypothesis discussed in the introduction?\r\n\r\n\r\nAnswer:\r\n\r\nIn the dataset as it is there is actually no variable that properly captures the treatment we are interested in. nonfree identifies firms that are eventually treated (the treatment being the bitter pill of having to pay for all their permits) but it is equal to 1 also in periods when they are not treated. But it is easy to create a dummy variable, which is only equal to one for those firms that treated in periods when they are treated: we simply have to create a dummy variable that is only true for nonfree firms during period 2. Let’s try that:\r\n\r\n\r\ndf=df %>% mutate( period2Xnonfree= (nonfree==1) & ( as.character(period)==\"2\" ) )\r\n\r\nlm(lnco2~period2Xnonfree,df) %>% summary()\r\n\r\nCall:\r\nlm(formula = lnco2 ~ period2Xnonfree, data = df)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-9.7086 -0.7132  0.0611  0.8741  6.5525 \r\n\r\nCoefficients:\r\n                    Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)          9.70864    0.02017  481.34   <2e-16 ***\r\nperiod2XnonfreeTRUE -0.38987    0.03710  -10.51   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.399 on 6827 degrees of freedom\r\n  (443 observations deleted due to missingness)\r\nMultiple R-squared:  0.01592,   Adjusted R-squared:  0.01578 \r\nF-statistic: 110.5 on 1 and 6827 DF,  p-value: < 2.2e-16\r\n\r\nAt face value this would suggest that firms not receiving free permits leads to 41% less CO2 emissions. However, there are at least 2 potential confounding factors: 1. The fact that firms still getting free permits have not been selected at random 2. There might be time effects present. For instance, after 2013 growth might have picked up following the recession of 2008.\r\nWe can control for the first issue by including nonfree as a control variable as (it measures how different the nonfree firms were before they were made to buy all permits). The second issue we can address with a period dummy variable. Hence:\r\n\r\n\r\nlm(lnco2~period+nonfree+period2Xnonfree,df) %>% summary()\r\n\r\nCall:\r\nlm(formula = lnco2 ~ period + nonfree + period2Xnonfree, data = df)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-9.3960 -0.7187  0.0645  0.8428  6.4183 \r\n\r\nCoefficients:\r\n                    Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)          9.80887    0.03675 266.940  < 2e-16 ***\r\nperiod2              0.08385    0.04893   1.714   0.0866 .  \r\nnonfree             -0.41288    0.05097  -8.100 6.45e-16 ***\r\nperiod2XnonfreeTRUE -0.16108    0.06779  -2.376   0.0175 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.387 on 6825 degrees of freedom\r\n  (443 observations deleted due to missingness)\r\nMultiple R-squared:  0.03268,   Adjusted R-squared:  0.03225 \r\nF-statistic: 76.86 on 3 and 6825 DF,  p-value: < 2.2e-16\r\n\r\nHence, this changes the the coefficient for period2Xfree quite a bit; i.e. it would suggest that nonfree permit allocation reduces emissions by 16% only.\r\nAn alternative way of implementing that is via the : operator which interacts (multiplies) variables “on the fly”:\r\n\r\n\r\nlm(lnco2~period+nonfree+period:nonfree,df) %>% summary()\r\n\r\nCall:\r\nlm(formula = lnco2 ~ period + nonfree + period:nonfree, data = df)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-9.3960 -0.7187  0.0645  0.8428  6.4183 \r\n\r\nCoefficients:\r\n                Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)      9.80887    0.03675 266.940  < 2e-16 ***\r\nperiod2          0.08385    0.04893   1.714   0.0866 .  \r\nnonfree         -0.41288    0.05097  -8.100 6.45e-16 ***\r\nperiod2:nonfree -0.16108    0.06779  -2.376   0.0175 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.387 on 6825 degrees of freedom\r\n  (443 observations deleted due to missingness)\r\nMultiple R-squared:  0.03268,   Adjusted R-squared:  0.03225 \r\nF-statistic: 76.86 on 3 and 6825 DF,  p-value: < 2.2e-16\r\n\r\n\r\nExercise 10.3\r\nFor this question use the dataset hals1prep.csv, containing data from the UK Health and Lifestyle Survey (1984-85). In this survey, several thousand people in the UK were being asked questions about their health and lifestyle.\r\nThe variable bmi records the body mass index (BMI) of the respondents. The BMI uses the weight and height to work out whether a weight is healthy or if someone is overweight. A value between 18.5 and 24.9 indicates a healthy weight. Based on the information below, which region of the UK had – on average – the most overweight population? Run a regression of BMI on regional categories (recorded in the variable region). Use this to figure out in which UK regions are on average outside the healthy BMI range .\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nhalsx=read.csv(\"https://mondpanther.github.io/datastorieshub/data/hals1prep.csv\")\r\ntable(halsx$ownh)\r\n\r\n   1    2    3    4 \r\n1850 4563 2076  482 \r\n\r\ntable(halsx$region)\r\n\r\n   east anglia  east midlands greater london          north \r\n           333            682            943            540 \r\n    north west       scotland     south east     south west \r\n          1092            925           1607            720 \r\n         wales  west midlands   yorks/humber \r\n           498            823            808 \r\n\r\n#summary(lm(bmi~ region, halsx))\r\nsummary(lm(bmi~0+ region, halsx))\r\n\r\nCall:\r\nlm(formula = bmi ~ 0 + region, data = halsx)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-12.3808  -2.8505  -0.5398   2.2378  30.3695 \r\n\r\nCoefficients:\r\n                     Estimate Std. Error t value Pr(>|t|)    \r\nregioneast anglia     24.5650     0.2417   101.6   <2e-16 ***\r\nregioneast midlands   24.6908     0.1723   143.3   <2e-16 ***\r\nregiongreater london  24.0111     0.1506   159.4   <2e-16 ***\r\nregionnorth           24.6737     0.1943   127.0   <2e-16 ***\r\nregionnorth west      24.7005     0.1371   180.2   <2e-16 ***\r\nregionscotland        24.9136     0.1504   165.7   <2e-16 ***\r\nregionsouth east      24.0898     0.1107   217.6   <2e-16 ***\r\nregionsouth west      24.7633     0.1694   146.2   <2e-16 ***\r\nregionwales           25.2405     0.2071   121.9   <2e-16 ***\r\nregionwest midlands   24.5064     0.1614   151.8   <2e-16 ***\r\nregionyorks/humber    24.6052     0.1585   155.3   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 4.08 on 7260 degrees of freedom\r\n  (1700 observations deleted due to missingness)\r\nMultiple R-squared:  0.9731,    Adjusted R-squared:  0.9731 \r\nF-statistic: 2.392e+04 on 11 and 7260 DF,  p-value: < 2.2e-16\r\n\r\nIf we drop the intercept by writing 0+... the dummies represent the average BMI values. We that Wales has the highest, with both Scotland and Wales above 24.9 and all other regions within the healthy range.\r\n\r\nThe variable ownh_num records responses to the question “Would you say that for someone of your age your own health in general is…” where users had the following response options:\r\n• Excellent (1)\r\n• Good (2)\r\n• Fair (3)\r\n• Poor (4)\r\nThe numbers in brackets indicate how these options were recorded in the ownh_num variable. Run a regression of ownh_num on bmi and provide a discussion of what you find. Is it in line with your expectations on this?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlm(ownh_num~bmi , halsx) %>% summary()\r\n\r\nCall:\r\nlm(formula = ownh_num ~ bmi, data = halsx)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.4558 -0.2020 -0.1069  0.8016  2.0259 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 1.777801   0.056678  31.367  < 2e-16 ***\r\nbmi         0.014155   0.002278   6.213 5.48e-10 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.7948 on 7269 degrees of freedom\r\n  (1700 observations deleted due to missingness)\r\nMultiple R-squared:  0.005283,  Adjusted R-squared:  0.005146 \r\nF-statistic:  38.6 on 1 and 7269 DF,  p-value: 5.481e-10\r\n\r\nAn increase in the BMI by 1 unit increases the health score 0.014 units. Because a higher value of the score implies worse health this suggests a reduction in health which is line with expectations.\r\n\r\nCan you think of at least two reasons why the estimate in b) does not provide a correct representation of the causal relationship between bmi and health?\r\n\r\n\r\nAnswer:\r\n\r\nThere might be a variety of confounding factors; e.g. richer people might be healthier and less overweight because they can afford higher quality food (making them slimmer) as well as better medical care (making them healthier for reasons unrelated to food intake and weight). Hence, because in this scenario money is negatively correlated with both BMI and the onwnh_num health score this leads to an upward bias; i.e. the coefficient would be lower in reality than what we found.\r\nEducation might play a similar role; i.e. better educated people will be healthier for a range of reasons (e.g. knowledge about health and how get the best care) and the same knowledge might also allow them to eat better and gain less weight.\r\nThere might also be a direct reverse causality: people who are sicker might find it hard to exercise and/or make the effort of doing high quality cooking which would again lead to an upward bias in our regression.\r\nHowever, note that one could imagine that this also goes the other way round: many diseases lead to extreme weight loss which would imply a downward bias in our regression.\r\nAge might be an other issue. Most people get a bit fatter as they age (well at least I do). Now the question asks to consider age when answering the question. However, there might be a systematic bias in how people respond to such questions. E.g. suppose older people tend to get more content than younger people so that they are more often just happy with their health. This would mean that age has a negative effect (more healthy) on the dependent variable. At the same time there is a positive effect on BMI. This would mean a negative correlation between errors and omitted variable implying a downward bias.\r\nAgain the bias could go the other way round if for instance older people are more likely to become hypochondriacs.\r\n\r\nThe dataset includes several additional control variables. These include\r\n• incomeB a categorical variable representing income brackets where “1” represents the lowest and “12” the highest income group. • agyrs – a variable recording the age of the participant\r\nInclude those in the regression of reported health from b) Discuss what the output suggests about the relationships between health and age, and health and income. Are they in line with what you would have expected? In each case can you provide an explanation for the kind of relationship found?\r\nAlso discuss the usefulness of including both the age and income controls for estimating the causal effect of BMI. In each case discuss at least one reason for and one reason against including these controls. [5 points]\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlm(ownh_num~bmi+agyrs+factor(incomeB) , halsx) %>% summary()\r\n\r\nCall:\r\nlm(formula = ownh_num ~ bmi + agyrs + factor(incomeB), data = halsx)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.64142 -0.34612 -0.08213  0.65138  2.18574 \r\n\r\nCoefficients:\r\n                    Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        1.9586826  0.1242120  15.769  < 2e-16 ***\r\nbmi                0.0123175  0.0025664   4.800 1.63e-06 ***\r\nagyrs             -0.0018862  0.0006752  -2.794  0.00523 ** \r\nfactor(incomeB)2   0.1639262  0.1097212   1.494  0.13522    \r\nfactor(incomeB)3   0.1738317  0.1087402   1.599  0.10997    \r\nfactor(incomeB)4   0.0090082  0.1099935   0.082  0.93473    \r\nfactor(incomeB)5  -0.0846940  0.1069890  -0.792  0.42862    \r\nfactor(incomeB)7  -0.2736317  0.1124488  -2.433  0.01499 *  \r\nfactor(incomeB)8  -0.2231460  0.1126696  -1.981  0.04769 *  \r\nfactor(incomeB)9  -0.1422836  0.1116748  -1.274  0.20268    \r\nfactor(incomeB)10 -0.2764668  0.1221840  -2.263  0.02369 *  \r\nfactor(incomeB)12 -0.3545969  0.1246759  -2.844  0.00447 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.7814 on 5699 degrees of freedom\r\n  (3260 observations deleted due to missingness)\r\nMultiple R-squared:  0.03819,   Adjusted R-squared:  0.03633 \r\nF-statistic: 20.57 on 11 and 5699 DF,  p-value: < 2.2e-16\r\n\r\nThe health score for higher income bands is lower suggesting richer people tend to be healthier (or at least report to be healther). This is inline what we would expect: richer people can afford better health care, live in healthier houses, in better neighbourhoods with less pollution etc.\r\nThe relation between age and health seems a bit more surprising as it suggests that older people report being healthier. But we have to remind ourselves that the question asked “how is your health given your age”. Hence, it could mean that older people lower their standards and are more content.\r\nAnother more sinister explanation is the following: suppose each generation has some people that are inherently healthy (e.g. based on their genes) and other that more sickly. Clearly we would expect the healthier to be less likely to die and thereby get older. This would mean that even if people respond in exactly the same to the health question throughout their life the only old people remaining to respond to the survey are the ones that always responded as being in great health.\r\nWe would want to include those variables if there is concern regarding some of the biases discussed in part c). For that they do not only need to have an affect on the dependent variable (health score) but also cause some of the variation in the BMI explanatory variable. See part c) for a more elaborate discussion.\r\nAn important reason not to include those is if we think the causal chain goes the other way round; e.g. it could be that people who are overweight have harder time in the job market making them poorer. Equally, being overweight might affect your chances of survival and thereby your age.\r\n\r\nConsider the R output below. It builds a new dataframe as a transformation the dataframe halsx with the health survey data. ownh_num is defined as in b). Can you provide an interpretation for the coefficients of the linear regression reported at the end of R output? Note that the rbind() command combines dataframes vertically .\r\n\r\n\r\nlabels=c(\"excellent\", \"good\", \"fair\", \"poor\")\r\n\r\nfor(i in 1:4){\r\n  fr=halsx\r\n  fr['dum']=fr$ownh_num==i\r\n  fr['label']=labels[i]\r\n  if(i==1){\r\n     longframe=fr\r\n  }\r\n  else {\r\n    \r\n    longframe=rbind(longframe,fr)\r\n  }\r\n  \r\n  print(nrow(longframe))\r\n\r\n}  \r\n\r\n[1] 8971\r\n[1] 17942\r\n[1] 26913\r\n[1] 35884\r\n\r\nsummary(lm(dum~label,longframe))\r\n\r\nCall:\r\nlm(formula = dum ~ label, data = longframe)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.50864 -0.23141 -0.20622  0.08254  0.94627 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.206220   0.004231   48.74  < 2e-16 ***\r\nlabelfair    0.025192   0.005984    4.21 2.56e-05 ***\r\nlabelgood    0.302419   0.005984   50.54  < 2e-16 ***\r\nlabelpoor   -0.152491   0.005984  -25.48  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4007 on 35880 degrees of freedom\r\nMultiple R-squared:  0.1436,    Adjusted R-squared:  0.1435 \r\nF-statistic:  2005 on 3 and 35880 DF,  p-value: < 2.2e-16\r\n\r\n\r\n\r\nAnswer:\r\n\r\nThe regression reports the share of responses to the health question in the data; i.e. from the intercept we can see that 20.6% of respondents report excellent health. (20.6+2.5)% respond that their health is fair and so on.\r\nWhy is this the case?\r\nFirstly, note that the for loop creates a new dataframe (called longframe) that is basically 5 copies (one for each possible health status answer category) of the original dataframe fr glued together (that’s what the command rbind() does; i.e. combining dataframes vertically). The only thing that is different between the 5 dataframe copies is the newly created variable dum. It is equal to 1 if a respondent responds to the health survey in the way that corresponds to the answer category. With that in mind you need to remember what we learned about dummy variables as dependent and explanatory variables. For instance we said that constant represents the average dependent variable for the reference group. So here this is the observations that are in the dataframe that was created for an response of “excellent” to the health question. So we get the average for the dependent variable which is equal to one for those people who respondent “excellent”. So that is the number of people responding “excellent” dividid by the number of people responding; i.e. the the share that have respondent excellent. For the other groups the coefficient tells us how much higher (or lower) that share is.\r\n\r\nExercise 10.4\r\nAir pollution has been shown to have a variety of adverse health effects. Recently, researchers have also started to investigate other negative effects. Below we report regression tables from a study that investigates a link between air pollution and car accidents.\r\nCan you suggest a causal mechanism that might explain why air pollution could have an effect on car accidents?\r\n\r\n\r\nAnswer:\r\n\r\nAir pollution affects the respiratory system. If people cannot breathe so well that might eventually affect their brain. Drivers might consequently be less able to focus and therefore are more likely to cause accidents. Pollution could also cause poorer visibility that leads to accidents.\r\n\r\nTable 3 below, extracted from an academic paper, reports various regressions of the log number of accidents per day across geographic grid cells for the UK over a period from 2009 to 2014. Column 6 provides a simple OLS regression of accidents on pollution concentration (measured as micro grams per cubic meter of PM). Can you think of reasons why this might not be a valid estimate of the causal impact?\r\n\r\n\r\n\r\nAnswer:\r\n\r\nThere are a number of potentially confounding factors for instance traffic: more traffic will cause more pollution but also makes accidents more likely. Similarly: weather factors such as heat or clouds induce more pollution but could also lead to more (or less accidents). Weather could also work the other way round: rain could reduce pollution, while increasing accidents; i.e. because pollution is negatively correlated with rain and rain positively with accidents we have a downward bias.\r\nMoreover, there could be a direct reverse causality: accidents cause traffic jams which increases pollution.\r\n\r\nColumn 7 of Table 3 in sub-question (b) repeats the same regression including various variables measuring weather conditions as well as region interacted with year, month and day of the week fixed effects/dummies. Would you say this provides a better estimate of the causal effect of pollution? Could it also lead to a worse estimate?\r\n\r\n\r\nAnswer:\r\n\r\nBy including a range of weather controls we address some of the points raised in (b) which should be an improvement. However, a concern with weather variables as controls is that while on the one hand weather can cause pollution and accidents, pollution could also cause the weather (e.g. clouds forming because of particulate pollution) which in turn could affect accidents. The estimate in column 7 would not account for this causal effect.\r\n\r\nThe study proposes an instrument for pollution derived from a weather phenomenon known as temperature inversion. Temperature inversion occurs from time to time when a layer of warmer air sits on top of colder air nearer to the ground. As consequence pollution is trapped near the ground and cannot easily escape. Thus, all else equal, pollution will be more severe near the ground when this happens. Meteorological studies suggest that the phenomenon is driven by wider movements in the atmosphere and crucially is not itself driven by local pollution. Table 2 reports regressions of the pollution variable from Table 3 on a binary variable that is equal to 1 if a temperature inversion is occurring in a particular area at a particular time. Discuss what this table is telling us.\r\n\r\n\r\nAnswer:\r\n\r\nTable 2 reports first stage regressions for this instrument. This allows us to check one of the three criteria for a valid instrument, namely if it is a strong driver of the relevant endogenous variable. This seems to be the case here: not only is the inversion variable significant, the F-statistic is also rather high (larger than 10).\r\n\r\n\r\nColumns 1 to 3 of Table 3 in sub-question (b) report 2 stage least squares regressions using the temperature inversion as instrument. Discuss if this provides a better estimate of the causal effect of pollution on accidents. Can you comment on the relative size of the coefficients comparing columns 1 and 6? Are they in line with what you would expect?\r\n\r\n\r\nAnswer:\r\n\r\n Given that Temperature inversions are likely not driven by pollution this could get round issues such as the traffic-> pollution nexus. However, it is also likely that inversions drive other potentially pollution causing weather events (e.g. clouds, rain). However, we can deal with that by including weather variables (as done in column 3). Of course, the same disclaimer applies as in par c); i.e. we might miss out on parts of the causal effect by doing that. Note that the effect of pollution becomes actually stronger when using the instrument (e.g. compare column 1 and 6, but also 3 vs 7). This suggest that it addresses an endogeneity arising from a negative correlation between un-observed heterogeneity and the endogenous variable in 6; e.g. it could be the case that there are less accidents when traffic goes up (and therefore pollution goes up) because traffic is moving more slower.\r\n The picture below summarises the various issues we discussed in this question:\r\n Firstly, our goal is to identify the causal effect of pollution on accidents. This comprises of direct effect (e.g. via bad visibility) represented by arrow a. This could also include more indirect effects via weather shown as arrows b and c. Simple OLS estimates of accidents on pollution will be biased because of confounding factors such as effects from traffic or weather on both pollution and accidents. Indeed accidents themselves could affect traffic which in turn could affect pollution (arrow i). Controlling for counfounding factors such as traffic or weather can be helpful in finding a non-biased estimate. However, it also could mean that we ignore part of the causal effect we hope to find; e.g. in the figure we would miss out the path shown via arrow b and c if we control for weather. An instrument such as temperature inversion is helpful as it drives pollution and is likely not affected by any endogenous factors (i.e. that ensures criterion 1 and 2 of IV estimation). However, there could be an issue with criterion 3 in an IV regression without further controls because Temperature Inversion is not only having an effect on pollution but it might also cause a range of other weather phenomena. If we include weather as an additional control as in column 3 we will avoid this issue. However, we also might again shut down channel b-c. The good news in Table 3 is that with or without weather variables we find the same effect from pollution which would suggest that channel b-c might not be so relevant.\r\n\r\nExercise 10.5\r\nConsider the dataset back2country_set.dta. It contains data on 71 countries for various 5 year periods from 1992 to 2012 (i.e. the period 2012 refers to the period from 2008 to 2012) Among other variables the dataset contains the following\r\n• en_cleanOdirtyPclean is the share of clean as a fraction of clean and dirty innovations (as measured from patent data).\r\n• social_ht is the share of people in the country that report to favour higher taxes for environmental reasons\r\n• ln_oil_PPP is the log of the country level oil price (inclusive of taxes)\r\n• period is a categorical variable referring to the different 5 year periods.\r\n• ccode contains country codes\r\nNote the dataset for this exercise is in dta (i.e. STATA format). You can load that into R as follows:\r\n\r\n\r\nlibrary(haven)  # this contains the read_dta() to load dta (i.e. STATA) files\r\nb2c=read_dta(\"https://www.dropbox.com/s/oksq88o336w161j/back2country_set.dta?dl=1\")\r\n\r\nRun an OLS regression of the clean innovation share on the social_ht and oil price variables controlling for period effects. Report your regression in your answer. (A copy of your STATA output is sufficient) Based on the regression, what do you expect happens to the share of clean innovations in response to a 5 percentage point increase in in the share of the population supporting higher environmental taxes?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlm(en_cleanOdirtyPclean~social_ht+ln_oil_PPP+factor(period),b2c) %>% summary()\r\n\r\nCall:\r\nlm(formula = en_cleanOdirtyPclean ~ social_ht + ln_oil_PPP + \r\n    factor(period), data = b2c)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.45419 -0.12094  0.01522  0.13474  0.45992 \r\n\r\nCoefficients:\r\n                   Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        -0.49010    0.22027  -2.225   0.0277 *  \r\nsocial_ht           0.02709    0.04747   0.571   0.5691    \r\nln_oil_PPP          0.18331    0.03137   5.843 3.48e-08 ***\r\nfactor(period)1997  0.06825    0.05097   1.339   0.1828    \r\nfactor(period)2002  0.08802    0.05209   1.690   0.0934 .  \r\nfactor(period)2007  0.03250    0.05658   0.574   0.5666    \r\nfactor(period)2012  0.11138    0.07342   1.517   0.1315    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.1872 on 139 degrees of freedom\r\n  (591 observations deleted due to missingness)\r\nMultiple R-squared:  0.3138,    Adjusted R-squared:  0.2842 \r\nF-statistic: 10.59 on 6 and 139 DF,  p-value: 1.135e-09\r\n\r\nsummary(b2c$social_ht)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \r\n  1.871   2.766   3.054   3.041   3.362   4.045     504 \r\n\r\ni.e. The regression suggest a positive (put not significant) coefficient for social attitudes (social_ht) of 0.027. Hence, a 5 percentage point increase in the share of people supporting higher environmental taxes would imply a 0.027 x 5 percentage points= 0.135 percentage point increase in the share of clean innovation.\r\n\r\nCan you suggest at least one mechanism that would motivate a causal effect from the share of support for environmental taxes to the share of clean innovation?\r\n\r\n\r\nAnswer:\r\n\r\nFirms cater to their markets. If a country has more pro environmental voters it will have more pro environmental customers. Firms will respond to that by conducting innovation into products that can be marketed to those customers as pro – environmental.\r\n\r\nDiscuss why or why not the estimate reported above provides a causal estimate of the impact of pro-environmental attitudes.\r\n\r\n\r\nAnswer:\r\n\r\n There are many possible omitted variable or reverse causality stories to be told here; e.g. pro environmental attitudes and stronger focus on clean innovation could be jointly driven by the level of income and development of a country. Success in a particular technology – e.g. clean technologies – might also by itself cause pro environmental attitudes.\r\nAlso higher oil price might be one of the channels via which pro environmental attitudes might affect innovation; e.g. pro environmental attitudes lead to policies such as energy taxes. Hence, including this variable might underestimate the full causal effect from attitudes. Having said that, one question we might have in this research is if attitudes have an impact on the direction of innovation, irrespective of taxes for fuel prices. In that case it would be appropriate to include this control. This shows, nicely that which controls you might want to include depends in part on the what exactly your analysis is trying to do.\r\n\r\nExamine the same relationship as in (a) while including country fixed effects in your regression. What does this regression suggest is the impact of a 10 percentage point change in the support share for environmental taxes?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlm(en_cleanOdirtyPclean~social_ht+ln_oil_PPP+factor(period)+factor(ccode),b2c) %>% summary()\r\n\r\nCall:\r\nlm(formula = en_cleanOdirtyPclean ~ social_ht + ln_oil_PPP + \r\n    factor(period) + factor(ccode), data = b2c)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.23354 -0.04737  0.00000  0.05087  0.20513 \r\n\r\nCoefficients:\r\n                    Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        -0.565119   0.277232  -2.038 0.044200 *  \r\nsocial_ht           0.114364   0.035509   3.221 0.001736 ** \r\nln_oil_PPP          0.121696   0.049067   2.480 0.014835 *  \r\nfactor(period)1997  0.097519   0.027820   3.505 0.000689 ***\r\nfactor(period)2002  0.145598   0.030457   4.780 6.16e-06 ***\r\nfactor(period)2007  0.137128   0.040795   3.361 0.001107 ** \r\nfactor(period)2012  0.241725   0.061112   3.955 0.000145 ***\r\nfactor(ccode)AU     0.178088   0.088830   2.005 0.047742 *  \r\nfactor(ccode)BE     0.077369   0.075707   1.022 0.309323    \r\nfactor(ccode)BG     0.280554   0.086384   3.248 0.001593 ** \r\nfactor(ccode)BR     0.274031   0.102835   2.665 0.009010 ** \r\nfactor(ccode)CA    -0.013153   0.069869  -0.188 0.851072    \r\nfactor(ccode)CH    -0.140932   0.073520  -1.917 0.058160 .  \r\nfactor(ccode)CN     0.225245   0.092335   2.439 0.016508 *  \r\nfactor(ccode)CY    -0.135501   0.114910  -1.179 0.241175    \r\nfactor(ccode)CZ     0.331754   0.080399   4.126 7.74e-05 ***\r\nfactor(ccode)DE     0.068754   0.068500   1.004 0.317994    \r\nfactor(ccode)DK     0.080852   0.079580   1.016 0.312136    \r\nfactor(ccode)ES     0.270450   0.071852   3.764 0.000285 ***\r\nfactor(ccode)FI    -0.142047   0.069755  -2.036 0.044410 *  \r\nfactor(ccode)FR     0.013279   0.075705   0.175 0.861120    \r\nfactor(ccode)GB     0.118911   0.071604   1.661 0.099977 .  \r\nfactor(ccode)GR     0.287559   0.117060   2.457 0.015788 *  \r\nfactor(ccode)HR     0.320115   0.080641   3.970 0.000137 ***\r\nfactor(ccode)HU     0.306553   0.084108   3.645 0.000430 ***\r\nfactor(ccode)ID     0.186398   0.133107   1.400 0.164562    \r\nfactor(ccode)IE     0.174563   0.079000   2.210 0.029457 *  \r\nfactor(ccode)IN     0.042168   0.108593   0.388 0.698629    \r\nfactor(ccode)IT    -0.217889   0.075439  -2.888 0.004768 ** \r\nfactor(ccode)JP     0.301983   0.069631   4.337 3.51e-05 ***\r\nfactor(ccode)KR     0.034720   0.079368   0.437 0.662740    \r\nfactor(ccode)LT     0.206439   0.115510   1.787 0.076998 .  \r\nfactor(ccode)LU    -0.161064   0.110599  -1.456 0.148508    \r\nfactor(ccode)LV     0.478426   0.071231   6.717 1.23e-09 ***\r\nfactor(ccode)MX     0.198181   0.070821   2.798 0.006185 ** \r\nfactor(ccode)NL    -0.070671   0.080835  -0.874 0.384108    \r\nfactor(ccode)NO     0.159818   0.082045   1.948 0.054284 .  \r\nfactor(ccode)NZ     0.220711   0.075119   2.938 0.004116 ** \r\nfactor(ccode)PL     0.317474   0.078613   4.038 0.000107 ***\r\nfactor(ccode)PT     0.340365   0.096022   3.545 0.000604 ***\r\nfactor(ccode)RO     0.267879   0.100362   2.669 0.008902 ** \r\nfactor(ccode)RU     0.252490   0.109050   2.315 0.022678 *  \r\nfactor(ccode)SE    -0.203249   0.085854  -2.367 0.019878 *  \r\nfactor(ccode)SK     0.307865   0.088092   3.495 0.000714 ***\r\nfactor(ccode)TH    -0.009082   0.137805  -0.066 0.947586    \r\nfactor(ccode)TR     0.164923   0.096589   1.707 0.090900 .  \r\nfactor(ccode)US    -0.013032   0.069369  -0.188 0.851369    \r\nfactor(ccode)ZA     0.014158   0.083461   0.170 0.865650    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.0926 on 98 degrees of freedom\r\n  (591 observations deleted due to missingness)\r\nMultiple R-squared:  0.8817,    Adjusted R-squared:  0.8249 \r\nF-statistic: 15.53 on 47 and 98 DF,  p-value: < 2.2e-16\r\n\r\nNote that the coefficient on social_ht is now 0.114. Hence a 10 percentage point increase would lead to a 0.114 x 10 pcp=1.14 pcp increase in the clean innovation share. Note that this is not only larger but also significant.\r\n\r\nDiscuss whether this last regression is better or worse in terms of capturing the causal effect of pro environmental attitudes on clean innovation.\r\n\r\n\r\nAnswer:\r\n\r\n Including country fixed effects will deal with confounding factors that are operating at the country level and are fixed over time. e.g. if the relationship found previously is in part driven by income and relative country incomes haven’t changed that much (while attitudes have) then this last regression might lead to a better – i.e. less biased estimate.\r\n\r\nExercise 10.6\r\nOver recent years the UK has increasingly become more xenophobic. An important question explored by many commentators is the economic damage that this xenophobia will cause. One way to examine this is by looking at the wages of foreign-born workers compared to UK born ones. If wages of foreigners tend to be higher, it is likely that reducing the number of foreigners by terrorising them with hostile immigration procedures – one of Theresa May’s flagship policies - will have negative economic consequences for the native population as well. The dataset [lfsclean].dta(https://www.dropbox.com/s/0mvyckpzsssi5k2/lfsclean.dta?dl=1) contains data from the quarterly labour force survey for the years from 2010 to 2018.\r\nAmong other variables it includes the following • lngrsswk: log of the average weekly gross wage • edu: years in education • foreign: a dummy variable indicating that a person was born abraod • quarter • year\r\nRun a regression of (log) wages on the “foreign” variable. Discuss your findings.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlibrary(haven)\r\nlfs=read_dta(\"https://www.dropbox.com/s/0mvyckpzsssi5k2/lfsclean.dta?dl=1\")\r\n\r\nlm(lngrsswk~foreign,lfs) %>% summary()\r\n\r\nCall:\r\nlm(formula = lngrsswk ~ foreign, data = lfs)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5.9138 -0.4349  0.0939  0.5443  5.4612 \r\n\r\nCoefficients:\r\n            Estimate Std. Error  t value Pr(>|t|)    \r\n(Intercept) 5.877326   0.001468 4003.762   <2e-16 ***\r\nforeign     0.036460   0.003996    9.124   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.8338 on 372925 degrees of freedom\r\n  (2937143 observations deleted due to missingness)\r\nMultiple R-squared:  0.0002232, Adjusted R-squared:  0.0002205 \r\nF-statistic: 83.24 on 1 and 372925 DF,  p-value: < 2.2e-16\r\n\r\nThe regression suggests that foreign workers earn (on average) 3.6% more than natives.\r\n\r\nExplain why we might want to include controls for both year and quarter in this regression? What happens and why if you do?\r\n\r\n\r\nAnswer:\r\n\r\nIt could be the case that there in the regression above the foreign variable is partially endogenous. For instance, it could be the case that foreigners come in time periods when the economy is doing better (both over the years but also within a given year - e.g. for seasonal work) and thus wages in general tend to be higher. Note that this would introduce a positive correlation between “foreign” and shocks to wages which might bias our coefficient upward. By including year and quarter dummies we can account for that. The regression below does that finding a slightly lower (but still significant) of 0.3.\r\n\r\n\r\nlm(lngrsswk~foreign+factor(year)+factor(quarter),lfs) %>% summary()\r\n\r\nCall:\r\nlm(formula = lngrsswk ~ foreign + factor(year) + factor(quarter), \r\n    data = lfs)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5.9979 -0.4271  0.0920  0.5535  5.4262 \r\n\r\nCoefficients:\r\n                 Estimate Std. Error  t value Pr(>|t|)    \r\n(Intercept)      5.789088   0.004442 1303.316  < 2e-16 ***\r\nforeign          0.029855   0.003990    7.482 7.31e-14 ***\r\nfactor(year)2011 0.014416   0.005428    2.656 0.007916 ** \r\nfactor(year)2012 0.042441   0.005441    7.800 6.20e-15 ***\r\nfactor(year)2013 0.064254   0.005465   11.756  < 2e-16 ***\r\nfactor(year)2014 0.083115   0.005456   15.233  < 2e-16 ***\r\nfactor(year)2015 0.104964   0.005480   19.153  < 2e-16 ***\r\nfactor(year)2016 0.123267   0.005583   22.080  < 2e-16 ***\r\nfactor(year)2017 0.165312   0.005551   29.781  < 2e-16 ***\r\nfactor(year)2018 0.193901   0.007096   27.326  < 2e-16 ***\r\nfactor(quarter)2 0.009685   0.003751    2.582 0.009819 ** \r\nfactor(quarter)3 0.013639   0.003909    3.489 0.000485 ***\r\nfactor(quarter)4 0.019056   0.003905    4.880 1.06e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.8318 on 372914 degrees of freedom\r\n  (2937143 observations deleted due to missingness)\r\nMultiple R-squared:  0.005003,  Adjusted R-squared:  0.004971 \r\nF-statistic: 156.3 on 12 and 372914 DF,  p-value: < 2.2e-16\r\n\r\n\r\nInclude education (edu) as additional control variable. Discuss what you find.\r\n\r\n\r\nAnswer:\r\n\r\nWhen including education the foreign coefficient becomes significantly negative (see below). Also note that the education coefficient is positive and significant (1 year more of eduction implies 8% higher wages). Hence it would seem that an important reason why foreigners earn more is because they tend to be more highly educated than the native population. Or put differently: foreigners with similar education levels seem to earn less than natives.\r\n\r\n\r\nlm(lngrsswk~foreign+edu+factor(year)+factor(quarter),lfs) %>% summary()\r\n\r\nCall:\r\nlm(formula = lngrsswk ~ foreign + edu + factor(year) + factor(quarter), \r\n    data = lfs)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6.3227 -0.3982  0.0795  0.4893  5.2480 \r\n\r\nCoefficients:\r\n                   Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       4.7309376  0.0074174 637.815  < 2e-16 ***\r\nforeign          -0.1588243  0.0037825 -41.989  < 2e-16 ***\r\nedu               0.0808206  0.0004485 180.206  < 2e-16 ***\r\nfactor(year)2011  0.0036654  0.0049737   0.737 0.461153    \r\nfactor(year)2012  0.0229880  0.0049866   4.610 4.03e-06 ***\r\nfactor(year)2013  0.0383417  0.0050063   7.659 1.88e-14 ***\r\nfactor(year)2014  0.0561895  0.0049999  11.238  < 2e-16 ***\r\nfactor(year)2015  0.0752652  0.0050251  14.978  < 2e-16 ***\r\nfactor(year)2016  0.0850140  0.0051215  16.600  < 2e-16 ***\r\nfactor(year)2017  0.1172969  0.0050895  23.047  < 2e-16 ***\r\nfactor(year)2018  0.1433484  0.0064992  22.056  < 2e-16 ***\r\nfactor(quarter)2  0.0080568  0.0034338   2.346 0.018961 *  \r\nfactor(quarter)3  0.0049519  0.0035772   1.384 0.166270    \r\nfactor(quarter)4  0.0125272  0.0035737   3.505 0.000456 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.7489 on 360915 degrees of freedom\r\n  (2949141 observations deleted due to missingness)\r\nMultiple R-squared:  0.0871,    Adjusted R-squared:  0.08707 \r\nF-statistic:  2649 on 13 and 360915 DF,  p-value: < 2.2e-16\r\n\r\n\r\nIn terms of discussing the contribution of foreign workers to the economy, would you say that the regression from part (a) or from part (c) is more appropriate?\r\n\r\n\r\nAnswer:\r\n\r\nThe regression in a would be more appropriate. As in most cases the foreign workers bring their education with them (and therefore the UK public doesn’t have to pay for that) and increase in “foreign” also tends to “cause” an increase in education. The combined effect of that is the contribution of a foreign relative to a native worker.\r\n\r\nExamine if foreigners are rewarded differently for more education from natives.\r\n\r\n\r\nAnswer:\r\n\r\nWe can examine this by allowing for a different education effect for foreigners. The regression below suggests a negative (and significant) interaction effect (foreign X edu). The coefficient suggest that the increase in wages for an additional year in education is 1.5 percentage points less for foreigners.\r\n\r\n\r\nCall:\r\nlm(formula = lngrsswk ~ foreign * edu + factor(year) + factor(quarter), \r\n    data = lfs)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6.3371 -0.3978  0.0791  0.4891  5.2570 \r\n\r\nCoefficients:\r\n                   Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       4.6901890  0.0080247 584.470  < 2e-16 ***\r\nforeign           0.0782408  0.0182375   4.290 1.79e-05 ***\r\nedu               0.0837327  0.0004991 167.776  < 2e-16 ***\r\nfactor(year)2011  0.0038495  0.0049725   0.774  0.43884    \r\nfactor(year)2012  0.0230811  0.0049854   4.630 3.66e-06 ***\r\nfactor(year)2013  0.0382001  0.0050051   7.632 2.31e-14 ***\r\nfactor(year)2014  0.0559443  0.0049988  11.192  < 2e-16 ***\r\nfactor(year)2015  0.0750334  0.0050239  14.935  < 2e-16 ***\r\nfactor(year)2016  0.0848432  0.0051202  16.570  < 2e-16 ***\r\nfactor(year)2017  0.1170249  0.0050883  22.999  < 2e-16 ***\r\nfactor(year)2018  0.1428669  0.0064978  21.987  < 2e-16 ***\r\nfactor(quarter)2  0.0080281  0.0034330   2.338  0.01936 *  \r\nfactor(quarter)3  0.0047887  0.0035763   1.339  0.18057    \r\nfactor(quarter)4  0.0124753  0.0035729   3.492  0.00048 ***\r\nforeign:edu      -0.0150768  0.0011347 -13.288  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.7487 on 360914 degrees of freedom\r\n  (2949141 observations deleted due to missingness)\r\nMultiple R-squared:  0.08755,   Adjusted R-squared:  0.08751 \r\nF-statistic:  2473 on 14 and 360914 DF,  p-value: < 2.2e-16\r\n\r\n\r\nExercise 10.7\r\nConsider the dataset unempprep.csv. It contains data for various regions (wards) of the UK. There are over 10,000 wards. For a long time the UK government has been supporting businesses that invest in disadvantaged areas by covering up to 35% of an investment the business undertakes if this promises to create or safeguard jobs in areas that are deemed disadvantaged by the government. In 2000 there was a review that changed which areas were considered disadvantaged and which not. In some, cases there was also a change in the intensity of support; i.e. some areas businesses received up to 35% support whereas in others the support would only amount to a maximum of 10%. In the relation to that the unempprep.dta dataset contains (among others) the following variables\r\n• DDDln1Punemp : the change in the (log) number of unemployed people in a ward between 2002 and 1997; (log) of the number of unemployed people in a ward in 2002 minus the (log) number of unemployed people in 1997.\r\n• DDDNGE : The change in support level between 2002 and 1997; i.e. support level in 2002 minus support level in 1997; e.g. if the support level was 10 in 2002 and 35% in 1997 DDDNGE would be equal to -0.25\r\nRun a regression of the change in (log) unemployed on change in support. What do you find? How can you interpret the regression?\r\n\r\n\r\nSolution\r\n\r\nSee regression below. We find a statistically significant coefficient for DDDNGE. Note that NGE - the support rate for investment project by the government - is recorded in decimals (i.e. a 20% support rate would be recorded as 0.2). Hence, the value implies that a 10 percentage point change (i.e. DDDNGE=0.1) would lead to a $100.221=2.21% reduction in unemployment.\r\n\r\n\r\n [1] \"X\"                     \"wardcode\"             \r\n [3] \"year\"                  \"NGE\"                  \r\n [5] \"ttwacode_1984\"         \"unemp\"                \r\n [7] \"actrate_1998\"          \"resid_emp_rate92\"     \r\n [9] \"resid_unemp_rate92\"    \"rate_2000\"            \r\n[11] \"rate_1993\"             \"actrate_1991\"         \r\n[13] \"LRunemp_1991\"          \"manufshare_1991\"      \r\n[15] \"occupation_1991\"       \"popdens_1981\"         \r\n[17] \"gdp91\"                 \"gdp94to96\"            \r\n[19] \"popdens_1991\"          \"current_unemprate1991\"\r\n[21] \"current_unemprate1998\" \"strunemp8690\"         \r\n[23] \"strunemp9397\"          \"vatgrowth8791\"        \r\n[25] \"vatgrowth9598\"         \"resid_emp_rate9698\"   \r\n[27] \"resid_unemp_rate9698\"  \"manufshare_9698\"      \r\n[29] \"districtcode\"          \"ELI00\"                \r\n[31] \"ELI93\"                 \"grate_00\"             \r\n[33] \"X_est_r00\"             \"p00_p0\"               \r\n[35] \"p00_p1\"                \"p00_p15\"              \r\n[37] \"p00_p2\"                \"p00_p3\"               \r\n[39] \"p00_p35\"               \"iv00\"                 \r\n[41] \"grate_93\"              \"X_est_r93\"            \r\n[43] \"p93_p0\"                \"p93_p2\"               \r\n[45] \"p93_p3\"                \"iv93\"                 \r\n[47] \"X_merge\"               \"xnivav\"               \r\n[49] \"ln1Punemp\"             \"lnunemp\"              \r\n[51] \"wardx\"                 \"lnunemp_1997\"         \r\n[53] \"DDDlnunemp\"            \"ln1Punemp_1997\"       \r\n[55] \"DDDln1Punemp\"          \"xnivav_1997\"          \r\n[57] \"DDDxnivav\"             \"NGE_1997\"             \r\n[59] \"DDDNGE\"               \r\n\r\n 2002 \r\n10764 \r\n\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \r\n-0.30000  0.00000  0.00000 -0.01505  0.00000  0.35000 \r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   2002    2002    2002    2002    2002    2002 \r\n\r\nCall:\r\nlm(formula = DDDln1Punemp ~ DDDNGE, data = up)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.5451 -0.1907  0.0165  0.2109  2.8601 \r\n\r\nCoefficients:\r\n             Estimate Std. Error  t value Pr(>|t|)    \r\n(Intercept) -0.462220   0.003648 -126.703  < 2e-16 ***\r\nDDDNGE      -0.221062   0.036012   -6.138 8.62e-10 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.3743 on 10762 degrees of freedom\r\nMultiple R-squared:  0.003489,  Adjusted R-squared:  0.003396 \r\nF-statistic: 37.68 on 1 and 10762 DF,  p-value: 8.624e-10\r\n\r\nunhide()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n\r\n\r\n/* update total correct if #total_correct exists */\r\nupdate_total_correct = function() {\r\n  if (t = document.getElementById(\"total_correct\")) {\r\n    t.innerHTML =\r\n      document.getElementsByClassName(\"correct\").length + \" of \" +\r\n      document.getElementsByClassName(\"solveme\").length + \" correct\";\r\n  }\r\n}\r\n\r\n/* solution button toggling function */\r\nb_func = function() {\r\n  var cl = this.parentElement.classList;\r\n  if (cl.contains('open')) {\r\n    cl.remove(\"open\");\r\n  } else {\r\n    cl.add(\"open\");\r\n  }\r\n}\r\n\r\n/* function for checking solveme answers */\r\nsolveme_func = function(e) {\r\n  var real_answers = JSON.parse(this.dataset.answer);\r\n  var my_answer = this.value;\r\n  var cl = this.classList;\r\n  if (cl.contains(\"ignorecase\")) {\r\n    my_answer = my_answer.toLowerCase();\r\n  }\r\n  if (cl.contains(\"nospaces\")) {\r\n    my_answer = my_answer.replace(/ /g, \"\");\r\n  }\r\n  \r\n  if (my_answer !== \"\" & real_answers.includes(my_answer)) {\r\n    cl.add(\"correct\");\r\n  } else {\r\n    cl.remove(\"correct\");\r\n  }\r\n  update_total_correct();\r\n}\r\n\r\nwindow.onload = function() {\r\n  /* set up solution buttons */\r\n  var buttons = document.getElementsByTagName(\"button\");\r\n\r\n  for (var i = 0; i < buttons.length; i++) {\r\n    if (buttons[i].parentElement.classList.contains('solution')) {\r\n      buttons[i].onclick = b_func;\r\n    }\r\n  }\r\n  \r\n  /* set up solveme inputs */\r\n  var solveme = document.getElementsByClassName(\"solveme\");\r\n\r\n  for (var i = 0; i < solveme.length; i++) {\r\n    /* make sure input boxes don't auto-anything */\r\n    solveme[i].setAttribute(\"autocomplete\",\"off\");\r\n    solveme[i].setAttribute(\"autocorrect\", \"off\");\r\n    solveme[i].setAttribute(\"autocapitalize\", \"off\"); \r\n    solveme[i].setAttribute(\"spellcheck\", \"false\");\r\n    solveme[i].value = \"\";\r\n    \r\n    /* adjust answer for ignorecase or nospaces */\r\n    var cl = solveme[i].classList;\r\n    var real_answer = solveme[i].dataset.answer;\r\n    if (cl.contains(\"ignorecase\")) {\r\n      real_answer = real_answer.toLowerCase();\r\n    }\r\n    if (cl.contains(\"nospaces\")) {\r\n      real_answer = real_answer.replace(/ /g, \"\");\r\n    }\r\n    solveme[i].dataset.answer = real_answer;\r\n    \r\n    /* attach checking function */\r\n    solveme[i].onkeyup = solveme_func;\r\n    solveme[i].onchange = solveme_func;\r\n  }\r\n  \r\n  update_total_correct();\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/exercises/exercises10/adidas-superstar-EG4958.jpg",
    "last_modified": "2020-12-18T12:14:52+00:00",
    "input_file": {}
  },
  {
    "path": "posts/exercises/exercises8/",
    "title": "Exercises 8",
    "description": "Learning like a machine",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2020-12-17",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\n\r\n\r\n\r\nThe Exercises consist of 2 machine learning competitions\r\nExercise 8.1 - Predicting Crime\r\nFor this exercise you have to work with an online app that you find here\r\nResults will be here.\r\nExercise 8.2 - Learning about Antivaxxers\r\nFor this exercise you have to work with an online app that you find here\r\nExercise 8.3 - Rich areas\r\nCan you find rich areas? Competition 2 is here.\r\nResults will be here.\r\n\r\n\r\n",
    "preview": "posts/exercises/exercises8/titanic_PNG1.png",
    "last_modified": "2020-12-17T18:24:25+00:00",
    "input_file": {},
    "preview_width": 1600,
    "preview_height": 459
  },
  {
    "path": "posts/exercises/exercises9/",
    "title": "Exercises 9",
    "description": "Time for Series",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2020-12-05",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\n\r\n\r\n\r\nExercise 9.1\r\nThe oj.csv dataset contains monthly data on the log price of (frozen) orange juice concentrate (for the US) as the variable lnp. A large part of the orange production in the US originates from Florida. Hence, the weather in Florida is potentially an important factor in the orange price. Frost is rare in Florida. But when it happens it is particularly detrimental for the orange harvest. The fdd contains the number of freezing degree days in a month.\r\n\r\n\r\nlibrary(dplyr)\r\noj=read.csv(\"https://www.dropbox.com/s/o76do0oyoxhhy6p/oj.csv?dl=1\")\r\n\r\n\r\n\r\n\r\nRun a regression of the (log) orange juice price on freezing degree days in the previous month and interpret the regression.\r\n\r\n\r\nAnswer:\r\n\r\nTo create a lagged version of the fdd variable we can use the dplyr::lag() function:\r\n\r\n\r\nlibrary(dplyr)\r\noj=oj %>% mutate(L1fdd=dplyr::lag(fdd))\r\n\r\nNow run a regression of the price on freezing degree days:\r\n\r\n\r\nlm(lnp~L1fdd,oj) %>% summary()\r\n\r\nCall:\r\nlm(formula = lnp ~ L1fdd, data = oj)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.45826 -0.17130 -0.00483  0.12521  0.60393 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 4.674738   0.009079 514.916   <2e-16 ***\r\nL1fdd       0.002637   0.002708   0.974    0.331    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.2257 on 638 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.001484,  Adjusted R-squared:  -8.083e-05 \r\nF-statistic: 0.9484 on 1 and 638 DF,  p-value: 0.3305\r\n\r\nThis suggest that freezing degree days have a positive impact on the price (as we would expect: freezing means there are less oranges around so the price goes up). One day more of freezing would imply an increase in the price by 0.2%. However, this result is not significant.\r\n\r\nCan you suggest reasons why the result in (a) might not be an unbiased estimate of the effect of freezing on orange juice prices?\r\n\r\n\r\nAnswer:\r\n\r\nFreezing is likely to be exogenous. So we don’t have to worry about the usual confounding factors. However, we are dealing with time series. One issue could be that the price series has a unit root. Plotting the series is a good start to examine this:\r\n\r\n\r\np=ggplot(oj,aes(x=date,y=lnp))+geom_line(color=\"green\")+theme_minimal()+xlab(\"Monthly data\")\r\n\r\nplot(p)\r\n\r\n\r\n\r\nCan you check if the price series has a unit root?\r\n\r\n\r\nAnswer:\r\n\r\nWe can use the Dickey-Fuller test:\r\n\r\n\r\nlibrary(urca)\r\nsummary(ur.df(oj$lnp))\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression none \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.28240 -0.01007 -0.00124  0.00659  0.41180 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)   \r\nz.lag.1    -0.0002675  0.0004203  -0.636  0.52480   \r\nz.diff.lag  0.1284067  0.0392853   3.269  0.00114 **\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.04975 on 637 degrees of freedom\r\nMultiple R-squared:  0.01722,   Adjusted R-squared:  0.01413 \r\nF-statistic:  5.58 on 2 and 637 DF,  p-value: 0.003957\r\n\r\n\r\nValue of test-statistic is: -0.6363 \r\n\r\nCritical values for test statistics: \r\n      1pct  5pct 10pct\r\ntau1 -2.58 -1.95 -1.62\r\n\r\nThe test statistic is larger than the even the 10pct cut-off. Hence we cannot reject the hypothesis that there is a unit root.\r\n\r\nCan you suggest an alternative (unbiased) approach to estimating the effect of freezing on price?\r\n\r\n\r\nAnswer:\r\n\r\nWe need to difference the series to get rid of the unit root. Note that for the differenced series we clearly reject the unit root:\r\n\r\n\r\nur.df(diff(oj$lnp)) %>% summary()\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression none \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.28373 -0.01140 -0.00246  0.00605  0.41061 \r\n\r\nCoefficients:\r\n           Estimate Std. Error t value Pr(>|t|)    \r\nz.lag.1    -0.85301    0.05233 -16.300   <2e-16 ***\r\nz.diff.lag -0.02086    0.03965  -0.526    0.599    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.04979 on 636 degrees of freedom\r\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.434 \r\nF-statistic: 245.7 on 2 and 636 DF,  p-value: < 2.2e-16\r\n\r\n\r\nValue of test-statistic is: -16.3003 \r\n\r\nCritical values for test statistics: \r\n      1pct  5pct 10pct\r\ntau1 -2.58 -1.95 -1.62\r\n\r\n\r\n\r\noj=oj %>% mutate(Dlnp=lnp-dplyr::lag(lnp))\r\nlm( Dlnp~L1fdd,oj) %>% summary()\r\n\r\nCall:\r\nlm(formula = Dlnp ~ L1fdd, data = oj)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.28200 -0.01001 -0.00095  0.00560  0.41224 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept) -0.0020334  0.0020063  -1.014    0.311  \r\nL1fdd        0.0014905  0.0005984   2.491    0.013 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.04989 on 638 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.009629,  Adjusted R-squared:  0.008077 \r\nF-statistic: 6.203 on 1 and 638 DF,  p-value: 0.01301\r\n\r\nWe now find a smaller effect than before: 1 extra freezing day leads to 0.149% higher orange juice prices (i.e. we previously had an upward bias). However, the result is now significant.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-05T15:31:14+00:00",
    "input_file": {}
  },
  {
    "path": "posts/quickguides/quickguide_unitroots/",
    "title": "Quick Guide - Unit roots",
    "description": {},
    "author": [
      {
        "name": "Ralf Martin",
        "url": {}
      }
    ],
    "date": "2020-12-03",
    "categories": [
      "Quickguides"
    ],
    "contents": "\r\n\r\n\r\n  /* Rearrange console label */\r\n  .datacamp-exercise ol li, .datacamp-exercise ul li {\r\n    margin-bottom: 0em !important;\r\n  }\r\n  \r\n  /* Remove bullet marker */\r\n  .datacamp-exercise ol li::before, .datacamp-exercise ul li::before {\r\n    content: '' !important;\r\n  }\r\nWhen dealing with time series we have to account for the possibility that the observations in our data are correlated from one observations to the next; e.g. if we see a particularly high value in one period we can also expect a higher value in the next period.\r\nOne of the simplest possibilities is a so called autoregressive model with one lag; i.e. \\[y_t=\\beta_0+\\beta_1 y_{t-1}+\\epsilon_t \\tag{1}\\] Hence, this is like the normal linear regression model except that one of our explanatory variables is the dependent variable in the previous period. In principle we can estimate that like any other regression model; i.e. with OLS using the lm() command in R.\r\nHowever, this goes wrong if \\(\\beta_1\\) gets too big; i.e. if it is equal or larger than 1 or smaller than -1 (i.e. \\(|\\beta_1|>1\\)). This is also referred to as the times series having a “unit root”.\r\nIf this happens,\r\nOLS will give us a (downward) biased estimate of \\(\\beta_1\\)\r\nWe might find spurious relationships between \\(y_t\\) and other variables that have a unit root\r\ni.e. we might find a strong correlation between two data series even though there is no causal mechanism between them. All that we are picking up is that both series have a unit root.\r\nTo check if we are dealing with a unit root in a given data series we can use the so called Dickey-Fuller (DF) test. This test is based on a transformed version of the above model where we subtract \\(y_{t-1}\\) from either side of the equation to get:\r\n\\[\\Delta y_t= y_t - y_{t-1}=  \\beta_0+ \\delta  y_{t-1}+\\epsilon_t \\tag{2}\\]\r\nwhere \\(\\delta = \\beta_1-1\\). Hence, in Equation 2 a unit root would imply \\(\\delta=0\\) and this is the (null) hypothesis the DF test examines.\r\nWe can implement this in R with the ur.df() command from the urca package. Here we do this with a simulated series with unit roots:\r\n\r\n\r\n\r\n\r\n\r\nobs=100\r\neps=rnorm(obs)\r\ny100=eps\r\nfor(i in 2:obs){\r\n  y100[i]= y100[i-1]*1+eps[i]\r\n}\r\nsdf=data.frame(y100,eps,period=1:obs) \r\n\r\nIf we plot y100 this will look as follows:\r\n\r\n\r\n\r\nWe can now implement the DF test:\r\n\r\n\r\nlibrary(urca)\r\nlibrary(dplyr)\r\n\r\nur.df(sdf$y100,lags=0) %>% summary()\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression none \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 - 1)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.21711 -0.74493 -0.05352  1.00502  2.23895 \r\n\r\nCoefficients:\r\n        Estimate Std. Error t value Pr(>|t|)\r\nz.lag.1 -0.03789    0.02850  -1.329    0.187\r\n\r\nResidual standard error: 1.153 on 98 degrees of freedom\r\nMultiple R-squared:  0.01771,   Adjusted R-squared:  0.00769 \r\nF-statistic: 1.767 on 1 and 98 DF,  p-value: 0.1868\r\n\r\n\r\nValue of test-statistic is: -1.3294 \r\n\r\nCritical values for test statistics: \r\n     1pct  5pct 10pct\r\ntau1 -2.6 -1.95 -1.61\r\n\r\nThe summary of the output of the ur.df() provides us with a the regression output we would be getting if we simply implmented equation 2 as an OLS model. What is called z.lag.1 corresponds to \\(\\delta\\) in equation 2. HOwever, as we said above: this estimate will be biased and so will be the P-value which we could normally use to decide if a regression coefficient is different from 0. Equally, we CANNOT use the typical critical values we might normally compare the t-value to. Luckily, ur.df() reports some new critical thresholds that we can use at the bottom of its output (i.e. the Critical values for test statistics:). We see critical values for a 1, 5 and 10% significance level. What we need to check is if our test statistic is smaller than the values reported there. If that is the case we can reject the hypothesis that \\(\\beta_1\\) is equal to 1. Not surprisingly - given that we simulated our series with a \\(\\beta_1=1\\) we cannot do that. So we conclude that we have unit root and we see if can get rid of it by differencing the series:\r\n\r\n\r\nlibrary(urca)\r\nlibrary(dplyr)\r\n\r\nur.df(diff(sdf$y100),lags=0) %>% summary()\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression none \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 - 1)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4514 -0.9035 -0.1341  0.8249  2.1159 \r\n\r\nCoefficients:\r\n        Estimate Std. Error t value Pr(>|t|)    \r\nz.lag.1  -1.0580     0.1015  -10.43   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.167 on 97 degrees of freedom\r\nMultiple R-squared:  0.5284,    Adjusted R-squared:  0.5235 \r\nF-statistic: 108.7 on 1 and 97 DF,  p-value: < 2.2e-16\r\n\r\n\r\nValue of test-statistic is: -10.4246 \r\n\r\nCritical values for test statistics: \r\n     1pct  5pct 10pct\r\ntau1 -2.6 -1.95 -1.61\r\n\r\nThis turns out to be the case. The test statistics is now much smaller than the critical value.\r\nUnit roots and causal inference\r\nA problem with unit roots is that they create spurious correlations. Let’s explore this by creating another time series with unit root:\r\n\r\n\r\nepsx=rnorm(obs)\r\nx100=eps\r\nfor(i in 2:obs){\r\n  x100[i]= x100[i-1]*1+epsx[i]\r\n}\r\nsdf=bind_cols(sdf,data.frame(x100,epsx)) \r\n\r\n Let’s plot the time series\r\n\r\n\r\n\r\nLet’s also run a regression of the Y on X:\r\n\r\n\r\nlm(y100~x100,sdf) %>% summary()\r\n\r\nCall:\r\nlm(formula = y100 ~ x100, data = sdf)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5.6537 -1.6672 -0.2056  1.4904  6.7883 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.39846    0.54150   0.736    0.464    \r\nx100         0.43921    0.08596   5.110 1.59e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2.615 on 98 degrees of freedom\r\nMultiple R-squared:  0.2104,    Adjusted R-squared:  0.2023 \r\nF-statistic: 26.11 on 1 and 98 DF,  p-value: 1.595e-06\r\n\r\nlm(y100~x100+period,sdf) %>% summary()\r\n\r\nCall:\r\nlm(formula = y100 ~ x100 + period, data = sdf)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.6662 -1.5504 -0.2242  1.0527  6.1297 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  2.534841   0.492652   5.145 1.39e-06 ***\r\nx100         0.599797   0.069212   8.666 1.01e-13 ***\r\nperiod      -0.059846   0.007295  -8.204 9.88e-13 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2.02 on 97 degrees of freedom\r\nMultiple R-squared:  0.5338,    Adjusted R-squared:  0.5242 \r\nF-statistic: 55.54 on 2 and 97 DF,  p-value: < 2.2e-16\r\n\r\nHence, despite X and Y being absolutely un-related to each other we find a strong correlation. The matter cannot in general be addressed by including a linear time trend. Let’s see what happens if we difference both series:\r\n\r\n\r\nsdf=sdf %>% mutate(Dy100=y100-dplyr::lag(y100))\r\nlm(Dy100~x100,sdf %>% filter(period>1)) %>% summary()\r\n\r\nCall:\r\nlm(formula = Dy100 ~ x100, data = sdf %>% filter(period > 1))\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4296 -0.8203 -0.1171  0.8573  2.1128 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)\r\n(Intercept) -2.152e-02  2.488e-01  -0.086    0.931\r\nx100        -7.717e-05  3.931e-02  -0.002    0.998\r\n\r\nResidual standard error: 1.169 on 97 degrees of freedom\r\nMultiple R-squared:  3.974e-08, Adjusted R-squared:  -0.01031 \r\nF-statistic: 3.854e-06 on 1 and 97 DF,  p-value: 0.9984\r\n\r\ni.e. we cannot find a significant relationship (as we should not).\r\nLet’s also see what happens if we have a Y variable that is actually driven by X. We using the following true model:\r\n\\[ y_t=\\beta_0 +  y_{t-1}+ 0.5  x_{t} +\\varepsilon_t\\]  Let’s head for Monte Carlo and simulate data from this model:\r\n\r\n\r\nepsyyy=rnorm(obs)\r\nyyy=eps+ 0.5 * x100[1]\r\nfor(i in 2:obs){\r\n  yyy[i]= yyy[i-1]*1+epsyyy[i] + 0.5 * x100[i]\r\n}\r\nsdf=bind_cols(sdf,data.frame(yyy,epsyyy)) \r\n\r\n Once more we can plot:\r\n\r\n\r\n\r\nAnd regress:\r\n\r\n\r\nlm(yyy~x100,sdf) %>% summary()\r\n\r\nCall:\r\nlm(formula = yyy ~ x100, data = sdf)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-125.73  -85.74  -22.33   73.25  190.74 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   90.644     20.611   4.398 2.78e-05 ***\r\nx100           7.917      3.272   2.420   0.0174 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 99.54 on 98 degrees of freedom\r\nMultiple R-squared:  0.05638,   Adjusted R-squared:  0.04675 \r\nF-statistic: 5.855 on 1 and 98 DF,  p-value: 0.01738\r\n\r\nsdf=sdf %>%  mutate(Dyyy=yyy-dplyr::lag(yyy),Dx100=x100-dplyr::lag(x100))\r\n\r\n\r\nlm(Dyyy~x100,sdf ) %>% summary()\r\n\r\nCall:\r\nlm(formula = Dyyy ~ x100, data = sdf)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.83541 -0.58899 -0.08512  0.62262  2.77286 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.19672    0.22708   0.866    0.388    \r\nx100         0.49008    0.03587  13.663   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.067 on 97 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.6581,    Adjusted R-squared:  0.6545 \r\nF-statistic: 186.7 on 1 and 97 DF,  p-value: < 2.2e-16\r\n\r\n Note that if we run a regression of yyy on x100 we get an estimate that is wildly off the true on estimate. If we run a regression in first differences we get something fairly close to the true value of 0.5.\r\nAlso good practice to check that yyy is really stationary now:\r\n\r\n\r\nur.df(diff(sdf$yyy),lags=0) %>% summary()\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression none \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 - 1)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.9700 -0.7473  0.2348  1.3351  4.6117 \r\n\r\nCoefficients:\r\n        Estimate Std. Error t value Pr(>|t|)  \r\nz.lag.1 -0.10552    0.04523  -2.333   0.0217 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.549 on 97 degrees of freedom\r\nMultiple R-squared:  0.05314,   Adjusted R-squared:  0.04337 \r\nF-statistic: 5.443 on 1 and 97 DF,  p-value: 0.02171\r\n\r\n\r\nValue of test-statistic is: -2.3331 \r\n\r\nCritical values for test statistics: \r\n     1pct  5pct 10pct\r\ntau1 -2.6 -1.95 -1.61\r\n\r\nIndeed, we can reject \\(\\delta=0\\) on the differenced series (at least at 5%).\r\nExtensions\r\nAbove we explore the simplest autoregressive model. More complex autoregressive models could include more lags, a (nonzero) constant or a time trend. We can accomodate these cases with the ur.df() command. For instance\r\n\r\n\r\nur.df((sdf$y100),lags=2, type=\"trend\") %>% summary()\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression trend \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.4080 -0.8137 -0.1309  0.8624  2.2273 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept)  0.805594   0.344225   2.340   0.0214 *\r\nz.lag.1     -0.111454   0.047331  -2.355   0.0207 *\r\ntt          -0.010131   0.004671  -2.169   0.0327 *\r\nz.diff.lag1 -0.027582   0.102665  -0.269   0.7888  \r\nz.diff.lag2  0.074329   0.102043   0.728   0.4682  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.14 on 92 degrees of freedom\r\nMultiple R-squared:  0.07854,   Adjusted R-squared:  0.03848 \r\nF-statistic:  1.96 on 4 and 92 DF,  p-value: 0.1071\r\n\r\n\r\nValue of test-statistic is: -2.3548 2.4378 3.593 \r\n\r\nCritical values for test statistics: \r\n      1pct  5pct 10pct\r\ntau3 -4.04 -3.45 -3.15\r\nphi2  6.50  4.88  4.16\r\nphi3  8.73  6.49  5.47\r\n\r\nexamines a model with 3 lags and a time trend. How do we know if that is necessary? Note that it says lags=2 because that refers to lags beyond lag one. How do we know if such a more general model is needed? We can go from more general to more specific; i.e. this would look as follows:\r\n\\[\\Delta y_t=\\beta_0 +\\rho \\times t+ \\delta y_{t-1}+ \\beta_2 \\Delta y_{t-1} + \\beta_3 \\Delta y_{t-2}+\\varepsilon_t\\]\r\nFirstly, on the lags. It turns out that the standard errors for the coefficients on the lags are actually not biased in the differences from of the equation. Hence, we can use the normal t-tests to throw out lags; e.g. let’s try lags=1 only as above the second lag term z.diff.lag2 is not significant.\r\n\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression trend \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.5129 -0.7817 -0.1183  0.8220  2.2355 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)   \r\n(Intercept)  0.861754   0.326152   2.642  0.00965 **\r\nz.lag.1     -0.112466   0.045150  -2.491  0.01450 * \r\ntt          -0.011050   0.004485  -2.464  0.01557 * \r\nz.diff.lag  -0.033659   0.101355  -0.332  0.74056   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.134 on 94 degrees of freedom\r\nMultiple R-squared:  0.08781,   Adjusted R-squared:  0.0587 \r\nF-statistic: 3.016 on 3 and 94 DF,  p-value: 0.03379\r\n\r\n\r\nValue of test-statistic is: -2.4909 2.9158 4.3499 \r\n\r\nCritical values for test statistics: \r\n      1pct  5pct 10pct\r\ntau3 -4.04 -3.45 -3.15\r\nphi2  6.50  4.88  4.16\r\nphi3  8.73  6.49  5.47\r\n\r\nTurns out the z.diff.lag1 is not significiant either. So maybe back to lags=0?\r\n\r\n\r\n############################################### \r\n# Augmented Dickey-Fuller Test Unit Root Test # \r\n############################################### \r\n\r\nTest regression trend \r\n\r\n\r\nCall:\r\nlm(formula = z.diff ~ z.lag.1 + 1 + tt)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.5413 -0.7635 -0.1778  0.8154  2.2900 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept)  0.804043   0.308978   2.602   0.0107 *\r\nz.lag.1     -0.108795   0.042926  -2.534   0.0129 *\r\ntt          -0.010252   0.004307  -2.380   0.0193 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.125 on 96 degrees of freedom\r\nMultiple R-squared:  0.08291,   Adjusted R-squared:  0.06381 \r\nF-statistic:  4.34 on 2 and 96 DF,  p-value: 0.01569\r\n\r\n\r\nValue of test-statistic is: -2.5345 2.9056 4.3396 \r\n\r\nCritical values for test statistics: \r\n      1pct  5pct 10pct\r\ntau3 -4.04 -3.45 -3.15\r\nphi2  6.50  4.88  4.16\r\nphi3  8.73  6.49  5.47\r\n\r\nBut what about the time trend? Note that compared to our simple case above we now have more values being reported as test statistics and we also have more rows in our critical values table above. These are test statistics along with critical values. What they can refer to can be a bit confusing at first, but hopefully it will become clear. Again, you might want to think of this as going from a more general to a more specific model. So Let’s first look a the third value in the Value of test-statistic is: line above which is 4.3396. That refers to the test statistic of a joint hypothesis test with \\[H0: \\delta = 1, \\rho=0, \\beta_0=0\\] Given that \\(4.3396 < 5.47\\) we cannot reject all of these restrictions jointly. Consequently, we have to proceed on the basis, that there is a unit root but there are no time trends or intercepts. Note that if we have a situation where the first test statistic is larger than tau3 and the third test statistic is larger than phi3 we can conclude that we have a unit root and a time trend. The second test statistic (i.e. 2.0237) examines the hypothesis that we have a unit root but no constant term (i.e. \\(H0: \\delta = 1, \\beta_0=0\\)).\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/quickguides/quickguide_unitroots/265-2656331_ginseng-root-png.png",
    "last_modified": "2020-12-03T17:32:11+00:00",
    "input_file": {},
    "preview_width": 525,
    "preview_height": 469
  },
  {
    "path": "posts/exercises/exercises4/",
    "title": "Exercises 4",
    "description": "Testing your knowledge of testing",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2020-11-29",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\n\r\n\r\n\r\nExercise 4.1\r\nA researcher hypothesizes that year of schooling, S, may be related to the number of siblings (brothers and sisters) one has, SIBLINGS, according to the relationship \\[S = \\beta_{1}+\\beta_{2}SIBLINGS+\\epsilon\\]\r\nShe is prepared to test the null hypothesis \\(H0: \\beta_{2}=0\\) against the alternative hypothesis \\(H1: \\beta_{2}\\neq 0\\) at the 5 and 1 percent levels. She has a sample of 60 observations. What are the t-distribution values and what will she conclude in the following cases?\r\nPart (a)\r\nIf \\(\\hat{\\beta}_{2} = -0.20, \\sigma_{\\hat{\\beta}_{2}}=0.07\\)?\r\nThe t-value (rounded up to 3 decimal places) is: \r\nCan she reject H0 at at the 5% significance level? NoYes\r\nCan she reject H0 at at the 1% significance level? NoYes\r\nPart (b)\r\nIf \\(\\hat{\\beta}_{2} = -0.12, \\sigma_{\\hat{\\beta}_{2}}=0.07\\)?\r\nThe t-value (rounded up to 3 decimal places) is: \r\nCan she reject H0 at at the 5% significance level? YesNo\r\nCan she reject H0 at at the 1% significance level)? YesNo\r\nPart (c)\r\nIf \\(\\hat{\\beta}_{2} = 0.06, \\sigma_{\\hat{\\beta}_{2}}=0.07\\)?\r\nThe t-value (rounded up to 3 decimal places) is: \r\nCan she reject H0 at at the 5% significance level? YesNo\r\nCan she reject H0 at at the 1% significance level? YesNo\r\nPart (d)\r\nIf \\(\\hat{\\beta}_{2} = 0.20, \\sigma_{\\hat{\\beta}_{2}}=0.07\\)?\r\nThe t-value (rounded up to 3 decimal places) is: \r\nCan she reject H0 at at the 5% significance level? NoYes\r\nCan she reject H0 at at the 1% significance level? NoYes\r\n\r\n\r\nHint:\r\n\r\nRecall that the rejection threshold c for the normal distribution are 1.959964 and 2.5758293 for a 5% and 1% significance level. For the t distribution the same values are 2.0017175 and 2.663287 (Note that we have 58 degrees of freedom).\r\nWe can also work out p-values. Remember that the p-value is the probability - assuming the H0 is correct - to have a value more extreme (further away from 0) than the one estimated. We can use the pt() command for that, which gives us the cumulative density function of the t-distribution; e.g. pt(0,58) gives us the probability to have a value smaller than 0. Because the t-distribution is symmetric that will always be equal to 0.5 Note that we have 58 degrees of freedom here as we 60 observations and we need to estimate 2 parameters (\\(\\beta_0\\) and \\(\\beta_1\\)). To work out the p-value call the pt() function with the t-value for a given estimate; e.g. for part a)\r\n\r\n\r\npt(-.2/0.07,58)\r\n\r\n[1] 0.002962872\r\n\r\ni.e. the probability of having an estimate lower than -.2 is 0.0029629. Note, because the our test considers the possibility of being to low and too high (and because the distribution is symmetric) we need to double this to get the actual p-value which becomes 0.0059257. However, this is still below 1% so we can safely reject the hypothesis that the true parameter is 0. Note, if we estimate a positive value we still need to use the same value but in negative terms. So, more generally we could write that we need compute 2*pt(-abs(t),58); i.e. the negative of the absolute value of the t-value. Because pt() (like any cumulative density) sums to 1 and is symmetric and alterive formula with the same result would be 2*(1-pt(-abs(t),58))\r\n\r\n<\/div>\r\n\r\n\r\n\r\n<div class='solution'><button>Answer:<\/button>\r\n\r\n\r\nHence we get the following answers\r\n\r\n1.  t=-0.2/0.07= -2.857\r\nSignificantly different from 0 at 5 and 1 percent; i.e. we reject the H0 at either significance level.\r\n\r\n2.  t=-0.12/0.07= -1.714\r\nNot significantly different from 0 at 5 and 1 percent; i.e. we don't reject the H0\r\n\r\n3.  t=0.06/0.07= 0.857\r\nNot significantly different from 0 at 5 and 1 percent.\r\n\r\n4.  t=0.2/0.07= 2.857\r\nSignificantly different from 0 at 5 and 1 percent; i.e. we reject the H0 at either significance level.\r\n\r\n\r\n<\/div>\r\n\r\n  \r\n    \r\n# Exercise 4.2\r\n\r\nLets use the WAGE1.DTA dataset again!\r\n\r\n<div class=\"layout-chunk\" data-layout=\"l-body\">\r\n\r\n```r\r\n  library(foreign)\r\n  data <- read.dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AADZ4MZhDDk9R8sFSjBvmcRma/WAGE1.DTA?dl=1\") \r\n\r\nPart (a)\r\nRegress wages on education (years of schooling). Perform a test of the hypothesis that the coefficient on schooling is equal to 0.\r\nCan you reject the hypothesis? YesYes\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod <- lm(wage~educ,data)\r\n  summary(mod)\r\n\r\nCall:\r\nlm(formula = wage ~ educ, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5.3396 -2.1501 -0.9674  1.1921 16.6085 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -0.90485    0.68497  -1.321    0.187    \r\neduc         0.54136    0.05325  10.167   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.378 on 524 degrees of freedom\r\nMultiple R-squared:  0.1648,    Adjusted R-squared:  0.1632 \r\nF-statistic: 103.4 on 1 and 524 DF,  p-value: < 2.2e-16\r\n\r\ni.e. t>2 and P very close to 0. Hence coefficient on education is highly significant (we can reject hypothesis that it is equal to 0).\r\n\r\nPart (b)\r\nRegress tenure (years with current employer) on education. Can you think of a causal mechanism that could support this specificiation?\r\nCan you reject the hypothesis? YesNo\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod2 <- lm(tenure~educ,data)\r\n  summary(mod2)\r\n\r\nCall:\r\nlm(formula = tenure ~ educ, data = data)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-6.946 -4.894 -2.601  1.520 38.813 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   6.9457     1.4638   4.745 2.69e-06 ***\r\neduc         -0.1466     0.1138  -1.288    0.198    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 7.22 on 524 degrees of freedom\r\nMultiple R-squared:  0.003155,  Adjusted R-squared:  0.001253 \r\nF-statistic: 1.659 on 1 and 524 DF,  p-value: 0.1984\r\n\r\ni.e. there is a negative relationship: one year of extra education reduces tenure by 0.14 years. That said: this is not a significant relationship.\r\nOne reason why education might lead less tenure is that time is finite and being in education allows for less time to work on a job. Also: we are dealing here with time spend with the same employer. So it could be that employees that are more educated tend to change jobs more often (they might have more general skills that are in high demand by various employers). As a consequence they accumulate less time with a specific employer.\r\n\r\nExercise 4.3\r\nLets go back to the auto dataset.\r\n\r\n\r\n  library(foreign)\r\n  data <- read.dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AACNkMy47ilXAMh3nmiIs_Bqa/auto.dta?dl=1\")\r\n\r\nPart (a)\r\nDraw a scatterplot of price versus weight. Which cars, in terms of weight, tend to be more expensive? LighterHeavier\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  library(ggplot2)\r\n  ggplot(data, aes(x=price, y=weight)) + geom_point()\r\n\r\n\r\nHeavier cars tend to be more expensive.\r\n\r\nPart (b)\r\nRun a regression of price on weight.\r\nWhat is the constant? \r\nWhat is the slope coefficient? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod<-lm(price~weight,data)\r\n  summary(mod)\r\n\r\nCall:\r\nlm(formula = price ~ weight, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3341.9 -1828.3  -624.1  1232.1  7143.7 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   -6.7074  1174.4296  -0.006    0.995    \r\nweight         2.0441     0.3768   5.424 7.42e-07 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2502 on 72 degrees of freedom\r\nMultiple R-squared:  0.2901,    Adjusted R-squared:  0.2802 \r\nF-statistic: 29.42 on 1 and 72 DF,  p-value: 7.416e-07\r\n\r\n\r\nPart (c)\r\nWhat is the predicted price of a car weighing 2,500lb? \r\nWhat is the predicted price of a car weighing 2,500lb? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  value_to_forecast <- data.frame(weight=c(2500,4000))\r\n  forecast <- predict(mod,value_to_forecast)\r\n\r\nAlternatively:\r\n\r\n\r\n  mod$coefficients[[1]]+mod$coefficients[[2]]*2500\r\n\r\n[1] 5103.449\r\n\r\n  mod$coefficients[[1]]+mod$coefficients[[2]]*4000\r\n\r\n[1] 8169.543\r\n\r\n\r\nPart (d)\r\nWhat is the expected price difference between two cars, one of which is 500kg(!) heavier? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod$coefficients[[2]]*1102\r\n\r\n[1] 2252.557\r\n\r\n\r\nPart (e)\r\nWhat is the predicted price of a car weighing 500lb? \r\nDo you think this number is very meaningful? YesNo\r\nWhy or why not? Check the answer below to see if you’re right.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod$coefficients[[1]]+mod$coefficients[[2]]*500\r\n\r\n[1] 1015.324\r\n\r\nThe lightest car in the sample weighs 1760lb. It is unlikely to tell us much about a car weighing 500lb.\r\n\r\nExercise 4.4\r\nConduct another Monter-Carlo expercise as in the previous exercise session (exercise session 2.3). Generate the \\(\\epsilon\\) shocks as before. For the X, however, use the following command instead: x=unif(obs)*eps.\r\nOtherwise take the same steps as in the previous Monte Carlo exercise, using the the same true model \\(Y=2+0.5 X +\\epsilon\\). Produce a histogram to visualise your betas.\r\nPart (a)\r\nWhat average estimate for the coefficient associated with X do you find now? (Round up to the nearest whole number) \r\nPart (b)\r\nHow does it relate to the true value of the coefficient of 0.5, is it larger or smaller? SmallerLarger\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n    obs <- 100\r\n    eps <- rnorm(obs)\r\n    x <- runif(obs)*eps\r\n    \r\n    cor(x,eps)  # Note that x and eps will be highly positively correlated\r\n\r\n[1] 0.8636346\r\n\r\n    y <- 2+0.5*x+eps\r\n    mod<-lm(y~x)\r\n    summary(mod)\r\n\r\nCall:\r\nlm(formula = y ~ x)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.34986 -0.32553 -0.06164  0.43712  2.13664 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  2.12850    0.06158   34.56   <2e-16 ***\r\nx            1.99589    0.08820   22.63   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6155 on 98 degrees of freedom\r\nMultiple R-squared:  0.8394,    Adjusted R-squared:  0.8377 \r\nF-statistic:   512 on 1 and 98 DF,  p-value: < 2.2e-16\r\n\r\n\r\n\r\n  beta <- double(1000)\r\n  \r\n  cc = double(1000)\r\n  \r\n  for(i in 1:1000){\r\n    eps <- rnorm(obs)\r\n    x <- runif(obs)  *eps\r\n    \r\n    \r\n    \r\n    y <- 2 + 0.5*x+eps\r\n    mod<-lm(y~x)\r\n    beta[i]<-mod$coefficients[[2]]\r\n    \r\n    \r\n    cc[i]=cor(x,eps)\r\n  }\r\n  \r\n  hist(beta,50)\r\n\r\n\r\n  mean(beta)\r\n\r\n[1] 2.007431\r\n\r\n  mean(cc)\r\n\r\n[1] 0.867978\r\n\r\ni.e. the mean of beta is much larger than 0.5\r\n\r\n\r\n\r\n",
    "preview": "posts/exercises/exercises4/selfservicetest.png",
    "last_modified": "2020-11-29T18:30:51+00:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/exercises/exercises7/",
    "title": "Exercises 7",
    "description": "Instrumental Variables",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2020-11-25",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\n\r\n\r\n\r\nExercise 7.1\r\nUse the dataset4.dta dataset. It contains weekly prices for rail transport of grain in the US midwest during the 1880s, and the quantity shipped. The railroad companies at the time operated a cartel, called the Joint Executive Committee (JEC), which is believed to have raised prices above the level that would have otherwise prevailed. This practice was legal before the Sherman Act of 1890 (antitrust legislation) was passed. From time to time, cheating by cartel members brought about a temporary collapse of the collusive price setting agreement. A dummy variable – “cartel”- in the data set indicates the period when price fixing was in effect.\r\n\r\n\r\nlibrary(haven)\r\nd4=read_dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AABQXwWvdZdvGlT6Y8LIfoMha/dataset4.dta?dl=1\")\r\nnames(d4)\r\n\r\n [1] \"week\"     \"price\"    \"cartel\"   \"quantity\" \"seas1\"    \"seas2\"   \r\n [7] \"seas3\"    \"seas4\"    \"seas5\"    \"seas6\"    \"seas7\"    \"seas8\"   \r\n[13] \"seas9\"    \"seas10\"   \"seas11\"   \"seas12\"   \"ice\"     \r\n\r\nPart (a)\r\nRun an OLS regression of the log quantity on the log price, controlling for ice, indicating that the Great Lakes were frozen preventing transport by ship, and a set of seasonal dummy variables (to capture seasonality in demand; note that dataset has 12 seasonal dummies; i.e. they tread every month as a season).\r\nWhat is the estimated price elasticity? \r\nDo you think you are estimating a demand curve? Explain. NoYes\r\nThink of what the the economic rationale is of including the variable “ice” in the regression?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nm1=lm(log(quantity)~log(price)+ice+seas1+seas2+seas3+seas4+seas5+seas6+seas7+seas8+seas9+seas10+seas11+seas12, data=d4)\r\ncoeftest(m1, vcov=vcovHC)\r\n\r\nt test of coefficients:\r\n\r\n              Estimate Std. Error t value  Pr(>|t|)    \r\n(Intercept)  8.8612335  0.1888154 46.9307 < 2.2e-16 ***\r\nlog(price)  -0.6388847  0.0754054 -8.4727 9.516e-16 ***\r\nice          0.4477537  0.1453460  3.0806  0.002249 ** \r\nseas1       -0.1328219  0.0987687 -1.3448  0.179671    \r\nseas2        0.0668882  0.0935005  0.7154  0.474909    \r\nseas3        0.1114364  0.0998924  1.1156  0.265464    \r\nseas4        0.1554218  0.1367509  1.1365  0.256604    \r\nseas5        0.1096585  0.1369305  0.8008  0.423836    \r\nseas6        0.0468326  0.1890620  0.2477  0.804521    \r\nseas7        0.1225525  0.2118166  0.5786  0.563290    \r\nseas8       -0.2350079  0.1874514 -1.2537  0.210886    \r\nseas9        0.0035606  0.1849417  0.0193  0.984652    \r\nseas10       0.1692468  0.1855618  0.9121  0.362430    \r\nseas11       0.2151843  0.1853410  1.1610  0.246519    \r\nseas12       0.2196331  0.1825211  1.2033  0.229758    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nThe regression implies a price elasticity of -0.64; i.e. a 1% increase in price will lead to a 0.64% reduction in demand. We can interpret this as a demand curve, if the price coefficient really represents the effect of a price change on demand holding all other things constant. One reason why this might not be the case is if shocks to demand cause price changes. On the other hand if we can control for big potential shocks to demand it will be possible to recover un-biased estimates. A big potential demand shock here is the freezing of the lakes: Because rail transport becomes the only option and hence demand increases it might lead to price changes. Indeed we see that “ice” has a significant positive effect on demand.\r\n\r\nPart (b)\r\nConsider using cartel as an instrument for price in order to identify the demand curve.\r\nIs the instrument is likely to satisfy the conditions for a valid instrument? NoYes\r\nCan you use the data to check these conditions? NoYesSomewhat\r\n\r\n\r\nAnswer:\r\n\r\nExogenous? Having a cartel is clearly a supply side factor that will by and large depend on the number of firms in the market and their respective manager’s ability to strike a deal, all factors un-related to demand. However, the incentive to form a cartel is also driven the amount of money to be made from having a cartel which is a demand side factor. Having said that, it is more likely that long term structural aspects of of demand are relevant for this rather than the weekly changes over a relatively short time period (6 years). Hence, it is plausible that we can use cartel as an IV.\r\nDriving explanatory variable? It is almost the definition of a cartel that it has a positive impact on price. At any rate this is something we can check via a first stage regression.\r\nExclusion? It is fairly plausible that the main effect of cartel on demand is via price. However, we can also imagine scenarios where the exclusion restriction might not hold in such a setting: imagine that the mere fact that the industry forms a cartel leads to some kind of negative press and consumer backlash perhaps with a boycott. Having said that, this is probably more of a concern with consumer goods rather than railway cargo transport.\r\n\r\nPart (c)\r\nEstimate the first stage and reduced form equations.\r\nWhat is the effect of cartel on price in the first stage? Not significantSignificant\r\nWhat is the effect of cartel on quantity in the reduced form? PositiveNegative\r\n\r\n\r\nAnswer:\r\n\r\nFirst stage suggests highly significant effect of cartel on price.\r\n\r\n\r\nfs=lm(log(price)~cartel+ice+seas1+seas2+seas3+seas4+seas5+seas6+seas7+seas8+seas9+seas10+seas11+seas12, data=d4)\r\ncoeftest(fs,vcov=vcovHC)\r\n\r\nt test of coefficients:\r\n\r\n              Estimate Std. Error  t value  Pr(>|t|)    \r\n(Intercept) -1.6937412  0.0729664 -23.2126 < 2.2e-16 ***\r\ncartel       0.3578984  0.0270566  13.2278 < 2.2e-16 ***\r\nice          0.0350030  0.0548972   0.6376  0.524193    \r\nseas1        0.0387253  0.0672492   0.5758  0.565132    \r\nseas2        0.1362884  0.0649431   2.0986  0.036655 *  \r\nseas3        0.1890486  0.0673860   2.8055  0.005339 ** \r\nseas4        0.0895226  0.0629519   1.4221  0.155999    \r\nseas5        0.0178628  0.0626018   0.2853  0.775572    \r\nseas6       -0.0257410  0.0753243  -0.3417  0.732779    \r\nseas7       -0.0671265  0.0802492  -0.8365  0.403526    \r\nseas8       -0.0358373  0.0755076  -0.4746  0.635390    \r\nseas9       -0.0057758  0.0771581  -0.0749  0.940376    \r\nseas10      -0.1002111  0.0745294  -1.3446  0.179733    \r\nseas11      -0.0867514  0.0780004  -1.1122  0.266909    \r\nseas12       0.0116931  0.0736123   0.1588  0.873891    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nlinearHypothesis(fs,\"cartel=0\")\r\n\r\nLinear hypothesis test\r\n\r\nHypothesis:\r\ncartel = 0\r\n\r\nModel 1: restricted model\r\nModel 2: log(price) ~ cartel + ice + seas1 + seas2 + seas3 + seas4 + seas5 + \r\n    seas6 + seas7 + seas8 + seas9 + seas10 + seas11 + seas12\r\n\r\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \r\n1    314 23.251                                  \r\n2    313 13.989  1    9.2616 207.22 < 2.2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nReduced form shows strong negative effect of cartel on quantity as would be expected.\r\n\r\n\r\nrf=lm(log(quantity)~cartel+ice+seas1+seas2+seas3+seas4+seas5+seas6+seas7+seas8+seas9+seas10+seas11+seas12, data=d4)\r\ncoeftest(rf,vcov=vcovHC)\r\n\r\nt test of coefficients:\r\n\r\n             Estimate Std. Error t value  Pr(>|t|)    \r\n(Intercept) 10.041308   0.193288 51.9499 < 2.2e-16 ***\r\ncartel      -0.310150   0.049899 -6.2156 1.631e-09 ***\r\nice          0.392601   0.152242  2.5788   0.01037 *  \r\nseas1       -0.164532   0.102436 -1.6062   0.10924    \r\nseas2       -0.027154   0.111576 -0.2434   0.80788    \r\nseas3       -0.027955   0.100929 -0.2770   0.78198    \r\nseas4        0.074932   0.134776  0.5560   0.57863    \r\nseas5        0.058082   0.149984  0.3873   0.69883    \r\nseas6        0.016243   0.201809  0.0805   0.93590    \r\nseas7        0.118403   0.221429  0.5347   0.59322    \r\nseas8       -0.262543   0.200101 -1.3121   0.19046    \r\nseas9       -0.053367   0.198743 -0.2685   0.78847    \r\nseas10       0.172652   0.199851  0.8639   0.38830    \r\nseas11       0.226969   0.197428  1.1496   0.25118    \r\nseas12       0.168523   0.196695  0.8568   0.39223    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\nPart (d)\r\nEstimate the demand function by IV. What is your estimated demand elasiticity? \r\nHow does it differ from your OLS estimate in (a)? More positiveMore negative\r\n\r\n\r\nAnswer:\r\n\r\nIV regression suggests a price elasticity of -0.86; i.e. the estimate has become stronger (more elastic; i.e. more negative) compared to OLS. This is consistent with positive demand shocks in exerting a positive influence on price (i.e. the OLS estimate would be biased upward so that it becomes less negative than is true in reality)\r\n\r\n\r\nlibrary(AER)\r\niv=ivreg(log(quantity)~log(price)+ice+seas1+seas2+seas3+seas4+seas5+seas6+seas7+seas8+seas9+seas10+seas11+seas12| cartel+ice+seas1+seas2+seas3+seas4+seas5+seas6+seas7+seas8+seas9+seas10+seas11+seas12, data=d4)\r\nrobust.se(iv)\r\n\r\n[1] \"Robust Standard Errors\"\r\n\r\nt test of coefficients:\r\n\r\n              Estimate Std. Error t value  Pr(>|t|)    \r\n(Intercept)  8.5735347  0.2106483 40.7007 < 2.2e-16 ***\r\nlog(price)  -0.8665866  0.1307362 -6.6285 1.484e-10 ***\r\nice          0.4229339  0.1315103  3.2160  0.001436 ** \r\nseas1       -0.1309732  0.1005382 -1.3027  0.193628    \r\nseas2        0.0909521  0.0927421  0.9807  0.327498    \r\nseas3        0.1358720  0.0980894  1.3852  0.166982    \r\nseas4        0.1525107  0.1313612  1.1610  0.246525    \r\nseas5        0.0735617  0.1271374  0.5786  0.563275    \r\nseas6       -0.0060642  0.1721703 -0.0352  0.971925    \r\nseas7        0.0602322  0.1964208  0.3066  0.759315    \r\nseas8       -0.2935992  0.1707605 -1.7194  0.086537 .  \r\nseas9       -0.0583723  0.1714096 -0.3405  0.733676    \r\nseas10       0.0858107  0.1738155  0.4937  0.621872    \r\nseas11       0.1517910  0.1716185  0.8845  0.377123    \r\nseas12       0.1786557  0.1668587  1.0707  0.285129    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\nPart (e)\r\nMicroeconomic theory suggests that a monopolist (like the cartel) should operate in a region of the demand curve where demand is elastic (i.e. the elasticity is <= -1). The estimate in c) is clearly larger than -1. Can we therefore conclude in a statistically significant way that the demand curve is in-elastic and therefore at odds with economic theory?\r\nNoYes\r\n\r\n\r\nAnswer:\r\n\r\nThe estimated elasticity is -0.8 in the IV result and thus larger than -1 and therefore non-elastic. That said, we know that even if the demand is actually ealstic (i.e. <=-1) we might still end up with an in-elastic estimate when estimating the elasticity from a sample of data. More formally we might want to test the hypothesis that the demand curve is equal to -1 (i.e. the highest value that would still allow to claim it is elastic) againts the alternative that it is in-elastic; i.e.\r\nHence we are tesing \\(H0: \\beta(lnprice)=-1 \\textrm{ vs } H1: \\beta(lnprice)>-1\\). Note that this is a one sided test, implying that we are only worried about being wrong if the value is larger. This only affects which critical threshold we require. For a 5% significance level we don’t need to divid 5% by 2 to find the threshold. Rather, the threshold becomes: qnorm(0.95)=1.6448536. Hence, we would reject that our demand is in-elastic if we find that our t-value is larger than this threshold. We can calculate our t-value here as:\r\n\\[t-stat=(-0.8666+1)/0.131= 1.0204783\\]\r\nwhich is not bigger than 1.6448536 , so we cannot reject the hypothesis that we are actually dealing with an in-elastic demand curve.\r\nAs further illustration for this exercise consider also the following diagram:  Note that we can work out the shape of a demand curve if we know that the demand curve stays fixed and prices vary for reasons other than shifts in demand (such as a shifts in marginal costs or price increases due to monopoly/cartel power). However, in practice we don’t know what exactly moved prices and demand from one datapoint to the next. We can make progress by controlling for (some) of the stuff that clearly will shift demand (e.g. ice preventing alternative forms of transport). In addition we can focus exclusively on price movements that are brought about by supply side price movements if we have variables (such as the existence of a Cartel) that we can assume to be exclusively driving the supply side.\r\n\r\n\r\n\r\n",
    "preview": "posts/exercises/exercises7/20131205_Istanbul_321_cropped.jpg",
    "last_modified": "2020-11-25T13:35:03+00:00",
    "input_file": {}
  },
  {
    "path": "posts/exercises/exercises5/",
    "title": "Exercises 5",
    "description": "Becoming a Data Detective: Uncovering Racial Bias & more",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2020-11-23",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\n\r\n\r\n\r\nExercise 5.1\r\nFor this question we will use a dataset from a randomized experiment conducted by Marianne Bertrand and Sendhil Mullainathan, who sent 4,870 fictitious resumes out to employers in response to job adverts in Boston and Chicago in 2001. The resumes differ in various attributes including the names of the applicants, and different resumes were randomly allocated to job openings. Some of the names are distinctly white sounding and some distinctly black sounding. The researchers collecting these data were interested to learn whether black sounding names obtain fewer callbacks for interviews than white names. Load the data set bm.dta.\r\n\r\n\r\n  library(haven)\r\n  data <- read_dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AABua74TH54FcmOsAs0ayMY5a/bm.dta?dl=1\")\r\n  summary(data)\r\n\r\n   education         ofjobs         yearsexp      computerskills  \r\n Min.   :0.000   Min.   :1.000   Min.   : 1.000   Min.   :0.0000  \r\n 1st Qu.:3.000   1st Qu.:3.000   1st Qu.: 5.000   1st Qu.:1.0000  \r\n Median :4.000   Median :4.000   Median : 6.000   Median :1.0000  \r\n Mean   :3.618   Mean   :3.661   Mean   : 7.843   Mean   :0.8205  \r\n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.: 9.000   3rd Qu.:1.0000  \r\n Max.   :4.000   Max.   :7.000   Max.   :44.000   Max.   :1.0000  \r\n      call             female           black    \r\n Min.   :0.00000   Min.   :0.0000   Min.   :0.0  \r\n 1st Qu.:0.00000   1st Qu.:1.0000   1st Qu.:0.0  \r\n Median :0.00000   Median :1.0000   Median :0.5  \r\n Mean   :0.08049   Mean   :0.7692   Mean   :0.5  \r\n 3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.0  \r\n Max.   :1.00000   Max.   :1.0000   Max.   :1.0  \r\n\r\nPart (a)\r\nThe data set contains two dummy variables (0-1 variables) for gender (female) and whether the applicant has computer skills (computerskills). Tabulate these variables by black.\r\nDo gender and computer skills look balanced – i.e. random - across race groups? NoYes\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  table(data$female,data$black)\r\n\r\n   \r\n       0    1\r\n  0  575  549\r\n  1 1860 1886\r\n\r\n  prop.table(table(data$female,data$black))\r\n\r\n   \r\n            0         1\r\n  0 0.1180698 0.1127310\r\n  1 0.3819302 0.3872690\r\n\r\nThere are many more female than male CVs (i.e. about 38%+38%=76% of the sample are female). However, gender seems not all correlated with race; i.e. the split between black and non black is virtually half for both men and women.\r\nA similar pattern emerges for computer skills and racial background.\r\n\r\n\r\n  prop.table(table(data$computerskills,data$black))\r\n\r\n   \r\n             0          1\r\n  0 0.09568789 0.08377823\r\n  1 0.40431211 0.41622177\r\n\r\n\r\nPart (b)\r\nDo a similar tabulation for education and the number of jobs previous held (ofjobs). These variables take on 5 and 7 different values, respectively.\r\nDoes education and the number of previous jobs look balanced across race groups? NoYes\r\nTo be sure, run a regression. Are the differences significant? YesNo\r\n\r\n\r\nHint:\r\n\r\nYou can tabulate to eye-ball proportions but if you want to be sure that there aren’t small discrepancies, it is always best to run a regression.\r\n\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  table(data$education,data$black)\r\n\r\n   \r\n       0    1\r\n  0   18   28\r\n  1   18   22\r\n  2  142  132\r\n  3  513  493\r\n  4 1744 1760\r\n\r\n  prop.table(table(data$education,data$black))\r\n\r\n   \r\n              0           1\r\n  0 0.003696099 0.005749487\r\n  1 0.003696099 0.004517454\r\n  2 0.029158111 0.027104723\r\n  3 0.105338809 0.101232033\r\n  4 0.358110883 0.361396304\r\n\r\n  table(data$ofjobs,data$black)\r\n\r\n   \r\n      0   1\r\n  1  54  56\r\n  2 347 357\r\n  3 726 703\r\n  4 800 811\r\n  5 258 275\r\n  6 243 221\r\n  7   7  12\r\n\r\n  prop.table(table(data$ofjobs,data$black))\r\n\r\n   \r\n              0           1\r\n  1 0.011088296 0.011498973\r\n  2 0.071252567 0.073305955\r\n  3 0.149075975 0.144353183\r\n  4 0.164271047 0.166529774\r\n  5 0.052977413 0.056468172\r\n  6 0.049897331 0.045379877\r\n  7 0.001437372 0.002464066\r\n\r\nThere is no clear relation between education/number of jobs and race either. If we are worried about small discrepancies we could also run a regression to test if differences are significant:\r\n\r\n\r\n data$ofjobs=factor(data$ofjobs)\r\n summary(lm(black~ofjobs,data))\r\n\r\nCall:\r\nlm(formula = black ~ ofjobs, data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.63158 -0.50341 -0.05394  0.49659  0.52371 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.509091   0.047690  10.675   <2e-16 ***\r\nofjobs2     -0.001989   0.051281  -0.039    0.969    \r\nofjobs3     -0.017138   0.049492  -0.346    0.729    \r\nofjobs4     -0.005677   0.049291  -0.115    0.908    \r\nofjobs5      0.006857   0.052381   0.131    0.896    \r\nofjobs6     -0.032798   0.053043  -0.618    0.536    \r\nofjobs7      0.122488   0.124264   0.986    0.324    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.5002 on 4863 degrees of freedom\r\nMultiple R-squared:  0.0007238, Adjusted R-squared:  -0.0005091 \r\nF-statistic: 0.587 on 6 and 4863 DF,  p-value: 0.741\r\n\r\nHence the different job categories are individually (and jointly) insignificant. Note an OLS regression always reports a test that all coefficients are jointly not significant (the p-value of 0.741 reported at the end of the output).\r\n\r\nPart (c)\r\nLook at the mean and standard deviation for the variable for years of experience (yearsexp) separately for black and whites.\r\nDoes this variable look similar by race? NoYes\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mean(data$yearsexp[data$black==1])\r\n\r\n[1] 7.829569\r\n\r\n  sd(data$yearsexp[data$black==1])\r\n\r\n[1] 5.010764\r\n\r\n  mean(data$yearsexp[data$black==0])\r\n\r\n[1] 7.856263\r\n\r\n  sd(data$yearsexp[data$black==0])\r\n\r\n[1] 5.079228\r\n\r\n\r\nPart (d)\r\nWhat do you make of the overall results on resume characteristics?\r\nIs it important to figure out if these variables look similar across the race groups? NoYes Think why or why not.\r\n\r\n\r\nAnswer:\r\n\r\nIf there was any evidence of a systematic relationship between race and any of those characteristics we could potentially be in trouble when simply comparing interview call backs for different race groups. Any differences found could simply due to those other factors rather than racial bias by employers.\r\n\r\nPart (e)\r\nThe variable of interest on the data set is the variable call, which indicates a call back for an interview.\r\nWhat percentage of people receive a call back (rounded up to 2 decimal places)? \r\nDo you find differences in call back rates by race? NoYes\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  prop.table(table(data$call))\r\n\r\n         0          1 \r\n0.91950719 0.08049281 \r\n\r\n  prop.table(table(data$call,data$black))\r\n\r\n   \r\n             0          1\r\n  0 0.45174538 0.46776181\r\n  1 0.04825462 0.03223819\r\n\r\n  prop.table(table(data$call,data$black),2) \r\n\r\n   \r\n             0          1\r\n  0 0.90349076 0.93552361\r\n  1 0.09650924 0.06447639\r\n\r\n  # Note that by specifying 2 we report proportions by column (if you specify 1 it reports proportions by rows)\r\n\r\nWe see that most CVs never received a call back (i.e. overall only 8.05% received a call back).\r\n\r\nPart (f)\r\nWhat do you conclude from the results of the Bertand and Mullainathan experiment?\r\nAre black people as likely to receive a call back as white people? YesNo\r\n\r\n\r\nAnswer:\r\n\r\nThere seems to be a clear difference between the races. Whereas for white people call back rates where above average (9.65%) they were below average for black people (6.45%) suggesting a racial bias by employers.\r\n\r\nExercise 5.2\r\nLets use the dataset from the experiment by Bertand and Mullainathan (bm.dta) again.\r\n\r\n\r\n  library(foreign)\r\n  data <- read_dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AABua74TH54FcmOsAs0ayMY5a/bm.dta?dl=1\")\r\n\r\nPart (a)\r\nDevelop a regression to examine if the difference in interview callbacks between black and white “sounding” CVs is significantly different.\r\nWhat is the code for the simplest linear regression (we call this model “mod”)? mod <- \r\nWhata is the code for the simplest logit regression (we call this model “mod2”)? mod2 <- \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod <- lm(call~black,data)\r\n  mod2 <- glm(call~black,data,family=binomial)\r\n\r\n\r\nPart (b)\r\nExecute both regression models in turn.\r\nAre the results similar? NoYes\r\n\r\n\r\nAnswer:\r\n\r\nLinear regression:\r\n\r\n\r\n  mod <- lm(call~black,data)\r\n  summary(mod)\r\n\r\nCall:\r\nlm(formula = call ~ black, data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.09651 -0.09651 -0.06448 -0.06448  0.93552 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.096509   0.005505  17.532  < 2e-16 ***\r\nblack       -0.032033   0.007785  -4.115 3.94e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.2716 on 4868 degrees of freedom\r\nMultiple R-squared:  0.003466,  Adjusted R-squared:  0.003261 \r\nF-statistic: 16.93 on 1 and 4868 DF,  p-value: 3.941e-05\r\n\r\nAlternatively we can use logit:\r\n\r\n\r\n  mod2 <- glm(call~black,data,family=binomial)\r\n  summary(mod2)\r\n\r\nCall:\r\nglm(formula = call ~ black, family = binomial, data = data)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-0.4505  -0.4505  -0.3651  -0.3651   2.3416  \r\n\r\nCoefficients:\r\n            Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept) -2.23663    0.06863 -32.590  < 2e-16 ***\r\nblack       -0.43818    0.10732  -4.083 4.45e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 2726.9  on 4869  degrees of freedom\r\nResidual deviance: 2709.9  on 4868  degrees of freedom\r\nAIC: 2713.9\r\n\r\nNumber of Fisher Scoring iterations: 5\r\n\r\n  library(margins)\r\n  margins(mod2) \r\n\r\n    black\r\n -0.03232\r\n\r\nNote that the marginal effects using logit are very similar to the linear model.\r\n\r\nExercise 5.3\r\nFor this question, download the data set cps.dta, which comes from the responses to the monthly US Current Population Survey (CPS) in 2001, a large labour market survey. This data set contains data on 8,891 individuals living in Boston and Chicago. We want to use these data to compare the skills of real live blacks and whites (as opposed to made up CVs), and their employment outcomes and see how they differ from the findings in the exercises involving the bm.dta dataset.\r\n\r\n\r\n  library(foreign)\r\n  data <- read.dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AAAOb_v-Y2V0NN4-rxahZjl4a/cps.dta?dl=1\")\r\n  summary(data)\r\n\r\n employed        black            female          education   \r\n No  :1942   Min.   :0.0000   Min.   :0.0000   HSD     : 980  \r\n Yes :6931   1st Qu.:0.0000   1st Qu.:0.0000   HSG     :2358  \r\n NA's:  18   Median :0.0000   Median :1.0000   some col:2460  \r\n             Mean   :0.1508   Mean   :0.5161   col+    :3093  \r\n             3rd Qu.:0.0000   3rd Qu.:1.0000                  \r\n             Max.   :1.0000   Max.   :1.0000                  \r\n    yearsexp    \r\n Min.   : 0.00  \r\n 1st Qu.:11.00  \r\n Median :21.00  \r\n Mean   :20.94  \r\n 3rd Qu.:30.00  \r\n Max.   :59.00  \r\n\r\nPart (a)\r\nThe data set contains a variable education, which takes on four values (high school dropouts, high school graduates, some college, and college degree and more). Use the education variable to create a new dummy for resumes indicating some college or more (i.e. those in the “some college” category plus those in the college and more category).\r\nWhat percentage of respondents has at least some college education (rounded up to the nearest percent)? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  HE <- as.integer(data$education==\"some col\" | data$education==\"col+\")\r\n  mean(HE)\r\n\r\n[1] 0.6245642\r\n\r\n  sum(HE)\r\n\r\n[1] 5553\r\n\r\n  #adding HE variable to dataframe\r\n  data[\"HE\"] <- HE\r\n  summary(data)\r\n\r\n employed        black            female          education   \r\n No  :1942   Min.   :0.0000   Min.   :0.0000   HSD     : 980  \r\n Yes :6931   1st Qu.:0.0000   1st Qu.:0.0000   HSG     :2358  \r\n NA's:  18   Median :0.0000   Median :1.0000   some col:2460  \r\n             Mean   :0.1508   Mean   :0.5161   col+    :3093  \r\n             3rd Qu.:0.0000   3rd Qu.:1.0000                  \r\n             Max.   :1.0000   Max.   :1.0000                  \r\n    yearsexp           HE        \r\n Min.   : 0.00   Min.   :0.0000  \r\n 1st Qu.:11.00   1st Qu.:0.0000  \r\n Median :21.00   Median :1.0000  \r\n Mean   :20.94   Mean   :0.6246  \r\n 3rd Qu.:30.00   3rd Qu.:1.0000  \r\n Max.   :59.00   Max.   :1.0000  \r\n\r\nNote that nearly 63% of the sample have some college education.\r\n\r\nPart (b)\r\nFirst make employed into dichotomous variable and then conduct a regression analysis of the chances of being employed for people with different racial backgrounds.\r\nAre black people significantly less likely to be employed? NoYes\r\nIf yes, what is the percentage difference (rounded to the nearest percentage point)? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  #modify employed as a dichotomous variable\r\n  table(data$employed)\r\n\r\n  No  Yes \r\n1942 6931 \r\n\r\n  table(as.integer(data$employed))\r\n\r\n   1    2 \r\n1942 6931 \r\n\r\n  data[\"employed2\"] <- as.integer(data$employed)-1\r\n  summary(data)\r\n\r\n employed        black            female          education   \r\n No  :1942   Min.   :0.0000   Min.   :0.0000   HSD     : 980  \r\n Yes :6931   1st Qu.:0.0000   1st Qu.:0.0000   HSG     :2358  \r\n NA's:  18   Median :0.0000   Median :1.0000   some col:2460  \r\n             Mean   :0.1508   Mean   :0.5161   col+    :3093  \r\n             3rd Qu.:0.0000   3rd Qu.:1.0000                  \r\n             Max.   :1.0000   Max.   :1.0000                  \r\n                                                              \r\n    yearsexp           HE           employed2     \r\n Min.   : 0.00   Min.   :0.0000   Min.   :0.0000  \r\n 1st Qu.:11.00   1st Qu.:0.0000   1st Qu.:1.0000  \r\n Median :21.00   Median :1.0000   Median :1.0000  \r\n Mean   :20.94   Mean   :0.6246   Mean   :0.7811  \r\n 3rd Qu.:30.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \r\n Max.   :59.00   Max.   :1.0000   Max.   :1.0000  \r\n                                  NA's   :18      \r\n\r\n  mod1 <- lm(employed2~black,data)\r\n  summary(mod1)\r\n\r\nCall:\r\nlm(formula = employed2 ~ black, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-0.7949  0.2051  0.2051  0.2051  0.2964 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.794879   0.004748  167.40  < 2e-16 ***\r\nblack       -0.091286   0.012237   -7.46 9.48e-14 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4122 on 8871 degrees of freedom\r\n  (18 observations deleted due to missingness)\r\nMultiple R-squared:  0.006234,  Adjusted R-squared:  0.006122 \r\nF-statistic: 55.65 on 1 and 8871 DF,  p-value: 9.481e-14\r\n\r\nA regression of employment status on a dummy indicating a black racial background suggests that black people are significantly less likely to be employed. The difference amounts to 9.1 percentage points.\r\n\r\nPart (c)\r\nConduct a regression analysis of the chances of having college education for people with different racial backgrounds.\r\nWhat is the percentage difference (rounded to the nearest percentage point)? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod2 <- lm(HE~black,data)\r\n  summary(mod2)\r\n\r\nCall:\r\nlm(formula = HE ~ black, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-0.6420 -0.6420  0.3580  0.3580  0.4735 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.641987   0.005553 115.607  < 2e-16 ***\r\nblack       -0.115514   0.014299  -8.078 7.41e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4825 on 8889 degrees of freedom\r\nMultiple R-squared:  0.007288,  Adjusted R-squared:  0.007177 \r\nF-statistic: 65.26 on 1 and 8889 DF,  p-value: 7.414e-16\r\n\r\nThe difference in the probability of having higher education between people with non black and black background is 11 percentage points.\r\n\r\nPart (d)\r\nOn the basis of your evidence what can you conclude about racial discrimination in the US labor market?\r\nDo we see a open-and-shut case of racial bias here? YesNo\r\nThink of the potential caveats and alternative explanations.\r\nWhat analysis could you undertake to address some of these caveats? None, it’s fine as it isRun a regression holding education level constantInclude parental income data and run a regression holding this variable constant\r\n\r\n\r\nAnswer:\r\n\r\nThe result in (b) is consistent with racial bias. However, from (c) we also see that people with black background tend to have less college education and college education is another major driver of being employed:\r\n\r\n\r\n  summary(lm(employed2~HE,data))\r\n\r\nCall:\r\nlm(formula = employed2 ~ HE, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-0.8206  0.1794  0.1794  0.2845  0.2845 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 0.715486   0.007109  100.64   <2e-16 ***\r\nHE          0.105124   0.008996   11.69   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4104 on 8871 degrees of freedom\r\n  (18 observations deleted due to missingness)\r\nMultiple R-squared:  0.01516,   Adjusted R-squared:  0.01505 \r\nF-statistic: 136.5 on 1 and 8871 DF,  p-value: < 2.2e-16\r\n\r\nHence, far from implying a racial issue, the result in (b) could simply reflect employers preference for more highly educated workers. We can examine this by doing the analysis in (b) separately for workers with different educational attainment; i.e.:\r\n\r\n\r\n  mod3 <- lm(employed2~black,data,subset=data$HE==1)\r\n  summary(mod3)\r\n\r\nCall:\r\nlm(formula = employed2 ~ black, data = data, subset = data$HE == \r\n    1)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-0.8260  0.1740  0.1740  0.1740  0.2162 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.825961   0.005513 149.809  < 2e-16 ***\r\nblack       -0.042177   0.015479  -2.725  0.00645 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.3835 on 5539 degrees of freedom\r\n  (12 observations deleted due to missingness)\r\nMultiple R-squared:  0.001339,  Adjusted R-squared:  0.001158 \r\nF-statistic: 7.425 on 1 and 5539 DF,  p-value: 0.006453\r\n\r\n  mod4 <- lm(employed2~black,data,subset=data$HE==0)\r\n  summary(mod4)\r\n\r\nCall:\r\nlm(formula = employed2 ~ black, data = data, subset = data$HE == \r\n    0)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-0.7392 -0.6145  0.2608  0.2608  0.3855 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.739163   0.008636   85.59  < 2e-16 ***\r\nblack       -0.124629   0.019814   -6.29 3.59e-10 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4487 on 3330 degrees of freedom\r\n  (6 observations deleted due to missingness)\r\nMultiple R-squared:  0.01174,   Adjusted R-squared:  0.01144 \r\nF-statistic: 39.56 on 1 and 3330 DF,  p-value: 3.588e-10\r\n\r\nThe results suggest that for either group there is a significant racial gap when it comes to being employed. Note that the effect is considerably stronger for less educated workers. Hence this re-enforces the hypothesis that there is discrimination against workers with black background which is un-related to their productivity in the workplace.\r\nHowever, there might be further caveats: our simple regression cannot account for the quality of the college education which can vary considerably and might vary systematically along racial lines. Furthermore, an important driver for a good education and for various other skills might have to do with parental income and status. Again, this is likely to vary systematically along racial lines.\r\nWhile it is interesting to ask if employers discriminate above and beyond what could be expected on the basis of education and skill of workers – which is what we were implicitly doing above - we might also be concerned about the overall impact of racial background on labor market outcomes which includes initially different educational outcomes. Hence, depending on our interest we might be primarily focused on the effect of race holding education fixed or we might be focused on the overall effect.\r\n\r\nExercise 5.4\r\nConsider once more the cps.dta dataset. In exercise 3 we argued that not accounting for education might bias our estimate of the impact of racial background. We then examined the issue by looking at college educated vs not college educated workers separately.\r\n\r\n\r\n  library(foreign)\r\n  data <- read.dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AAAOb_v-Y2V0NN4-rxahZjl4a/cps.dta?dl=1\")\r\n  summary(data)\r\n\r\n employed        black            female          education   \r\n No  :1942   Min.   :0.0000   Min.   :0.0000   HSD     : 980  \r\n Yes :6931   1st Qu.:0.0000   1st Qu.:0.0000   HSG     :2358  \r\n NA's:  18   Median :0.0000   Median :1.0000   some col:2460  \r\n             Mean   :0.1508   Mean   :0.5161   col+    :3093  \r\n             3rd Qu.:0.0000   3rd Qu.:1.0000                  \r\n             Max.   :1.0000   Max.   :1.0000                  \r\n    yearsexp    \r\n Min.   : 0.00  \r\n 1st Qu.:11.00  \r\n Median :21.00  \r\n Mean   :20.94  \r\n 3rd Qu.:30.00  \r\n Max.   :59.00  \r\n\r\nPart (a)\r\nFirst add the HE (higher education) variable to the data frame and modify the employed variable as a dichotomous variable calling it “employed2”. Can you propose an alternative strategy using a multivariate regression approach?\r\nWhat is the command for a linear regression with two variables (we call this model “mod1”)? mod1 <- \r\nWhat is the command for a slightly more complex linear regression with two variables and their interaction (we call this model “mod2”)? mod2 <- \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  #adding HE variable to dataframe\r\n  data[\"HE\"] <- as.integer(data$education==\"some col\" | data$education==\"col+\")\r\n  #modify employed as a dichotomous variable\r\n  data[\"employed2\"] <- as.integer(data$employed)-1\r\n  mod1 <- lm(employed2~black+HE,data)\r\n  summary(mod1)\r\n\r\nCall:\r\nlm(formula = employed2 ~ black + HE, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-0.8307  0.1693  0.1693  0.2694  0.3491 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.730628   0.007462  97.917  < 2e-16 ***\r\nblack       -0.079705   0.012198  -6.534 6.75e-11 ***\r\nHE           0.100094   0.009008  11.111  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4094 on 8870 degrees of freedom\r\n  (18 observations deleted due to missingness)\r\nMultiple R-squared:  0.01988,   Adjusted R-squared:  0.01966 \r\nF-statistic: 89.94 on 2 and 8870 DF,  p-value: < 2.2e-16\r\n\r\n  mod2 <- lm(employed2~black+HE+black*HE,data)\r\n  summary(mod2)\r\n\r\nCall:\r\nlm(formula = employed2 ~ black + HE + black * HE, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-0.8260  0.1740  0.1740  0.2608  0.3855 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.739163   0.007876  93.849  < 2e-16 ***\r\nblack       -0.124629   0.018070  -6.897 5.68e-12 ***\r\nHE           0.086798   0.009831   8.829  < 2e-16 ***\r\nblack:HE     0.082451   0.024481   3.368  0.00076 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4092 on 8869 degrees of freedom\r\n  (18 observations deleted due to missingness)\r\nMultiple R-squared:  0.02113,   Adjusted R-squared:  0.0208 \r\nF-statistic: 63.81 on 3 and 8869 DF,  p-value: < 2.2e-16\r\n\r\n\r\nPart (b)\r\nThere are potentially two models you might have used in part (a) one that implies that the effect of race is the same for both educational groups or one that implies the effect of race is different for different educational groups.\r\nWhich model is more appropriate? The one without the interaction termThe one with the interaction term\r\n\r\n\r\nAnswer:\r\n\r\nThe test for the interaction coefficient is significant. That suggest the negative “black” effect is significatly less strong for higher educated workers; i.e. it’s important to have the more complicated model with interaction term.\r\nSome alternative ways of testing include:\r\n\r\n\r\n  anova(mod2,mod1)\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: employed2 ~ black + HE + black * HE\r\nModel 2: employed2 ~ black + HE\r\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \r\n1   8869 1484.9                                  \r\n2   8870 1486.8 -1   -1.8992 11.344 0.0007603 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n  # or alternatively\r\n  library(lmtest)\r\n  waldtest(mod2,mod1)\r\n\r\nWald test\r\n\r\nModel 1: employed2 ~ black + HE + black * HE\r\nModel 2: employed2 ~ black + HE\r\n  Res.Df Df      F    Pr(>F)    \r\n1   8869                        \r\n2   8870 -1 11.344 0.0007603 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n  # or alternatively\r\n  coeftest(mod2)\r\n\r\nt test of coefficients:\r\n\r\n              Estimate Std. Error t value  Pr(>|t|)    \r\n(Intercept)  0.7391627  0.0078761 93.8488 < 2.2e-16 ***\r\nblack       -0.1246287  0.0180702 -6.8969 5.677e-12 ***\r\nHE           0.0867985  0.0098305  8.8295 < 2.2e-16 ***\r\nblack:HE     0.0824513  0.0244806  3.3680 0.0007603 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\nPart (c)\r\nOn the basis of your regressions in (b), what is the racial gap for college educated people in percentage points (to 1 decimal place)? \r\nWhat is the racial gap for non college educated people in percentage points (to 1 decimal place)? \r\nThink of how this compares to your findings in Exercise 5.3.\r\n\r\n\r\nAnswer:\r\n\r\nCollege educated black people have a 12.5-8.2=4.3 percentage points lower likelihood of being employed. For non college educated the propbability is 12.5 percentage points lower; i.e. short of rounding error this corresponds to exactly what we found in exercise 3 as well.\r\n\r\nExercise 5.5\r\nUse wage1.dta and examine once more the relationship between education and wages.\r\n\r\n\r\n  library(foreign)\r\n  library(car)\r\n  data <- read.dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AADZ4MZhDDk9R8sFSjBvmcRma/WAGE1.DTA?dl=1\")\r\n\r\nWould you say the relationship is different for men and women? NoYes\r\n\r\n\r\nAnswer:\r\n\r\nWe can allow for a separate response for women by including a female dummy and also interact the education variable with gender status. Note that you can do this by creating the various variables first. A faster way is to use R’s ability to create new variables as part of the model description of the lm command:\r\n\r\n\r\n  mod2 <- lm(wage~educ+educ*female,data)\r\n  summary(mod2)\r\n\r\nCall:\r\nlm(formula = wage ~ educ + educ * female, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6.1611 -1.8028 -0.6367  1.0054 15.5258 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  0.20050    0.84356   0.238    0.812    \r\neduc         0.53948    0.06422   8.400 4.24e-16 ***\r\nfemale      -1.19852    1.32504  -0.905    0.366    \r\neduc:female -0.08600    0.10364  -0.830    0.407    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.186 on 522 degrees of freedom\r\nMultiple R-squared:  0.2598,    Adjusted R-squared:  0.2555 \r\nF-statistic: 61.07 on 3 and 522 DF,  p-value: < 2.2e-16\r\n\r\n  linearHypothesis(mod2,c(\"educ:female=0\",\"female=0\"))\r\n\r\nLinear hypothesis test\r\n\r\nHypothesis:\r\neduc:female = 0\r\nfemale = 0\r\n\r\nModel 1: restricted model\r\nModel 2: wage ~ educ + educ * female\r\n\r\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \r\n1    524 5980.7                                  \r\n2    522 5300.2  2    680.51 33.511 2.031e-14 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nNote that this leads to “female” parameters that individually are not significant. However, a joint significance test reveals that the variables combined have explanatory power; i.e. the low significance in the regression is likely a consequence of co-linearity of the female and female*education variables. We can also see this by just regressing on a female dummy which leads to a highly significant (and negative) effect (as we have seen before).\r\n\r\n\r\n    summary( lm(wage~educ+female:educ,data))\r\n\r\nCall:\r\nlm(formula = wage ~ educ + female:educ, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6.3234 -1.7394 -0.6856  1.0567 15.5795 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -0.28526    0.65041  -0.439    0.661    \r\neduc         0.57548    0.05039  11.421  < 2e-16 ***\r\neduc:female -0.17764    0.02183  -8.138 2.95e-15 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.186 on 523 degrees of freedom\r\nMultiple R-squared:  0.2586,    Adjusted R-squared:  0.2558 \r\nF-statistic: 91.23 on 2 and 523 DF,  p-value: < 2.2e-16\r\n\r\nYou might consequently ask, which is the right model? There are several considerations:\r\nWork with the more general model even though the coefficients are individually not significant what matters is that they jointly matter.\r\nConsider theory: is it more plausible that there are other factors that imply that women have a lower wage whatever they do or is it more plausible that the impact of education is radically different? The former seems more plausible to me but you are free to differ.\r\n\r\nExercise 5.6\r\nUse the production2.dta dataset. This data set contains data on output (value added) and inputs at the industry level for 459 industries in 1958 and 1993.\r\n\r\n\r\n  library(foreign)\r\n  library(lmtest)\r\n  library(car)\r\n  data <- read.dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AAB8jv5xON_4OlZG50sj2l-xa/production2.dta?dl=1\")\r\n  summary(data)\r\n\r\n      sic            year           emp             vship         \r\n Min.   :2011   Min.   :58.0   Min.   :  0.10   Min.   :    24.9  \r\n 1st Qu.:2437   1st Qu.:58.0   1st Qu.: 10.30   1st Qu.:   323.3  \r\n Median :3221   Median :75.5   Median : 19.30   Median :  1026.6  \r\n Mean   :3039   Mean   :75.5   Mean   : 35.59   Mean   :  3768.3  \r\n 3rd Qu.:3560   3rd Qu.:93.0   3rd Qu.: 40.05   3rd Qu.:  3493.3  \r\n Max.   :3999   Max.   :93.0   Max.   :511.40   Max.   :167825.8  \r\n    matcost              vadd             energy       \r\n Min.   :     8.1   Min.   :    9.1   Min.   :   0.20  \r\n 1st Qu.:   155.4   1st Qu.:  155.7   1st Qu.:   3.20  \r\n Median :   510.9   Median :  492.0   Median :  13.85  \r\n Mean   :  1997.6   Mean   : 1773.3   Mean   :  71.80  \r\n 3rd Qu.:  1726.9   3rd Qu.: 1699.1   3rd Qu.:  53.15  \r\n Max.   :120458.8   Max.   :47272.0   Max.   :3780.60  \r\n      cap         \r\n Min.   :    3.7  \r\n 1st Qu.:  292.2  \r\n Median :  682.6  \r\n Mean   : 1974.8  \r\n 3rd Qu.: 1711.6  \r\n Max.   :61909.7  \r\n\r\nSuppose the relationship between output and inputs is described by a Cobb-Douglas production function: \\[Y_{i} = AK_{\\alpha}^{i}L_{\\beta}^{i}\\]\r\nwhere \\(Y_{i}\\) is a measure of output, \\(K_{i}\\) is the capital stock, and \\(L_{i}\\) is employment. Answer all questions for the year 1958 only.\r\nPart (a)\r\nTransform the production function to a linear equation by taking logs. Estimate the parameters and by an OLS regression using total value added as your measure of output.\r\nWhat coefficient on the log(emp) variable (rounded to 3 decimal places)? \r\nWhat coefficient on the log(cap) variable (rounded to 3 decimal places)? \r\n\r\n\r\nAnswer:\r\n\r\nWe can regress the following linear regression model:\r\n\\[\\ln Y=\\alpha \\ln K +\\beta \\ln L +\\varepsilon\\]\r\n\r\n\r\n  mod1 <- lm(log(vadd)~log(emp)+log(cap),data[data$year==58,])\r\n  summary(mod1)\r\n\r\nCall:\r\nlm(formula = log(vadd) ~ log(emp) + log(cap), data = data[data$year == \r\n    58, ])\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.72254 -0.16410 -0.02135  0.16161  1.17701 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  1.41898    0.06172   22.99   <2e-16 ***\r\nlog(emp)     0.69748    0.01868   37.34   <2e-16 ***\r\nlog(cap)     0.27609    0.01430   19.30   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.2754 on 456 degrees of freedom\r\nMultiple R-squared:  0.9269,    Adjusted R-squared:  0.9266 \r\nF-statistic:  2893 on 2 and 456 DF,  p-value: < 2.2e-16\r\n\r\n\r\nPart (b)\r\nTest whether your estimates are consistent with the production function exhibiting constant returns to scale, i.e. \r\n\\[H_{0}: \\alpha+\\beta=1\\]\r\nagainst the alternative\r\n\\[H_{1}: \\alpha+\\beta\\neq1\\]\r\nDo you reject the null hypothesis at the 5% level? NoYes\r\nWhat is the p-value of your test (rounded to 3 decimal places)? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlibrary(car)\r\nlinearHypothesis(mod1,\"log(emp)+log(cap)=1\" )\r\n\r\nLinear hypothesis test\r\n\r\nHypothesis:\r\nlog(emp)  + log(cap) = 1\r\n\r\nModel 1: restricted model\r\nModel 2: log(vadd) ~ log(emp) + log(cap)\r\n\r\n  Res.Df    RSS Df Sum of Sq     F  Pr(>F)  \r\n1    457 34.892                             \r\n2    456 34.580  1   0.31205 4.115 0.04309 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\nPart (c)\r\nAn alternative way to test the hypothesis of constant returns to scale is to impose this restriction on the parameters and transform your regression model. Derive the necessary transformation, and show how the constant returns hypothesis amounts to a t-test in this transformed model. Carry out this test.\r\nDoes your result match what you found in (b)? NoYes\r\n\r\n\r\nAnswer:\r\n\r\nIf H0 is true we have that \\(\\beta=1-\\alpha\\). This means we can re-write the original equation as \\[\\ln\\frac{Y}{L}=\\alpha \\ln\\frac{K}{L}+\\varepsilon\\]\r\nHence we can regress \\(\\ln\\frac{Y}{L}\\) on \\(\\ln\\frac{K}{L})\\) and \\(\\ln L\\) and do a t-test on the hypothesis that the coefficient on \\(\\ln L\\) is equal to 0:\r\n\r\n\r\nmod2 <- lm(log(vadd/emp)~log(cap/emp)+log(emp),data[data$year==58,])\r\nsummary(mod2)\r\n\r\nCall:\r\nlm(formula = log(vadd/emp) ~ log(cap/emp) + log(emp), data = data[data$year == \r\n    58, ])\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.72254 -0.16410 -0.02135  0.16161  1.17701 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   1.41898    0.06172  22.991   <2e-16 ***\r\nlog(cap/emp)  0.27609    0.01430  19.304   <2e-16 ***\r\nlog(emp)     -0.02643    0.01303  -2.029   0.0431 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.2754 on 456 degrees of freedom\r\nMultiple R-squared:  0.4571,    Adjusted R-squared:  0.4547 \r\nF-statistic: 191.9 on 2 and 456 DF,  p-value: < 2.2e-16\r\n\r\nNote that we get the same p-value as before because it is essentially the same test.\r\n\r\nPart (d)\r\nWhat is the average size in numbers employed across all industries in 1958? \r\nSuppose an industry of average size employs an additional 1000 workers. What does the model estimated in part (a) imply about the effect this will have on value added (in percent, to 1 decimal place)? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  memp58 <- mean(data$emp[data$year==58])\r\n  memp58\r\n\r\n[1] 34.26035\r\n\r\nAverage industry employs 34 thousand workers (note that employment is measured in thousands).\r\nAdding 1000 workers to the average industry implies and increase of about 3%. Given that our estimate of β which we can interpret as an elasticity we would expect that value added increases by 3%×0.7=2.1%.\r\n\r\nPart (e)\r\nDo you think that our estimates in (a) could be biased? NoYes If so, why would that be?\r\n\r\n\r\nAnswer:\r\n\r\nThere is concern of endogeneity with regression such as in (a). While clearly more input factors has a causal effect on outputs it is also plausible that industries which have for whatever reason a positive shock to their output might attract more workers and capital. What’s more: with more than one explanatory variable it’s no longer that easy to predict the sign of the bias because it not only depends on the relationship between the error and the dependent variable but also on the relationship between the various explanatory variables as well as the relative strength of endogeneity for different variables. For instance in the case of production function regression the suspicion is that the labour coefficient is upward biased whereas the capital coefficient is downward biased. Not because one is positively correlated with the shock and the other is negatively correlated but because labour – being a more flexible production factor – is likely to be more positively correlated with the shock than capital.\r\n\r\nExercise 5.7\r\nUse the dataset attend.dta to analyze whether attending lectures has a causal effect on final exam performance. The dataset contains 674 observations on college students who took a particular course. Most variables on the dataset should be self-explanatory. The ACT is a college entry test. GPA is grade point average, the average performance in all courses.\r\n\r\n\r\n  library(foreign)\r\n  library(lmtest)\r\n  library(car)\r\n  data <- read.dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AABGEGjs2k7bZZQxU0H9SDtga/attend.dta?dl=1\")\r\n  summary(data)\r\n\r\n     attend         termgpa          priGPA           ACT       \r\n Min.   : 2.00   Min.   :0.000   Min.   :0.857   Min.   :13.00  \r\n 1st Qu.:24.00   1st Qu.:2.150   1st Qu.:2.200   1st Qu.:20.00  \r\n Median :28.00   Median :2.680   Median :2.560   Median :22.00  \r\n Mean   :26.28   Mean   :2.614   Mean   :2.592   Mean   :22.48  \r\n 3rd Qu.:30.00   3rd Qu.:3.120   3rd Qu.:2.950   3rd Qu.:25.00  \r\n Max.   :32.00   Max.   :4.000   Max.   :3.930   Max.   :32.00  \r\n     final          atndrte           hwrte            frosh     \r\n Min.   :10.00   Min.   :  6.25   Min.   : 12.50   Min.   :0.00  \r\n 1st Qu.:22.00   1st Qu.: 75.00   1st Qu.: 87.50   1st Qu.:0.00  \r\n Median :26.00   Median : 87.50   Median :100.00   Median :0.00  \r\n Mean   :25.89   Mean   : 82.12   Mean   : 87.91   Mean   :0.23  \r\n 3rd Qu.:29.00   3rd Qu.: 93.75   3rd Qu.:100.00   3rd Qu.:0.00  \r\n Max.   :39.00   Max.   :100.00   Max.   :100.00   Max.   :1.00  \r\n      soph           skipped          stndfnl        \r\n Min.   :0.0000   Min.   : 0.000   Min.   :-3.30882  \r\n 1st Qu.:0.0000   1st Qu.: 2.000   1st Qu.:-0.78782  \r\n Median :1.0000   Median : 4.000   Median : 0.05252  \r\n Mean   :0.5801   Mean   : 5.721   Mean   : 0.02914  \r\n 3rd Qu.:1.0000   3rd Qu.: 8.000   3rd Qu.: 0.68277  \r\n Max.   :1.0000   Max.   :30.000   Max.   : 2.78361  \r\n\r\nPart (a)\r\nRun a regression of stndfnl, the standardized final exam score, on attend, the number of lectures attended (note: the data is from the US where they call a lecture a class).\r\nHow much will attending one extra lecture add to the standardised final score (4 decimal places)? \r\nIs the effect large or small? Quite smallQuite large\r\n\r\n\r\nAnswer:\r\n\r\nAttending one extra lecture add 0.0278 to the standardized final score. In total there were 32 classes. Based on this, attending only half of those classes would imply a reduction in final outcome of 16×0.0278=0.44. This seems fairly substantial. While it is not enough to make a difference between a weak student (e.g. somebody in the bottom quartile; stndfnl=-0.78780) and a strong student (say somebody in the top quartile, stndfnl=0.68280) it has the potential to move a median student (stndfnl=0.0525) into top quartile territory (see summary stats for stndfnl below)\r\n\r\n\r\n  mod1 <- lm(stndfnl~attend,data)\r\n  summary(mod1)\r\n\r\nCall:\r\nlm(formula = stndfnl ~ attend, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.4415 -0.6700 -0.0245  0.6614  2.6787 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -0.702364   0.192243  -3.654 0.000279 ***\r\nattend       0.027836   0.007172   3.881 0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9824 on 672 degrees of freedom\r\nMultiple R-squared:  0.02192,   Adjusted R-squared:  0.02047 \r\nF-statistic: 15.06 on 1 and 672 DF,  p-value: 0.0001143\r\n\r\n  summary(data$stndfnl)\r\n\r\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \r\n-3.30882 -0.78782  0.05252  0.02914  0.68277  2.78361 \r\n\r\n  mod1$coefficients[[2]]*sd(data$attend)/sd(data$stndfnl)\r\n\r\n[1] 0.1480643\r\n\r\n\r\nPart (b)\r\nDo you think that our estimates in (a) could be biased? NoYes\r\nWhat would be the ideal way of addressing the problem? Do a randomised controlled trialInclude variables to control for ability/diligenceUse an instrumental variable\r\n\r\n\r\nAnswer:\r\n\r\nThe regression in (a) could be biased for a number of reasons. The main worry would be that students who are better irrespective of attendance are also more diligent in general and therefore more likely to attend class.\r\nThe best way of dealing with endogeneity in this context would be to use an instrumental variable. In absence of a good instrument (e.g. a nice instrument would be if some students where kept from attending class because of some weather or transport problem) we have to rely on the inclusion of further controls to deal which hopefully control for ability/diligence etc. of students.\r\n\r\nPart (c)\r\nEnter each of the following variables one at a time as a control in your regression: termgpa, priGPA, ACT. For each of these controls, answer the following questions:\r\nDoes entering the control variable termgpa help solve the problem you discussed in part (b) and gets you closer to a causal effect of lecture attendance? YesNo\r\nDoes entering the control variable priGPA create potential new problems in interpreting the coefficient on attend causally? NoYes Why?\r\nWhat happens to the coefficient on attend when you add the ACT variable? It becomes negative and not significantnegative and significantpositive and not significantpositive and significant. Interpret this result.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nmod2 <- lm(stndfnl~attend+termgpa,data)\r\nsummary(mod2)\r\n\r\nCall:\r\nlm(formula = stndfnl ~ attend + termgpa, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.3122 -0.5612 -0.0228  0.5745  2.3635 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -1.273935   0.166898  -7.633 7.90e-14 ***\r\nattend      -0.035146   0.007222  -4.866 1.42e-06 ***\r\ntermgpa      0.851879   0.052597  16.196  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.8336 on 671 degrees of freedom\r\nMultiple R-squared:  0.2968,    Adjusted R-squared:  0.2947 \r\nF-statistic: 141.6 on 2 and 671 DF,  p-value: < 2.2e-16\r\n\r\nThe term grade point average is strongly positive and significant and leads to a negative attendance effect. Note that the termgpa is most likely affected by higher attendance; However, by including it as a separate explanatory variable the resulting estimate ignore this effect. Instead what we are picking up is the final grade of people that attended class a lot but it did not make a difference to their term GPA. In other words these are probably very weak students who desperately tried to improve by attending classes a lot but it did not have a positive effect on either their term GPA or their final score.\r\n\r\n\r\nmod3 <- lm(stndfnl~attend+priGPA,data)\r\nsummary(mod3)\r\n\r\nCall:\r\nlm(formula = stndfnl ~ attend + priGPA, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.4949 -0.6228 -0.0443  0.6241  2.4043 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -1.698644   0.208907  -8.131 2.06e-15 ***\r\nattend      -0.001748   0.007425  -0.235    0.814    \r\npriGPA       0.684353   0.072085   9.494  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9231 on 671 degrees of freedom\r\nMultiple R-squared:  0.1377,    Adjusted R-squared:  0.1352 \r\nF-statistic:  53.6 on 2 and 671 DF,  p-value: < 2.2e-16\r\n\r\npriGPA is the GPA before the term. Again, we find a strong positive relation with stndfnl which is not surprising as ability to get good (or bad) results is very persistent. Again, the attendance coefficient becomes negative although this is not significant. Unlike for termgpa we cannot argue that attendance causes priGPA as well: attendance cannot affect past grades. So perhaps this means that there is really no causal effect of attendance on final outcomes.\r\n\r\n\r\nmod4 <- lm(stndfnl~attend+ACT,data)\r\nsummary(mod4)\r\n\r\nCall:\r\nlm(formula = stndfnl ~ attend + ACT, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.3142 -0.5702 -0.0045  0.6175  2.4836 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -3.448301   0.307065 -11.230  < 2e-16 ***\r\nattend       0.037599   0.006671   5.636 2.56e-08 ***\r\nACT          0.110749   0.010114  10.950  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9056 on 671 degrees of freedom\r\nMultiple R-squared:  0.1702,    Adjusted R-squared:  0.1677 \r\nF-statistic: 68.81 on 2 and 671 DF,  p-value: < 2.2e-16\r\n\r\nThe college entrance results captured by ACT are another way of controlling for ability. However, including it, far from destroying the effect of attendance makes is slightly stronger. A mechanism that could explain this is as follows: far from being more diligent, the most able students might have actually somewhat of a cavalier attitude. They know they are good and they know they don’t attend class to be good. Thus if this was the only effect we expect a downward bias in the simple regression of final scores on attendance. It is plausible that ACT captures this as it is the college entrance score.\r\n\r\nPart (d)\r\nDrawing on your discussion in (c), which of the control variables termgpa, priGPA, ACT would you like to have in your regression in order to uncover the causal effect of lecture attendance? termgpapriGPAACTtermgpa and priGPAtermgpa and ACTAll threeACT and priGPA\r\nWhy? Run your preferred specification and think of why it’s best.\r\n\r\n\r\nAnswer:\r\n\r\nIn reality we will have both effects potentially causing endogeneity (diligent students attending more class and cavalier geniuses not attending class) Our best bet to control for it is by including both ACT and priGPA. However, we should not include termGPA because of the potential reverse causality. This leads to a lower effect of attendance (0.017) but it is still significant.\r\n\r\n\r\n  mod5 <- lm(stndfnl~attend+priGPA+ACT,data)\r\n  summary(mod5)\r\n\r\nCall:\r\nlm(formula = stndfnl ~ attend + priGPA + ACT, data = data)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.2386 -0.5518 -0.0396  0.5927  2.3329 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -3.359279   0.301720 -11.134  < 2e-16 ***\r\nattend       0.017415   0.007603   2.291   0.0223 *  \r\npriGPA       0.410436   0.078675   5.217 2.43e-07 ***\r\nACT          0.083060   0.011253   7.381 4.66e-13 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.8884 on 670 degrees of freedom\r\nMultiple R-squared:  0.2026,    Adjusted R-squared:  0.199 \r\nF-statistic: 56.74 on 3 and 670 DF,  p-value: < 2.2e-16\r\n\r\n\r\nPart (e)\r\nStudents who are diligent in attending lectures may also be more diligent about other aspects of their coursework, like completing homework. There is a variable hwrte in the dataset indicating the percentage of homework turned in. Add this variable as a regressor to your preferred specification from (d).\r\nIs hwrte significant at the 5% level? YesNo\r\nIs it a regressor you want in order to uncover the causal effect of lecture attendance on student performance? YesNo\r\nWhat happens to the coefficient on attend? It increases and becomes significantIt reduces and becomes significantIt increases and becomes not significantIt reduces and becomes not significant\r\nInterpret your results.\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod6 <- lm(stndfnl~attend+priGPA+ACT+hwrte,data)\r\n  summary(mod6)\r\n\r\nCall:\r\nlm(formula = stndfnl ~ attend + priGPA + ACT + hwrte, data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.98362 -0.56827 -0.03468  0.60324  2.32557 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -3.471448   0.308536 -11.251  < 2e-16 ***\r\nattend       0.009107   0.009046   1.007   0.3144    \r\npriGPA       0.400493   0.078786   5.083 4.82e-07 ***\r\nACT          0.083834   0.011246   7.454 2.81e-13 ***\r\nhwrte        0.003855   0.002282   1.689   0.0917 .  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.8872 on 669 degrees of freedom\r\nMultiple R-squared:  0.206, Adjusted R-squared:  0.2012 \r\nF-statistic: 43.39 on 4 and 669 DF,  p-value: < 2.2e-16\r\n\r\nIncluding hwrte reduces the coefficient on attend and makes it not significant. However, the effect of hwrte is equally not very significant. This is likely because the two variables are very co-linear: students who attend class a lot are also more likely to do their homework diligently and so it is difficult to separately identify the effect of the two.\r\nIt is easy to imagine a situation where knowing them separately would be useful: Suppose a teacher is trying to figure out if it would be more useful to give another lecture or instead set another homework. Equally, it might not important or even outright wrong to try to distinguish the two; e.g. suppose a teacher wants to work out the effect of introducing sanctions (i.e. a negative participation grade) for not attending lectures. Part of the mechanism from more lecture to better grades might a positive effect on homework as well (e.g. students might be less forgetful about homework or they might be worried about looking bad in class). This causal effect of attendance would be ignored in a joint regression.\r\n\r\nPart (f)\r\nThere is a variable skipped in the dataset indicating the number of skipped lectures. Add this variable as a regressor to your preferred specification from (d).\r\nWhat happens and why? It becomes negative and not significantnegative and significantpositive and not significantpositive and significantnone of the above because it is perfectly collinear with attend. Interpret this result.\r\n\r\n\r\nAnswer:\r\n\r\n“skipped” is perfectly collinear with “attend”\r\n\r\n\r\n  mod7 <- lm(stndfnl~attend+priGPA+ACT+hwrte+skipped,data)\r\n  summary(mod7)\r\n\r\nCall:\r\nlm(formula = stndfnl ~ attend + priGPA + ACT + hwrte + skipped, \r\n    data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.98362 -0.56827 -0.03468  0.60324  2.32557 \r\n\r\nCoefficients: (1 not defined because of singularities)\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -3.471448   0.308536 -11.251  < 2e-16 ***\r\nattend       0.009107   0.009046   1.007   0.3144    \r\npriGPA       0.400493   0.078786   5.083 4.82e-07 ***\r\nACT          0.083834   0.011246   7.454 2.81e-13 ***\r\nhwrte        0.003855   0.002282   1.689   0.0917 .  \r\nskipped            NA         NA      NA       NA    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.8872 on 669 degrees of freedom\r\nMultiple R-squared:  0.206, Adjusted R-squared:  0.2012 \r\nF-statistic: 43.39 on 4 and 669 DF,  p-value: < 2.2e-16\r\n\r\n\r\nExercise 5.8\r\nDownload the data TeachingRatings.dta. This data set contains data on the teaching evaluations of 463 professors at the University of Texas, and various attributes of the professor and course.\r\n\r\n\r\n  library(foreign)\r\n  library(lmtest)\r\n  data <- read.dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AACPj9dkKUrGIGji8Ar8nEPla/TeachingRatings.dta?dl=1\")\r\n  summary(data)\r\n\r\n    minority           age            female         onecredit      \r\n Min.   :0.0000   Min.   :29.00   Min.   :0.0000   Min.   :0.00000  \r\n 1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:0.0000   1st Qu.:0.00000  \r\n Median :0.0000   Median :48.00   Median :0.0000   Median :0.00000  \r\n Mean   :0.1382   Mean   :48.37   Mean   :0.4212   Mean   :0.05832  \r\n 3rd Qu.:0.0000   3rd Qu.:57.00   3rd Qu.:1.0000   3rd Qu.:0.00000  \r\n Max.   :1.0000   Max.   :73.00   Max.   :1.0000   Max.   :1.00000  \r\n     beauty          course_eval        intro       \r\n Min.   :-1.45049   Min.   :2.100   Min.   :0.0000  \r\n 1st Qu.:-0.65627   1st Qu.:3.600   1st Qu.:0.0000  \r\n Median :-0.06801   Median :4.000   Median :0.0000  \r\n Mean   : 0.00000   Mean   :3.998   Mean   :0.3391  \r\n 3rd Qu.: 0.54560   3rd Qu.:4.400   3rd Qu.:1.0000  \r\n Max.   : 1.97002   Max.   :5.000   Max.   :1.0000  \r\n   nnenglish      \r\n Min.   :0.00000  \r\n 1st Qu.:0.00000  \r\n Median :0.00000  \r\n Mean   :0.06048  \r\n 3rd Qu.:0.00000  \r\n Max.   :1.00000  \r\n\r\nPart (a)\r\nRun a regression of course_eval on beauty. Beauty is an index that was based on a subjective scoring.\r\nWhat is the slope coefficient of the regression? \r\n\r\n\r\nAnswer:\r\n\r\nWe get a slope coefficient of 0.133.\r\n\r\n\r\n  mod1 <- lm(course_eval~beauty,data)\r\n  summary(mod1)\r\n\r\nCall:\r\nlm(formula = course_eval ~ beauty, data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.80015 -0.36304  0.07254  0.40207  1.10373 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.99827    0.02535 157.727  < 2e-16 ***\r\nbeauty       0.13300    0.03218   4.133 4.25e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.5455 on 461 degrees of freedom\r\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \r\nF-statistic: 17.08 on 1 and 461 DF,  p-value: 4.247e-05\r\n\r\n\r\nPart (b)\r\nThe number in (a) will not have much meaning to anyone who doesn’t know anything about the data.\r\nIs the effect of beauty on course_eval big or small? How would you go about assessing this?\r\n\r\n\r\nAnswer:\r\n\r\nTo understand if something is big or small we need to have some sort of benchmark to compare it to. If you lack other obvious benchmarks, we can always what happends in response to a reasonable change in the explanatory variable to a reasonable change in the dependent variable. For instance we could see what happens if we change the beauty variable by 1 standard deviation (sd(data$beauty= 0.789)) (of the beauty variable). The coefficient above suggest that this would lead to a change of 0.133 \\(\\times\\) 0.789 = 0.105.\r\nIs this big? Well depends in part on how much the dependent variable usually changes; e.g. we can look at the change of standard deviation of the dependent variable as well, which is 0.555. Hence, the beauty effect would amount of 18.904% of the evaluation standard deviation. Seems quite a lot for something that seems to not exactly relevant to learning. But of course that depends a bit on the eye of the beholder. Of course we could understand very well if some of you only came to university to enjoy the physical beauty of your lecturers.\r\nAs an alternative we can look at the change in the inter-quartile (i.e. the difference between the 75th and 25th percentile) range in beauty and see how that compares to the inter-quartile range in course scores:\r\n\r\n\r\n  a=mod1$coefficients[[2]]*(quantile(data$beauty,0.75)[[1]]-quantile(data$beauty,0.25)[[1]])\r\n  b=(quantile(data$course_eval,0.75)[[1]]-quantile(data$course_eval,0.25)[[1]])\r\n  \r\n  a/b\r\n\r\n[1] 0.1998132\r\n\r\ni.e. gives us a very similar answer.\r\n\r\nPart (c)\r\nIs the slope coefficient in (a) statistically significantly different from zero? NoYes\r\n\r\n\r\nAnswer:\r\n\r\nWe get a significant slope coefficient.\r\n\r\n\r\n  mod1 <- lm(course_eval~beauty,data)\r\n  summary(mod1)\r\n\r\nCall:\r\nlm(formula = course_eval ~ beauty, data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.80015 -0.36304  0.07254  0.40207  1.10373 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.99827    0.02535 157.727  < 2e-16 ***\r\nbeauty       0.13300    0.03218   4.133 4.25e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.5455 on 461 degrees of freedom\r\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \r\nF-statistic: 17.08 on 1 and 461 DF,  p-value: 4.247e-05\r\n\r\n\r\nPart (d)\r\nRun a regression of course_eval on beauty and female. Did the coefficient on beauty become bigger or smaller? SmallerBigger\r\nHow would you explain this?\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\n  mod2 <- lm(course_eval~beauty+female,data)\r\n  summary(mod2)\r\n\r\nCall:\r\nlm(formula = course_eval ~ beauty + female, data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.87197 -0.36913  0.03493  0.39919  1.03237 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  4.08158    0.03293  123.94  < 2e-16 ***\r\nbeauty       0.14859    0.03195    4.65 4.34e-06 ***\r\nfemale      -0.19781    0.05098   -3.88  0.00012 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.5373 on 460 degrees of freedom\r\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06224 \r\nF-statistic: 16.33 on 2 and 460 DF,  p-value: 1.407e-07\r\n\r\nThe beauty coefficient becomes a bit bigger (0.149). This is because it seems that women tend to get lower teaching but higher beauty scores.\r\n\r\nPart (e)\r\nWhat is the \\(R^{2}\\) of the regression in (d)? \r\nDoes it increase or decrease compared to the regression which does not include female? DecreaseIncrease\r\nThink why that could be.\r\n\r\n\r\nAnswer:\r\n\r\n\\(R^{2}\\) goes from 0.0357 to 0.0663. Adding more variables will always increase the \\(R^{2}\\) .\r\n\r\nPart (f)\r\nIs it possible that even in this case, the interpretation of the effect of beauty might not necessarily be causal? NoYes\r\nThink of why.\r\n\r\n\r\nAnswer:\r\n\r\nAs usual we have to consider confounding factors. It’s not immediately clear why other factors driving good teaching could have a reverse causality on teaching. But here is one story: beauty is of course a very subjective measure. We perceive people who are well dressed as more beautiful. Perhaps the same factors that make a person dress well are also useful when teaching (e.g. lecture slides are more tidy). Hence, when we just consider beauty we may in part pick up the effect of tidier lecture slides.\r\n`r unhide()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n\r\n\r\n/* update total correct if #total_correct exists */\r\nupdate_total_correct = function() {\r\n  if (t = document.getElementById(\"total_correct\")) {\r\n    t.innerHTML =\r\n      document.getElementsByClassName(\"correct\").length + \" of \" +\r\n      document.getElementsByClassName(\"solveme\").length + \" correct\";\r\n  }\r\n}\r\n\r\n/* solution button toggling function */\r\nb_func = function() {\r\n  var cl = this.parentElement.classList;\r\n  if (cl.contains('open')) {\r\n    cl.remove(\"open\");\r\n  } else {\r\n    cl.add(\"open\");\r\n  }\r\n}\r\n\r\n/* function for checking solveme answers */\r\nsolveme_func = function(e) {\r\n  var real_answers = JSON.parse(this.dataset.answer);\r\n  var my_answer = this.value;\r\n  var cl = this.classList;\r\n  if (cl.contains(\"ignorecase\")) {\r\n    my_answer = my_answer.toLowerCase();\r\n  }\r\n  if (cl.contains(\"nospaces\")) {\r\n    my_answer = my_answer.replace(/ /g, \"\");\r\n  }\r\n  \r\n  if (my_answer !== \"\" & real_answers.includes(my_answer)) {\r\n    cl.add(\"correct\");\r\n  } else {\r\n    cl.remove(\"correct\");\r\n  }\r\n  update_total_correct();\r\n}\r\n\r\nwindow.onload = function() {\r\n  /* set up solution buttons */\r\n  var buttons = document.getElementsByTagName(\"button\");\r\n\r\n  for (var i = 0; i < buttons.length; i++) {\r\n    if (buttons[i].parentElement.classList.contains('solution')) {\r\n      buttons[i].onclick = b_func;\r\n    }\r\n  }\r\n  \r\n  /* set up solveme inputs */\r\n  var solveme = document.getElementsByClassName(\"solveme\");\r\n\r\n  for (var i = 0; i < solveme.length; i++) {\r\n    /* make sure input boxes don't auto-anything */\r\n    solveme[i].setAttribute(\"autocomplete\",\"off\");\r\n    solveme[i].setAttribute(\"autocorrect\", \"off\");\r\n    solveme[i].setAttribute(\"autocapitalize\", \"off\"); \r\n    solveme[i].setAttribute(\"spellcheck\", \"false\");\r\n    solveme[i].value = \"\";\r\n    \r\n    /* adjust answer for ignorecase or nospaces */\r\n    var cl = solveme[i].classList;\r\n    var real_answer = solveme[i].dataset.answer;\r\n    if (cl.contains(\"ignorecase\")) {\r\n      real_answer = real_answer.toLowerCase();\r\n    }\r\n    if (cl.contains(\"nospaces\")) {\r\n      real_answer = real_answer.replace(/ /g, \"\");\r\n    }\r\n    solveme[i].dataset.answer = real_answer;\r\n    \r\n    /* attach checking function */\r\n    solveme[i].onkeyup = solveme_func;\r\n    solveme[i].onchange = solveme_func;\r\n  }\r\n  \r\n  update_total_correct();\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/exercises/exercises5/p03dq1gg.jpg",
    "last_modified": "2020-11-23T18:33:22+00:00",
    "input_file": {}
  },
  {
    "path": "posts/exercises/exercises6/",
    "title": "Exercises 6",
    "description": "Econometrics for dummies",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2020-11-23",
    "categories": [
      "Exercises"
    ],
    "contents": "\r\n\r\n\r\n\r\nExercise 6.1\r\nConsider the dataset Brexit.dta. It contains data on the outcome of the Brexit vote from June 23 this year by local area along with a range of area characteristics. The variable pct_leave records the percentage of voters in an area that voted for leave.\r\n\r\n\r\nlibrary(haven)\r\ndf=read_dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AAC_4UZXJG9kmImypJXTZ9IOa/brexit.dta?dl=1\")\r\nnames(df)\r\n\r\n  [1] \"oslaua\"               \"region_code\"         \r\n  [3] \"region\"               \"area\"                \r\n  [5] \"pct_turnout\"          \"pct_leave\"           \r\n  [7] \"pct_rejected\"         \"electorate\"          \r\n  [9] \"expectedballots\"      \"verifiedballotpapers\"\r\n [11] \"votes_cast\"           \"valid_votes\"         \r\n [13] \"remain\"               \"leave\"               \r\n [15] \"rejected_ballots\"     \"no_official_mark\"    \r\n [17] \"writing_or_mark\"      \"unmarked_or_void\"    \r\n [19] \"pop91\"                \"pop11\"               \r\n [21] \"sh_young\"             \"m_migr\"              \r\n [23] \"b_migr\"               \"b_migr11\"            \r\n [25] \"etn11_W\"              \"etn11_AI\"            \r\n [27] \"etn11_AP\"             \"etn11_AB\"            \r\n [29] \"etn11_AC\"             \"etn11_AO\"            \r\n [31] \"etn11_BCA\"            \"etn11_BAF\"           \r\n [33] \"etn11_BO\"             \"etn11_O\"             \r\n [35] \"ni11_sco\"             \"ni11_bri\"            \r\n [37] \"ni11_eng\"             \"ni11_oth\"            \r\n [39] \"ni11_oe\"              \"shni11_sco\"          \r\n [41] \"shni11_bri\"           \"shni11_eng\"          \r\n [43] \"shni11_oth\"           \"shni11_oe\"           \r\n [45] \"citshare\"             \"urate2004\"           \r\n [47] \"urate2005\"            \"urate2006\"           \r\n [49] \"urate2007\"            \"urate2008\"           \r\n [51] \"urate2009\"            \"urate2010\"           \r\n [53] \"urate2011\"            \"urate2012\"           \r\n [55] \"urate2013\"            \"urate2014\"           \r\n [57] \"urate2015\"            \"epop2004\"            \r\n [59] \"epop2005\"             \"epop2006\"            \r\n [61] \"epop2007\"             \"epop2008\"            \r\n [63] \"epop2009\"             \"epop2010\"            \r\n [65] \"epop2011\"             \"epop2012\"            \r\n [67] \"epop2013\"             \"epop2014\"            \r\n [69] \"epop2015\"             \"zage18_24\"           \r\n [71] \"zage45_59\"            \"zage25_29\"           \r\n [73] \"zage60\"               \"dlmig\"               \r\n [75] \"zshi61_ACD\"           \"zshi61_F\"            \r\n [77] \"zshi61_GHI\"           \"zshi61_JKO\"          \r\n [79] \"zshi61_LMN\"           \"zshi71_ACD\"          \r\n [81] \"zshi71_F\"             \"zshi71_GHI\"          \r\n [83] \"zshi71_JKO\"           \"zshi71_LMN\"          \r\n [85] \"zshi81_ACD\"           \"zshi81_F\"            \r\n [87] \"zshi81_GHI\"           \"zshi81_JKO\"          \r\n [89] \"zshi81_LMN\"           \"zshi91_ACD\"          \r\n [91] \"zshi91_F\"             \"zshi91_GHI\"          \r\n [93] \"zshi91_JKO\"           \"zshi91_LMN\"          \r\n [95] \"zshi01_ACD\"           \"zshi01_F\"            \r\n [97] \"zshi01_GHI\"           \"zshi01_JKO\"          \r\n [99] \"zshi01_LMN\"           \"zshi11_ACD\"          \r\n[101] \"zshi11_F\"             \"zshi11_GHI\"          \r\n[103] \"zshi11_JKO\"           \"zshi11_LMN\"          \r\n[105] \"dzshi_ACD\"            \"dzshi_F\"             \r\n[107] \"dzshi_GHI\"            \"dzshi_JKO\"           \r\n[109] \"dzshi_LMN\"            \"zshedu11_noqual\"     \r\n[111] \"zshedu11_l1\"          \"zshedu11_l2\"         \r\n[113] \"zshedu11_l3\"          \"zshedu11_l4\"         \r\n[115] \"wrkage\"               \"age16over\"           \r\n[117] \"zsh11_wrk\"           \r\n\r\nPart (a)\r\nConsider the variable b_migr11. It records the share (in %) of foreign born residents in an area (according to the last census, which was in 2011). There is no shortage of politicians claiming that the vote for Brexit was due to immigration particularly after 2005 when Eastern European countries joined the EU and their residents could freely move to countries like Britain. Hence, we would expect there to be a strong effect from the presence of foreigners in an area to the vote outcome. Explore this using the pct_leave and b_migr11 variables using graphical and regression analysis.\r\nWhich way is the line of best fit sloping on your scatter plot? UpwardsDownwards\r\nWhat is the constant (rounded to 3 decimal places)? \r\nWhat is the slope coefficient (rounded to 3 decimal places)? \r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nlibrary(ggplot2)\r\nggplot(df, aes(x=b_migr11, y=pct_leave)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE)\r\n\r\n\r\nLooking at the scatter plot it seems there is rather a negative relationship between the proportion of foreigners in an area and the support for leave; i.e. exactly the opposite of what one would expect. Regression analysis confirms this:\r\n\r\n\r\nsummary(lm(pct_leave~b_migr11, data=df))\r\n\r\nCall:\r\nlm(formula = pct_leave ~ b_migr11, data = df)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-30.4487  -5.3237   0.6445   5.6518  24.9745 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 58.55342    0.68603   85.35   <2e-16 ***\r\nb_migr11    -0.50359    0.04668  -10.79   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 9.12 on 378 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.2354,    Adjusted R-squared:  0.2334 \r\nF-statistic: 116.4 on 1 and 378 DF,  p-value: < 2.2e-16\r\n\r\nWe see that there is a significant negative relationship. 1 percentage point more foreigners in an area leads to 0.5 percentage point loss in support for the vote to leave.\r\n\r\nPart (b)\r\nVarious commentators have suggested that it might not be so much the level of immigrants as such, but the experience of a change due to more foreigners in an area that was driving the vote. The variable b_migr contains the share of immigrants in 1991.\r\nConstruct a new variable recording the change in the share of immigrants between 2011 and 1992. Explore its impact by extending the regression model from part a).\r\nWhat is the coefficient on this new variable (rounded to 3 decimal places)? \r\nIs it statistically significant? NoYes\r\n\r\n\r\nAnswer:\r\n\r\nWe can construct the change in migration shares as:\r\n\r\n\r\ndf[\"Db_migr\"] <- df$b_migr11-df$b_migr\r\n\r\nWe can then regress\r\n\r\n\r\nsummary(lm(pct_leave~Db_migr+b_migr11, data=df))\r\n\r\nCall:\r\nlm(formula = pct_leave ~ Db_migr + b_migr11, data = df)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-30.2777  -4.4700   0.6883   5.8477  28.5925 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  58.6970     0.6703  87.566  < 2e-16 ***\r\nDb_migr       0.9259     0.2078   4.454 1.11e-05 ***\r\nb_migr11     -1.0987     0.1412  -7.783 6.86e-14 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 8.901 on 377 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.2736,    Adjusted R-squared:  0.2698 \r\nF-statistic:    71 on 2 and 377 DF,  p-value: < 2.2e-16\r\n\r\nIt seems that indeed the change in the migration share has a strong positive and significant impact on the leave share. Notice also that the share of migrants in 2011 now becomes larger as well. This is because the areas with higher increase in foreigners are also the areas with a higher share of foreigners (e.g. like London). Consequently, in the previous univariate regression the b_migr11 coefficient suffered from an upward bias.\r\n\\[LeaveShare = \\beta_{0}+\\beta_{1}MShare_{2011}+\\beta_{2}(MShare_{2011}-MShare_{1991})+\\epsilon\\]\r\n\r\nPart (c)\r\nWork out the change in an area’s leave percentage if the 2011 migrant share would move to back to its level in 1991 in every area.\r\nAccording to your model from part (b), what would have happened to the vote if there would not have been any change in the share of migrants between 1991 and 2011? Support for Brexit would DecreaseIncrease\r\nIn how many areas would the vote flip from a majority support for Brexit to a majority support for Remain? \r\n\r\n\r\nAnswer:\r\n\r\nNote that the impact of changing the 2011 migrant share is a combination of the factors found in the previous section i.e. reducing the migrant share by one percentage point leads to a change in the leave share of \\(-(\\beta_{1}+\\beta_{2})=-(-1.099+0.926)=0.173\\) percentage points. In other words, it would seem that a reversal in migrant presence would tend to lead to an increase in the support for Brexit, rather than an increase in the support for remain. Consequently, in no area would be find a flip in the vote from majority support for Brexit to remain.\r\n\r\nPart (d)\r\nCan you think of any reason why the estimates in (b) might not adequately reflect the causal impact of immigration on the vote? What are plausible confounding forces? Aging population (more boomers)Economic opportunity\r\n\r\n\r\nAnswer:\r\n\r\nOne reason could be as follows: an important factor that drives immigration is economic opportunity. Hence, it is quite likely that immigration is higher in areas of the country where economic growth was higher. This could mean that there is a positive correlation between the errors and immigration which could imply a downward bias in the estimate of the coefficient on immigration. This in turn could in principle be an explanation for why we find a negative coefficient for the immigration variable (i.e. immigration in 2011 has actually a positive effect on support for leave but we fail to detect it because it is conflated by the more substantial negative effect of economic conditions on the leave vote).\r\n\r\nPart (e)\r\nThe dataset contains a large number of additional characteristics about a local area. Which variable would you add to your model from part b) to test the alternative explanation mentioned in d)? Number of Scottish peopleShare of people living in citiesUnemployment\r\n\r\n\r\nAnswer:\r\n\r\nWe can explore the point made in answer d) by using unemployment as an additional control variable. Below we include both the level of the unemployment rate in 2004 and the change in in the rate between 2011 and 2004.\r\n\r\n\r\ndf[\"Durate\"] <- df$urate2011-df$urate2004\r\nsummary(lm(pct_leave~Db_migr + b_migr11 + Durate + urate2004, data=df))\r\n\r\nCall:\r\nlm(formula = pct_leave ~ Db_migr + b_migr11 + Durate + urate2004, \r\n    data = df)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-27.9548  -4.3850   0.6518   5.1247  19.8195 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  55.7178     1.3368  41.681  < 2e-16 ***\r\nDb_migr       0.9224     0.2165   4.260 2.60e-05 ***\r\nb_migr11     -1.0015     0.1453  -6.894 2.42e-11 ***\r\nDurate        1.0426     0.1781   5.854 1.08e-08 ***\r\nurate2004    -0.3586     0.2610  -1.374     0.17    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 8.393 on 362 degrees of freedom\r\n  (14 observations deleted due to missingness)\r\nMultiple R-squared:  0.3543,    Adjusted R-squared:  0.3471 \r\nF-statistic: 49.65 on 4 and 362 DF,  p-value: < 2.2e-16\r\n\r\nNote that including the unemployment variables does not change the migration variables by much, which suggests that the economic conditions are not conflating the results on immigration. That said, note that the net negative effect of b_migr11 (coefficient for b_migr11+ coefficient for Db_migr) is slightly less negative than in in (b) which would be consistent with a slight conflation of the immigration effect by economic factors.\r\nAlso, note that the change in the unemployment rate has a high and significant coefficient. If the unemployment rate goes up by 1 percentage point the support for leave goes up by about 1 percentage point as well. Hence, it might be more useful to consider economic conditions as a factor that has been driving the vote rather than recent immigration.\r\n\r\nExercise 6.2\r\nThe dataset data/prod.dta contains production data for various companies from 1979 to 1986.\r\n\r\n\r\nlibrary(haven)\r\nprod=read_dta(\"https://www.dropbox.com/sh/rqmo1hvij1veff0/AACD9OHn_yCnKFAX7hbEASVha/prod.dta?dl=1\")\r\nnames(prod)\r\n\r\n[1] \"year\"      \"id\"        \"go\"        \"m\"         \"l\"        \r\n[6] \"k\"         \"sic3dig\"   \"countyear\" \"va\"       \r\n\r\nPart (a)\r\nExamine the data using a Cobb-Douglas production function in terms of value added; i.e. regress ln value added on ln capital and ln labour (va contains the value added, k the capital stock and l labour all not in logs). On the basis of the regression examine the hypothesis that the production function has constant returns to scale (i.e. the labour and capital coefficients would add ot 1).\r\nThe hypothesis is SupportedRejected\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nprod$year=factor(prod$year)\r\nmod1=lm(log(va)~log(k)+log(l)+year, prod)\r\nsummary(mod1)\r\n\r\nCall:\r\nlm(formula = log(va) ~ log(k) + log(l) + year, data = prod)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.7397 -0.4336  0.0305  0.4477  2.1339 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  2.985474   0.128579  23.219   <2e-16 ***\r\nlog(k)       0.346498   0.021273  16.288   <2e-16 ***\r\nlog(l)       0.938894   0.038330  24.495   <2e-16 ***\r\nyear80       0.036981   0.082998   0.446   0.6560    \r\nyear81       0.141971   0.083009   1.710   0.0875 .  \r\nyear82       0.092141   0.082992   1.110   0.2671    \r\nyear83       0.005795   0.082994   0.070   0.9443    \r\nyear84       0.035601   0.083003   0.429   0.6681    \r\nyear85      -0.061518   0.083138  -0.740   0.4595    \r\nyear86       0.108689   0.083340   1.304   0.1924    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.7091 on 1158 degrees of freedom\r\nMultiple R-squared:  0.7846,    Adjusted R-squared:  0.7829 \r\nF-statistic: 468.6 on 9 and 1158 DF,  p-value: < 2.2e-16\r\n\r\nlibrary(car)\r\nlibrary(plm)\r\nlinearHypothesis(mod1,\"log(k)+log(l)=1\",vcov=vcovHC)\r\n\r\nLinear hypothesis test\r\n\r\nHypothesis:\r\nlog(k)  + log(l) = 1\r\n\r\nModel 1: restricted model\r\nModel 2: log(va) ~ log(k) + log(l) + year\r\n\r\nNote: Coefficient covariance matrix supplied.\r\n\r\n  Res.Df Df      F    Pr(>F)    \r\n1   1159                        \r\n2   1158  1 136.16 < 2.2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nWe turn year into a categorical (factor) variable. Treating year as a categorical variable will calculate effect of each individual year - i.e. what impact on the target variable was on average in a given year.\r\nWe then go on to test the hypothesis that the production function has constant returns to scale, formulated as log(k)+log(l)=1. We want to tell R to use robust standard errors and therefore we need to include vcov=vcovHC. We introduce this to deal with heteroskedasticity.\r\nWe need to install and load two packages (package “car”, which includes the function linearHypothesis and package “plm”, which includes our robust covariance matrix estimators, including vcovHC).\r\nConstant returns are clearly rejected.\r\n\r\nPart (b)\r\nThe variable sic3dig contains an industry classifier which groups the firms into 17 industries.\r\nWhy might it be useful to include industry classifiers in order to estimate the production function better? It will increase the R^2 of the modelIt could reduce bias\r\nRe-estimate the production function controlling for industry. Does your assessment about constant returns to scale change based on this new estimate? YesNo\r\n\r\n\r\nAnswer:\r\n\r\nTypically the residual from a production function estimation is interpreted as productivity. However, it is plausible that more productive firms will want to employ more production factors. This might lead to a correlation between residuals and the explanatory variables which could lead to biases. A big part of that might come from variations between sectors; i.e. some sectors are just more productive and profitable and those will also be the sectors that attract more capital and other production factors.\r\n\r\n\r\nprod$sic3dig=factor(prod$sic3dig)\r\nmod2=lm(log(va)~log(k)+log(l)+year+sic3dig, prod)\r\nsummary(mod2)\r\n\r\nCall:\r\nlm(formula = log(va) ~ log(k) + log(l) + year + sic3dig, data = prod)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.88207 -0.40007  0.05372  0.44165  1.92283 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.216195   0.134458  23.920  < 2e-16 ***\r\nlog(k)       0.285436   0.022608  12.625  < 2e-16 ***\r\nlog(l)       0.995616   0.038889  25.602  < 2e-16 ***\r\nyear80       0.036104   0.077877   0.464 0.643019    \r\nyear81       0.142005   0.077888   1.823 0.068537 .  \r\nyear82       0.094111   0.077871   1.209 0.227085    \r\nyear83       0.007594   0.077874   0.098 0.922338    \r\nyear84       0.032116   0.077884   0.412 0.680161    \r\nyear85      -0.071111   0.078036  -0.911 0.362352    \r\nyear86       0.093891   0.078264   1.200 0.230517    \r\nsic3dig321  -0.107406   0.079196  -1.356 0.175304    \r\nsic3dig322   0.155755   0.079807   1.952 0.051224 .  \r\nsic3dig323   0.847057   0.239067   3.543 0.000411 ***\r\nsic3dig324   0.034193   0.124769   0.274 0.784093    \r\nsic3dig331  -0.175586   0.085783  -2.047 0.040900 *  \r\nsic3dig332   0.260137   0.085636   3.038 0.002438 ** \r\nsic3dig341   0.733683   0.176477   4.157 3.46e-05 ***\r\nsic3dig342   0.035795   0.095630   0.374 0.708245    \r\nsic3dig351   1.297901   0.175805   7.383 2.98e-13 ***\r\nsic3dig352   0.597134   0.102678   5.816 7.84e-09 ***\r\nsic3dig355   0.509182   0.237587   2.143 0.032313 *  \r\nsic3dig356   0.195583   0.095147   2.056 0.040050 *  \r\nsic3dig369  -0.239435   0.101992  -2.348 0.019065 *  \r\nsic3dig371   0.771417   0.139946   5.512 4.38e-08 ***\r\nsic3dig381   0.174591   0.071427   2.444 0.014663 *  \r\nsic3dig383   0.367663   0.140038   2.625 0.008769 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6653 on 1142 degrees of freedom\r\nMultiple R-squared:  0.813, Adjusted R-squared:  0.8089 \r\nF-statistic: 198.5 on 25 and 1142 DF,  p-value: < 2.2e-16\r\n\r\nlinearHypothesis(mod2, \"log(k)+log(l)=1\", vcov=vcovHC)\r\n\r\nLinear hypothesis test\r\n\r\nHypothesis:\r\nlog(k)  + log(l) = 1\r\n\r\nModel 1: restricted model\r\nModel 2: log(va) ~ log(k) + log(l) + year + sic3dig\r\n\r\nNote: Coefficient covariance matrix supplied.\r\n\r\n  Res.Df Df     F    Pr(>F)    \r\n1   1143                       \r\n2   1142  1 130.6 < 2.2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\ni.e. the test continues to be rejected. Consequently, we don’t find constant returns to scale.\r\n\r\nPart (c)\r\nWhich of the 17 industries has the largest number of observations? 381383311\r\nLets pick the first two industries that appear in our table, 311 and 321. For each of the two industries separately, estimate a Cobb-Douglas production function.\r\nWould you say the functions are very different in the two industries? NoYes\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\ntable(prod$sic3dig)\r\n\r\n311 321 322 323 324 331 332 341 342 351 352 355 356 369 371 381 383 \r\n400  88  88   8  32  72  72  16  56  16  48   8  56  48  24 112  24 \r\n\r\nThe table reveals that the industry with the largest number of observations is 311.\r\n\r\n\r\nlibrary(dplyr)\r\nmod311=lm(log(va)~log(k)+log(l)+year, prod %>% filter(sic3dig==\"311\")) \r\nsummary(mod311)\r\n\r\nCall:\r\nlm(formula = log(va) ~ log(k) + log(l) + year, data = prod %>% \r\n    filter(sic3dig == \"311\"))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.30248 -0.41505  0.07536  0.45801  1.73235 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  2.56445    0.20804  12.327  < 2e-16 ***\r\nlog(k)       0.41456    0.04113  10.079  < 2e-16 ***\r\nlog(l)       0.80244    0.07461  10.756  < 2e-16 ***\r\nyear80       0.08684    0.13017   0.667 0.505074    \r\nyear81       0.26948    0.13017   2.070 0.039090 *  \r\nyear82       0.39208    0.13033   3.008 0.002798 ** \r\nyear83       0.23738    0.13065   1.817 0.070003 .  \r\nyear84       0.22519    0.13086   1.721 0.086078 .  \r\nyear85       0.07980    0.13209   0.604 0.546113    \r\nyear86       0.48876    0.13271   3.683 0.000263 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6508 on 390 degrees of freedom\r\nMultiple R-squared:  0.8318,    Adjusted R-squared:  0.8279 \r\nF-statistic: 214.2 on 9 and 390 DF,  p-value: < 2.2e-16\r\n\r\nmod321=lm(log(va)~log(k)+log(l)+year, prod%>% filter(sic3dig==\"321\"))\r\nsummary(mod321)\r\n\r\nCall:\r\nlm(formula = log(va) ~ log(k) + log(l) + year, data = prod %>% \r\n    filter(sic3dig == \"321\"))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.12790 -0.32740  0.02603  0.38229  0.95998 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.61053    0.41939   8.609 6.19e-13 ***\r\nlog(k)       0.20105    0.07984   2.518   0.0138 *  \r\nlog(l)       1.04942    0.13980   7.507 8.45e-11 ***\r\nyear80       0.16420    0.22541   0.728   0.4685    \r\nyear81       0.17994    0.22527   0.799   0.4268    \r\nyear82       0.17906    0.22650   0.791   0.4316    \r\nyear83       0.22520    0.22623   0.995   0.3226    \r\nyear84      -0.03102    0.22546  -0.138   0.8909    \r\nyear85      -0.16535    0.22671  -0.729   0.4680    \r\nyear86       0.26596    0.22574   1.178   0.2423    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.5282 on 78 degrees of freedom\r\nMultiple R-squared:  0.8231,    Adjusted R-squared:  0.8027 \r\nF-statistic: 40.32 on 9 and 78 DF,  p-value: < 2.2e-16\r\n\r\nIn each case the labor coefficient is larger than the capital coefficient. However, the numbers are not necessarily very close. In the case of sector 321 the capital coefficient is only half of the coefficient for sector 311. Still, to make sure they are statistically different it’s good to do a formal test which is the next question.\r\n\r\nPart (d)\r\nConduct a hypothesis test to compare the two functions formally. Note, that for that you need to estimate both functions using a single regression model.\r\nAre the coefficients statistically different from zero? YesNo\r\nCould they be jointly significant? NoYes\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nmod_inter=lm(log(va)~log(k)+log(l)+sic3dig*log(k)+sic3dig*log(l)+sic3dig+year, prod %>% filter(sic3dig==\"311\"|sic3dig==\"321\"))\r\nsummary(mod_inter)\r\n\r\nCall:\r\nlm(formula = log(va) ~ log(k) + log(l) + sic3dig * log(k) + sic3dig * \r\n    log(l) + sic3dig + year, data = prod %>% filter(sic3dig == \r\n    \"311\" | sic3dig == \"321\"))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.28294 -0.41035  0.05918  0.41383  1.74454 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        2.60151    0.19647  13.242  < 2e-16 ***\r\nlog(k)             0.40941    0.03950  10.364  < 2e-16 ***\r\nlog(l)             0.81146    0.07167  11.322  < 2e-16 ***\r\nsic3dig321         0.86072    0.49179   1.750 0.080733 .  \r\nyear80             0.10020    0.11394   0.879 0.379609    \r\nyear81             0.25290    0.11392   2.220 0.026894 *  \r\nyear82             0.35202    0.11414   3.084 0.002161 ** \r\nyear83             0.23322    0.11434   2.040 0.041942 *  \r\nyear84             0.17799    0.11443   1.555 0.120516    \r\nyear85             0.03436    0.11543   0.298 0.766105    \r\nyear86             0.44649    0.11578   3.856 0.000131 ***\r\nlog(k):sic3dig321 -0.18972    0.09998  -1.898 0.058355 .  \r\nlog(l):sic3dig321  0.20729    0.17579   1.179 0.238910    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6291 on 475 degrees of freedom\r\nMultiple R-squared:  0.8316,    Adjusted R-squared:  0.8274 \r\nF-statistic: 195.5 on 12 and 475 DF,  p-value: < 2.2e-16\r\n\r\ncoeftest(mod_inter, vcov=vcovHC)\r\n\r\nt test of coefficients:\r\n\r\n                   Estimate Std. Error t value  Pr(>|t|)    \r\n(Intercept)        2.601515   0.210652 12.3498 < 2.2e-16 ***\r\nlog(k)             0.409406   0.048051  8.5202 < 2.2e-16 ***\r\nlog(l)             0.811456   0.087765  9.2458 < 2.2e-16 ***\r\nsic3dig321         0.860720   0.456747  1.8845 0.0601137 .  \r\nyear80             0.100202   0.108075  0.9272 0.3543179    \r\nyear81             0.252903   0.106088  2.3839 0.0175224 *  \r\nyear82             0.352021   0.102787  3.4248 0.0006687 ***\r\nyear83             0.233218   0.112821  2.0672 0.0392611 *  \r\nyear84             0.177992   0.114145  1.5594 0.1195786    \r\nyear85             0.034356   0.117276  0.2929 0.7696888    \r\nyear86             0.446495   0.109287  4.0855 5.161e-05 ***\r\nlog(k):sic3dig321 -0.189720   0.098068 -1.9346 0.0536363 .  \r\nlog(l):sic3dig321  0.207288   0.178478  1.1614 0.2460545    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nWe continue to work with a subset of our data. We introduce interaction terms into our model – multiplying our logged labor variable with the sector code and the logged capital variable with the sector code. The t-test on these interaction terms is what we are interested in. Remember interaction terms should be interpreted as “effect modifiers” - we are interested in whether the industry modifies the relationship between labour/capital and value added.\r\nAs before, we are concerned about heteroskedascity. We therefore perform a coeftest, specifying we want robust standard errors. It turns out the coefficients are not statistically different; i.e. the t-tests for the interaction coefficients are not significantly different from zero. However, it could still be the case that they are jointly significant (e.g. there could be a high degree of collinearity in the interaction variables.)\r\nWe can examine that with the linearHypothesis command:\r\n\r\n\r\nlinearHypothesis(mod_inter, c(\"log(k):sic3dig321=0\",\"log(l):sic3dig321=0\"), vcov=vcovHC)\r\n\r\nLinear hypothesis test\r\n\r\nHypothesis:\r\nlog(k):sic3dig321 = 0\r\nlog(l):sic3dig321 = 0\r\n\r\nModel 1: restricted model\r\nModel 2: log(va) ~ log(k) + log(l) + sic3dig * log(k) + sic3dig * log(l) + \r\n    sic3dig + year\r\n\r\nNote: Coefficient covariance matrix supplied.\r\n\r\n  Res.Df Df      F Pr(>F)  \r\n1    477                   \r\n2    475  2 2.4557 0.0869 .\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nThis suggests that the production function for sector 321 is weakly significantly different from that in sector 311.\r\n\r\nPart (e)\r\nRe-estimate your extended model from d) by allowing for fixed effects.\r\nDoes this change your assessment concerning the hypothesis that the production functions are identical in the two industries? NoYes\r\n\r\n\r\nAnswer:\r\n\r\n\r\n\r\nmod_fe=plm(log(va)~log(k)+log(l)+sic3dig*log(k)+sic3dig*log(l)+sic3dig+year, index=c(\"id\",\"year\"),data=prod %>% filter(sic3dig==\"311\"|sic3dig==\"321\"), model=\"within\")\r\nsummary(mod_fe)\r\n\r\nOneway (individual) effect Within Model\r\n\r\nCall:\r\nplm(formula = log(va) ~ log(k) + log(l) + sic3dig * log(k) + \r\n    sic3dig * log(l) + sic3dig + year, data = prod %>% filter(sic3dig == \r\n    \"311\" | sic3dig == \"321\"), model = \"within\", index = c(\"id\", \r\n    \"year\"))\r\n\r\nBalanced Panel: n = 61, T = 8, N = 488\r\n\r\nResiduals:\r\n      Min.    1st Qu.     Median    3rd Qu.       Max. \r\n-1.5714319 -0.2166290  0.0028078  0.2322813  1.3800158 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t-value  Pr(>|t|)    \r\nlog(k)            0.308997   0.134568  2.2962  0.022160 *  \r\nlog(l)            0.359801   0.118239  3.0430  0.002491 ** \r\nyear80            0.102850   0.079814  1.2886  0.198247    \r\nyear81            0.251459   0.079807  3.1508  0.001746 ** \r\nyear82            0.344801   0.080830  4.2657 2.469e-05 ***\r\nyear83            0.230999   0.082761  2.7911  0.005494 ** \r\nyear84            0.183991   0.085504  2.1518  0.031985 *  \r\nyear85            0.079124   0.090678  0.8726  0.383396    \r\nyear86            0.477635   0.094902  5.0329 7.204e-07 ***\r\nlog(k):sic3dig321 0.238587   0.236232  1.0100  0.313097    \r\nlog(l):sic3dig321 0.138285   0.204708  0.6755  0.499719    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nTotal Sum of Squares:    96.276\r\nResidual Sum of Squares: 80.641\r\nR-Squared:      0.1624\r\nAdj. R-Squared: 0.019445\r\nF-statistic: 7.33249 on 11 and 416 DF, p-value: 1.6864e-11\r\n\r\nlinearHypothesis(mod_fe, c(\"log(k):sic3dig321=0\",\"log(l):sic3dig321=0\"), vcov=vcovHC)\r\n\r\nLinear hypothesis test\r\n\r\nHypothesis:\r\nlog(k):sic3dig321 = 0\r\nlog(l):sic3dig321 = 0\r\n\r\nModel 1: restricted model\r\nModel 2: log(va) ~ log(k) + log(l) + sic3dig * log(k) + sic3dig * log(l) + \r\n    sic3dig + year\r\n\r\nNote: Coefficient covariance matrix supplied.\r\n\r\n  Res.Df Df  Chisq Pr(>Chisq)\r\n1    418                     \r\n2    416  2 1.5389     0.4633\r\n\r\nWe continue to work with the same subset of our data but introduce a different model here. Instead of OLS regression, we use a fixed effects panel regression. This allows us to control for differences between firms that we cannot observe by including firm-specific dummies (that is why it is sometimes called Least Squares Dummy Variable Model).\r\nAs for lm() we have to specify the dependent and independent variables and the data to be used in our call of plm(). We also need to include a vector of names of entity and time ID variables to the argument index. Since the fixed effects estimator is also called the within estimator, we set model = “within”.\r\nIt seems that once we allow for fixed effects the interaction coefficients are no longer significant even in a joint significance test.\r\n\r\n\r\n\r\n",
    "preview": "posts/exercises/exercises6/5e91e263d083e200047f064e.png",
    "last_modified": "2020-11-23T12:57:16+00:00",
    "input_file": {},
    "preview_width": 450,
    "preview_height": 481
  },
  {
    "path": "posts/quickguides/quickguide_ttests/",
    "title": "Quick Guide - Hypothesis testing",
    "description": "What is significant?",
    "author": [
      {
        "name": "Ralf Martin",
        "url": "https://mondpanther.github.io/wwwmondpanther/"
      }
    ],
    "date": "2020-11-16",
    "categories": [
      "Quickguides"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\npdf version\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-11-16T15:33:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/quickguides/quickguide_loglinear/",
    "title": "Quick Guide - Log linear models",
    "description": {},
    "author": [
      {
        "name": "Ralf Martin",
        "url": {}
      }
    ],
    "date": "2020-11-14",
    "categories": [
      "Quickguides"
    ],
    "contents": "\r\nWhat is the difference between the linear, log and log-log models? They describe different possible relationships between the dependent and explanatory variable. This affects how we can interpret estimated parameters. There are three cases\r\nLinear: \\(y=a+bX\\)\r\nLog: \\(\\ln{y}=a+bX\\)\r\nLog-log: \\(\\ln{y}=a+b \\ln{X}\\)\r\nWe can draw all three in the same y-x diagram by re-writing the equations (and recalling that the exponential function \\(\\exp(\\cdot)\\) is the inverse of the natural logarithm function \\(\\ln(\\cdot)\\)\r\nLinear: \\(y=a+bX\\)\r\nLog: \\(y=\\exp(a+bX)\\)\r\nLog-log: \\(y=\\exp(a+b\\ln{X})\\)\r\nPlotting this for a=1 and b=0.1 will look as follows:\r\n\r\n\r\n\r\ni.e. log vs log-log are two different types of a non-linear relationship between x and y; e.g. in the log case a given change in x will have an increasingly bigger impact on y. The opposite is true in the log-log case. The interpretation of the parameters also changes in the three cases. To understand the differences consider the derivative (or gradient) in each case:\r\nLinear: \\(\\frac{\\partial y}{\\partial x} = b\\)\r\nLog: \\(\\frac{\\partial y}{\\partial x} = \\exp(a+b x)b=y\\times b\\)\r\nLog-Log: \\(\\frac{\\partial y}{\\partial x} = \\exp(a+b \\ln x)b=y\\times b \\times \\frac{1}{x}\\)\r\nFor this we use the following rules for taking derivatives:\r\nDerivative of \\(\\ln z\\) is \\(\\frac{1}{z}\\)\r\nDerivative of \\(exp(z)\\) is \\(exp(z)\\)\r\nChain rule: \\(\\frac{\\partial(f(g(x)))}{\\partial x} = \\frac{\\partial(f(g(x)))}{\\partial g(x)} \\times \\frac{\\partial g(x)}{\\partial x}\\)\r\nTo understand how to interpret the b coefficient, we can re-write the derivative formulas so that we have b on one side of the equation. We can also use the fact that the derivative is approximately the small change in y you get in response to a small change in x; i.e. \\(\\frac{\\partial y}{\\partial x} \\approx \\frac{dy}{dx}\\)\r\nLinear: \\(b=\\frac{\\partial y}{\\partial x} \\approx \\frac{dy}{dx}\\)\r\nLog: \\(b=\\frac{\\partial y}{\\partial x} \\frac{1}{y}\\approx \\frac{dy}{y} \\times \\frac{1}{dx}\\)\r\nLog-log: \\(b=\\frac{\\partial y}{\\partial x} \\frac{x}{y}\\approx \\frac{\\frac{dy}{y}}{ \\frac{dx}{x}}\\)\r\nHence, we get the following interpretations (recalling that \\(dz/z\\) is the growth rate of a variable z):\r\nLinear: Change in \\(y\\) if we change \\(x\\) by 1 unit\r\nLog: Growth rate of \\(y\\) if we change \\(x\\) by 1 unit (the growth rate in percent will be b times 100)\r\nLog-log: Growth rate of \\(y\\) (in percent) in response to growth of \\(x\\) by 1 percent. Note that if \\(x\\) grows by 1 percent we have that \\(dx/x=0.01\\)\r\nExamples:\r\nSuppose that y is the wage in $ and \\(x\\) is the years of experience of an individual\r\nLinear: 1 year more of experience means b$ more wage\r\nLog: 1 year more of experience means b×100 percent more wage\r\nLog-log: If your experience goes up by 1% (e.g. if you current experience is 10 years, 1% is 0.1 years or about 1 months 1 one week) then your wage will go up by b%.\r\nSuppose that y is the number of crimes and is the number of foreigners in an area\r\nLinear: 1 foreigner more implies b more crimes\r\nLog: 1 foreigner more implies \\(b\\times 100\\) percent more crimes\r\nLog-log: If the number of foreigners goes up by 1% then the number of crimes goes up by b%.\r\n",
    "preview": "posts/quickguides/quickguide_loglinear/quickguide_loglinear_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2020-11-14T12:25:24+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
